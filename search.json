[
  {
    "objectID": "02-about.html",
    "href": "02-about.html",
    "title": "About",
    "section": "",
    "text": "DABEST is written in Python by Joses W. Ho, with design and input from Adam Claridge-Chang and other lab members.\nAdditional features in v2023.02.14 were added by Yixuan Li, Zinan Lu and Rou Zhang.\nTo find out more about the authors’ research, please visit the Claridge-Chang lab webpage."
  },
  {
    "objectID": "02-about.html#authors",
    "href": "02-about.html#authors",
    "title": "About",
    "section": "",
    "text": "DABEST is written in Python by Joses W. Ho, with design and input from Adam Claridge-Chang and other lab members.\nAdditional features in v2023.02.14 were added by Yixuan Li, Zinan Lu and Rou Zhang.\nTo find out more about the authors’ research, please visit the Claridge-Chang lab webpage."
  },
  {
    "objectID": "02-about.html#contributors",
    "href": "02-about.html#contributors",
    "title": "About",
    "section": "Contributors",
    "text": "Contributors\n\nStatistics supervision by Hyungwon Choi\nAlpha testers from the Claridge-Chang lab: Sangyu Xu, Xianyuan Zhang, Farhan Mohammad, Jurga Mituzaitė, Stanislav Ott, Tayfun Tumkaya, Jonathan Anns, Nicole Lee and Yishan Mai.\nDizietAsahi (DizietAsahi) with PR #86: Fix bugs in slopegraph and reference line keyword parsing.\nAdam Li (@adam2392) with PR #85: Implement Lq-Likelihood-Ratio-Type Test in statistical output.\nMason Malone (@MasonM) with PR #30: Fix plot error when effect size is 0.\nMatthew Edwards (@mje-nz) with PR #71: Specify dependencies correctly in setup.py.\nAdam Nekimken (@anekimken) with PR #73: Implement inset axes so estimation plots can be plotted on a pre-determined :py:mod:matplotlib :py:class:Axes object.\nMarin Manuel (@MarinManuel) with PR #109: Fixed bug preventing non-string columns from being used."
  },
  {
    "objectID": "02-about.html#typography",
    "href": "02-about.html#typography",
    "title": "About",
    "section": "Typography",
    "text": "Typography\nThis documentation uses Spectral for the body text, Merriweather Sans for the side bar and titles, and IBM Plex Mono for the monospace code font."
  },
  {
    "objectID": "02-about.html#license",
    "href": "02-about.html#license",
    "title": "About",
    "section": "License",
    "text": "License\nThe DABEST package in Python is licenced under the BSD 3-clause Clear License.\nCopyright (c) 2016-2023, Joses W. Ho All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted (subject to the limitations in the disclaimer below) provided that the following conditions are met:\n * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n\n * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\n\n * Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\nNO EXPRESS OR IMPLIED LICENSES TO ANY PARTY’S PATENT RIGHTS ARE GRANTED BY THIS LICENSE. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "tutorials/03-proportion_plot.html",
    "href": "tutorials/03-proportion_plot.html",
    "title": "Proportion Plots",
    "section": "",
    "text": "As of v2023.02.14, DABEST can be used to produce Cohen’s h and the corresponding proportion plot for binary data. It’s important to note that the code we provide only supports numerical proportion data, where the values are limited to 0 (failure) and 1 (success). This means that the code is not suitable for analyzing proportion data that contains non-numeric values, such as strings like ‘yes’ and ‘no’."
  },
  {
    "objectID": "tutorials/03-proportion_plot.html#load-libraries",
    "href": "tutorials/03-proportion_plot.html#load-libraries",
    "title": "Proportion Plots",
    "section": "Load libraries",
    "text": "Load libraries\n\nimport numpy as np\nimport pandas as pd\nimport dabest\n\nprint(\"We're using DABEST v{}\".format(dabest.__version__))\n\nWe're using DABEST v2023.02.14"
  },
  {
    "objectID": "tutorials/03-proportion_plot.html#create-dataset-for-demo",
    "href": "tutorials/03-proportion_plot.html#create-dataset-for-demo",
    "title": "Proportion Plots",
    "section": "Create dataset for demo",
    "text": "Create dataset for demo\n\nnp.random.seed(9999) # Fix the seed so the results are replicable.\nNs = 40 # The number of samples taken from each population\n\n# Create samples\nn = 1\nc1 = np.random.binomial(n, 0.2, size=Ns)\nc2 = np.random.binomial(n, 0.2, size=Ns)\nc3 = np.random.binomial(n, 0.8, size=Ns)\n\nt1 = np.random.binomial(n, 0.5, size=Ns)\nt2 = np.random.binomial(n, 0.2, size=Ns)\nt3 = np.random.binomial(n, 0.3, size=Ns)\nt4 = np.random.binomial(n, 0.4, size=Ns)\nt5 = np.random.binomial(n, 0.5, size=Ns)\nt6 = np.random.binomial(n, 0.6, size=Ns)\n\n\n# Add a `gender` column for coloring the data.\nfemales = np.repeat('Female', Ns / 2).tolist()\nmales = np.repeat('Male', Ns / 2).tolist()\ngender = females + males\n\n# Add an `id` column for paired data plotting.\nid_col = pd.Series(range(1, Ns + 1))\n\n# Combine samples and gender into a DataFrame.\ndf = pd.DataFrame({'Control 1': c1, 'Test 1': t1,\n                   'Control 2': c2, 'Test 2': t2,\n                   'Control 3': c3, 'Test 3': t3,\n                   'Test 4': t4, 'Test 5': t5, 'Test 6': t6,\n                   'Gender': gender, 'ID': id_col\n                   })\ndf.head()\n\n\n\n\n\n\n\n\nControl 1\nTest 1\nControl 2\nTest 2\nControl 3\nTest 3\nTest 4\nTest 5\nTest 6\nGender\nID\n\n\n\n\n0\n1\n1\n0\n0\n1\n0\n0\n1\n0\nFemale\n1\n\n\n1\n0\n0\n0\n1\n1\n1\n0\n0\n0\nFemale\n2\n\n\n2\n0\n0\n0\n0\n1\n0\n1\n1\n0\nFemale\n3\n\n\n3\n0\n0\n0\n0\n1\n0\n0\n1\n0\nFemale\n4\n\n\n4\n0\n1\n0\n0\n1\n0\n0\n0\n1\nFemale\n5"
  },
  {
    "objectID": "tutorials/03-proportion_plot.html#loading-data",
    "href": "tutorials/03-proportion_plot.html#loading-data",
    "title": "Proportion Plots",
    "section": "Loading Data",
    "text": "Loading Data\nWhen loading data, specify proportional=True.\n\ntwo_groups_unpaired = dabest.load(df, idx=(\"Control 1\", \"Test 1\"), proportional=True)\n\n\ntwo_groups_unpaired\n\nDABEST v2023.02.14\n==================\n                  \nGood evening!\nThe current time is Sun Mar 19 22:41:40 2023.\n\nEffect size(s) with 95% confidence intervals will be computed for:\n1. Test 1 minus Control 1\n\n5000 resamples will be used to generate the effect size bootstraps."
  },
  {
    "objectID": "tutorials/03-proportion_plot.html#effect-sizes",
    "href": "tutorials/03-proportion_plot.html#effect-sizes",
    "title": "Proportion Plots",
    "section": "Effect sizes",
    "text": "Effect sizes\nFor proportion plot, dabest features two effect sizes: - the mean difference (mean_diff) - Cohen’s h ([cohens_h](https://ZHANGROU-99.github.io/DABEST-python/API/effsize.html#cohens_h))\nEach of these are attributes of the Dabest object.\n\ntwo_groups_unpaired.mean_diff\n\nDABEST v2023.02.14\n==================\n                  \nGood evening!\nThe current time is Sun Mar 19 22:42:28 2023.\n\nThe unpaired mean difference between Control 1 and Test 1 is 0.375 [95%CI 0.15, 0.525].\nThe p-value of the two-sided permutation t-test is 0.0, calculated for legacy purposes only. \n\n5000 bootstrap samples were taken; the confidence interval is bias-corrected and accelerated.\nAny p-value reported is the probability of observing theeffect size (or greater),\nassuming the null hypothesis ofzero difference is true.\nFor each p-value, 5000 reshuffles of the control and test labels were performed.\n\nTo get the results of all valid statistical tests, use `.mean_diff.statistical_tests`\n\n\nLet’s compute the Cohen’s h for our comparison.\n\ntwo_groups_unpaired.cohens_h\n\nDABEST v2023.02.14\n==================\n                  \nGood evening!\nThe current time is Sun Mar 19 22:42:45 2023.\n\nThe unpaired Cohen's h between Control 1 and Test 1 is 0.825 [95%CI 0.33, 1.22].\nThe p-value of the two-sided permutation t-test is 0.0, calculated for legacy purposes only. \n\n5000 bootstrap samples were taken; the confidence interval is bias-corrected and accelerated.\nAny p-value reported is the probability of observing theeffect size (or greater),\nassuming the null hypothesis ofzero difference is true.\nFor each p-value, 5000 reshuffles of the control and test labels were performed.\n\nTo get the results of all valid statistical tests, use `.cohens_h.statistical_tests`"
  },
  {
    "objectID": "tutorials/03-proportion_plot.html#producing-proportional-plots",
    "href": "tutorials/03-proportion_plot.html#producing-proportional-plots",
    "title": "Proportion Plots",
    "section": "Producing Proportional Plots",
    "text": "Producing Proportional Plots\nTo produce a Gardner-Altman estimation plot, simply use the .plot() method.\nEvery effect size instance has access to the .plot() method. This means you can quickly create plots for different effect sizes easily.\n\ntwo_groups_unpaired.mean_diff.plot();\n\n\n\n\n\ntwo_groups_unpaired.cohens_h.plot();\n\n\n\n\nThe white part in the bar represents the proportion of observations in the dataset that do not belong to the category, which is equivalent to the proportion of 0 in the data. The colored part, on the other hand, represents the proportion of observations that belong to the category, which is equivalent to the proportion of 1 in the data. By default, the value of ‘group_summaries’ is set to “mean_sd”. This means that the error bars in the plot display the mean and ± standard deviation of each group as gapped lines. The gap represents the mean, while the vertical ends represent the standard deviation. Alternatively, if the value of ‘group_summaries’ is set to “median_quartiles”, the median and 25th and 75th percentiles of each group are plotted instead. By default, the bootstrap effect sizes is plotted on the right axis.\nInstead of a Gardner-Altman plot, you can produce a Cumming estimation plot by setting float_contrast=False in the plot() method. This will plot the bootstrap effect sizes below the raw data.\n\ntwo_groups_unpaired.mean_diff.plot(float_contrast=False);\n\n\n\n\nYou can also modify the width of bars as you expect by setting bar_width in the plot() method.\n\ntwo_groups_unpaired.mean_diff.plot(bar_width=0.3);\n\n\n\n\nThe bar_desat is used to control the amount of desaturation applied to the bar colors. A value of 0.0 means full desaturation (i.e., grayscale), while a value of 1.0 means no desaturation (i.e., full color saturation). Default is 0.8.\n\ntwo_groups_unpaired.mean_diff.plot(bar_desat=1.0);\n\n\n\n\nbar_label and contrast_label can be used to set labels for the y-axis of the bar plot and the contrast plot.\n\ntwo_groups_unpaired.mean_diff.plot(bar_label=\"success\",contrast_label=\"difference\");\n\n\n\n\nThe color of error bar can be modified by setting ‘err_color’.\n\ntwo_groups_unpaired.mean_diff.plot(err_color=\"purple\");"
  },
  {
    "objectID": "tutorials/03-proportion_plot.html#producing-paired-proportion-plots",
    "href": "tutorials/03-proportion_plot.html#producing-paired-proportion-plots",
    "title": "Proportion Plots",
    "section": "Producing Paired Proportion Plots",
    "text": "Producing Paired Proportion Plots\n\ntwo_groups_unpaired.cohens_h.results\n\n\n\n\n\n\n\n\ncontrol\ntest\ncontrol_N\ntest_N\neffect_size\nis_paired\ndifference\nci\nbca_low\nbca_high\n...\npvalue_permutation\npermutation_count\npermutations_var\npvalue_welch\nstatistic_welch\npvalue_students_t\nstatistic_students_t\npvalue_mann_whitney\nstatistic_mann_whitney\nproportional_difference\n\n\n\n\n0\nControl 1\nTest 1\n40\n40\nCohen's h\nNone\n0.825418\n95\n0.329684\n1.219937\n...\n0.0\n5000\n[0.011266025641025641, 0.011266025641025641, 0...\n0.000289\n-3.81474\n0.000271\n-3.81474\n0.000434\n500.0\n0.825418\n\n\n\n\n1 rows × 28 columns"
  },
  {
    "objectID": "tutorials/03-proportion_plot.html#producing-estimation-plots",
    "href": "tutorials/03-proportion_plot.html#producing-estimation-plots",
    "title": "Proportion Plots",
    "section": "Producing estimation plots",
    "text": "Producing estimation plots\n\ntwo_groups_unpaired.mean_diff.plot();\n\n\n\n\n\ntwo_groups_unpaired.cohens_h.plot();\n\n\n\n\nThe white part in the bar represents the proportion of observations in the dataset that do not belong to the category, which is equivalent to the proportion of 0 in the data. The colored part, on the other hand, represents the proportion of observations that belong to the category, which is equivalent to the proportion of 1 in the data. By default, the value of “group_summaries” is set to “mean_sd”. This means that the error bars in the plot display the mean and ± standard deviation of each group as gapped lines. The gap represents the mean, while the vertical ends represent the standard deviation. Alternatively, if the value of “group_summaries” is set to “median_quartiles”, the median and 25th and 75th percentiles of each group are plotted instead. By default, the bootstrap effect sizes is plotted on the right axis.\nInstead of a Gardner-Altman plot, you can produce a Cumming estimation plot by setting float_contrast=False in the plot() method. This will plot the bootstrap effect sizes below the raw data.\n\ntwo_groups_unpaired.mean_diff.plot(float_contrast=False);"
  },
  {
    "objectID": "tutorials/03-proportion_plot.html#producing-paired-proportion-plots-1",
    "href": "tutorials/03-proportion_plot.html#producing-paired-proportion-plots-1",
    "title": "Proportion Plots",
    "section": "Producing Paired Proportion Plots",
    "text": "Producing Paired Proportion Plots\nFor paired version of proportional plot, we adapt the style of Sankey Diagram. The width of each bar in each xticks represent the proportion of corresponding label in the group, and the strip denotes the paired relationship for each observation.\nSimilar to the unpaired version, the .plot() method is used to produce a Gardner-Altman estimation plot, the only difference is that the is_paired parameter is set to either baseline or sequential when loading data.\n\ntwo_groups_baseline = dabest.load(df, idx=(\"Control 1\", \"Test 1\"), \n                                  proportional=True, paired=\"baseline\", id_col=\"ID\")\n    \ntwo_groups_baseline.mean_diff.plot();\n\n\n\n\nThe paired proportional plot also supports the float_contrast parameter, which can be set to False to produce a Cumming estimation plot.\n\ntwo_groups_baseline.mean_diff.plot(float_contrast=False);\n\n\n\n\nThe upper part (grey part) of the bar represents the proportion of observations in the dataset that do not belong to the category, which is equivalent to the proportion of 0 in the data. The lower part, on the other hand, represents the proportion of observations that belong to the category, which is or success, which is equivalent to the proportion of 1 in the data.\nRepeated measures is also supported in paired proportional plot, by changing the is_paired parameter, two types of plot can be produced.\n\nmulti_group_baseline = dabest.load(df, idx=(((\"Control 1\", \"Test 1\",\"Test 2\", \"Test 3\"),\n                                (\"Test 4\", \"Test 5\", \"Test 6\"))),\n                    proportional=True, paired=\"baseline\", id_col=\"ID\")\n\nmulti_group_baseline.mean_diff.plot();\n\n\n\n\n\nmulti_group_sequential = dabest.load(df, idx=(((\"Control 1\", \"Test 1\",\"Test 2\", \"Test 3\"),\n                                (\"Test 4\", \"Test 5\", \"Test 6\"))),\n                    proportional=True, paired=\"sequential\", id_col=\"ID\")\n\nmulti_group_sequential.mean_diff.plot();\n\n\n\n\nFrom the above two images, we can see that the on both the observed value plot and delta plot, the pairs compared are different in terms of the paired settings. And for detailed information about repeated measures, please refer to :doc:repeatedmeasures .\nIf you want to specify the order of the groups, you can use the idx parameter in the .load() method.\nFor all the groups to be compared together, you can put all the groups in the idx parameter in the .load() method without subbrackets.\n\nmulti_group_baseline_specify = dabest.load(df, idx=((\"Control 1\", \"Test 1\",\"Test 2\", \"Test 3\",\n                                \"Test 4\", \"Test 5\", \"Test 6\")),\n                    proportional=True, paired=\"baseline\", id_col=\"ID\")\n\nmulti_group_baseline_specify.mean_diff.plot();\n\n\n\n\nSeveral exclusive parameters can be supplied to the plot() method to customize the paired proportional plot. By updating the sankey_kwargs parameter, you can customize the Sankey plot. The following parameters are supported:\n\nwidth: The width of each Sankey bar. Default is 0.5.\nalign: The alignment of each Sankey bar. Default is “center”.\nalpha: The transparency of each Sankey bar. Default is 0.4.\nbar_width: The width of each bar on the side in the plot. Default is 0.1.\n\n\ntwo_groups_baseline.mean_diff.plot(sankey_kwargs = {\"alpha\": 0.2});"
  },
  {
    "objectID": "tutorials/05-deltadelta.html",
    "href": "tutorials/05-deltadelta.html",
    "title": "Delta - Delta",
    "section": "",
    "text": "Since version 2023.02.14, DABEST also supports the calculation of delta-delta, an experimental function that allows the comparison between two bootstrapped effect sizes computed from two independent categorical variables.\nMany experimental designs investigate the effects of two interacting independent variables on a dependent variable. The delta-delta effect size lets us distill the net effect of the two variables. To illustrate this, let’s delve into the following problem.\nConsider an experiment where we test the efficacy of a drug named Drug on a disease-causing mutation M based on disease metric Y. The greater value Y has the more severe the disease phenotype is. Phenotype Y has been shown to be caused by a gain of function mutation M, so we expect a difference between wild type (W) subjects and mutant subjects (M). Now, we want to know whether this effect is ameliorated by the administration of Drug treatment. We also administer a placebo as a control. In theory, we only expect Drug to have an effect on the M group, although in practice many drugs have non-specific effects on healthy populations too.\nEffectively, we have 4 groups of subjects for comparison.\nThere are 2 Treatment conditions, Placebo (control group) and Drug (test group). There are 2 Genotype: W (wild type population) and M (mutant population). In addition, each experiment was done twice (Rep1 and Rep2). We shall do a few analyses to visualise these differences in a simulated dataset."
  },
  {
    "objectID": "tutorials/05-deltadelta.html#load-libraries",
    "href": "tutorials/05-deltadelta.html#load-libraries",
    "title": "Delta - Delta",
    "section": "Load Libraries",
    "text": "Load Libraries\n\nimport numpy as np\nimport pandas as pd\nimport dabest\n\nprint(\"We're using DABEST v{}\".format(dabest.__version__))\n\nWe're using DABEST v2023.02.14"
  },
  {
    "objectID": "tutorials/05-deltadelta.html#simulate-a-dataset",
    "href": "tutorials/05-deltadelta.html#simulate-a-dataset",
    "title": "Delta - Delta",
    "section": "Simulate a dataset",
    "text": "Simulate a dataset\n\nfrom scipy.stats import norm # Used in generation of populations.\nnp.random.seed(9999) # Fix the seed so the results are replicable.\n\n# Create samples\nN = 20\ny = norm.rvs(loc=3, scale=0.4, size=N*4)\ny[N:2*N] = y[N:2*N]+1\ny[2*N:3*N] = y[2*N:3*N]-0.5\n\n# Add a `Treatment` column\nt1 = np.repeat('Placebo', N*2).tolist()\nt2 = np.repeat('Drug', N*2).tolist()\ntreatment = t1 + t2 \n\n# Add a `Rep` column as the first variable for the 2 replicates of experiments done\nrep = []\nfor i in range(N*2):\n    rep.append('Rep1')\n    rep.append('Rep2')\n\n# Add a `Genotype` column as the second variable\nwt = np.repeat('W', N).tolist()\nmt = np.repeat('M', N).tolist()\nwt2 = np.repeat('W', N).tolist()\nmt2 = np.repeat('M', N).tolist()\n\n\ngenotype = wt + mt + wt2 + mt2\n\n# Add an `id` column for paired data plotting.\nid = list(range(0, N*2))\nid_col = id + id \n\n\n# Combine all columns into a DataFrame.\ndf_delta2 = pd.DataFrame({'ID'        : id_col,\n                  'Rep'      : rep,\n                   'Genotype'  : genotype, \n                   'Treatment': treatment,\n                   'Y'         : y\n                })\n\n\ndf_delta2.head()\n\n\n\n\n\n\n\n\nID\nRep\nGenotype\nTreatment\nY\n\n\n\n\n0\n0\nRep1\nW\nPlacebo\n2.793984\n\n\n1\n1\nRep2\nW\nPlacebo\n3.236759\n\n\n2\n2\nRep1\nW\nPlacebo\n3.019149\n\n\n3\n3\nRep2\nW\nPlacebo\n2.804638\n\n\n4\n4\nRep1\nW\nPlacebo\n2.858019"
  },
  {
    "objectID": "tutorials/05-deltadelta.html#unpaired-data",
    "href": "tutorials/05-deltadelta.html#unpaired-data",
    "title": "Delta - Delta",
    "section": "Unpaired Data",
    "text": "Unpaired Data\nTo make a delta-delta plot, you need to simply set delta2 = True in the dabest.load() function. However, here x needs to be declared as a list consisting of 2 elements rather than 1 in most of the cases. The first element in x will be the variable plotted along the horizontal axis, and the second one will determine the colour of dots for scattered plots or the colour of lines for slopegraphs. We use the experiment input to specify grouping of the data.\n\nunpaired_delta2 = dabest.load(data = df_delta2, x = [\"Genotype\", \"Genotype\"], y = \"Y\", delta2 = True, experiment = \"Treatment\")\n\nThe above function creates the following object:\n\nunpaired_delta2\n\nDABEST v2023.02.14\n==================\n                  \nGood evening!\nThe current time is Sun Mar 19 23:07:31 2023.\n\nEffect size(s) with 95% confidence intervals will be computed for:\n1. M Placebo minus W Placebo\n2. M Drug minus W Drug\n3. Drug minus Placebo (only for mean difference)\n\n5000 resamples will be used to generate the effect size bootstraps.\n\n\nWe can quickly check out the effect sizes:\n\nunpaired_delta2.mean_diff\n\nDABEST v2023.02.14\n==================\n                  \nGood evening!\nThe current time is Sun Mar 19 23:07:42 2023.\n\nThe unpaired mean difference between W Placebo and M Placebo is 1.23 [95%CI 0.948, 1.52].\nThe p-value of the two-sided permutation t-test is 0.0, calculated for legacy purposes only. \n\nThe unpaired mean difference between W Drug and M Drug is 0.326 [95%CI 0.0934, 0.584].\nThe p-value of the two-sided permutation t-test is 0.0122, calculated for legacy purposes only. \n\nThe delta-delta between Placebo and Drug is -0.903 [95%CI -1.26, -0.535].\nThe p-value of the two-sided permutation t-test is 0.0, calculated for legacy purposes only. \n\n5000 bootstrap samples were taken; the confidence interval is bias-corrected and accelerated.\nAny p-value reported is the probability of observing the effect size (or greater),\nassuming the null hypothesis of zero difference is true.\nFor each p-value, 5000 reshuffles of the control and test labels were performed.\n\nTo get the results of all valid statistical tests, use `.mean_diff.statistical_tests`\n\n\n\nunpaired_delta2.mean_diff.plot();\n\n\n\n\nIn the above plot, the horizontal axis represents the Genotype condition and the dot colour is also specified by Genotype. The left pair of scattered plots is based on the Placebo group while the right pair is based on the Drug group. The bottom left axis contains the two primary deltas: the Placebo delta and the Drug delta. We can easily see that when only the placebo was administered, the mutant phenotype is around 1.23 [95%CI 0.948, 1.52]. This difference was shrunken to around 0.326 [95%CI 0.0934, 0.584] when the drug was administered. This gives us some indication that the drug is effective in amiliorating the disease phenotype. Since the Drug did not completely eliminate the mutant phenotype, we have to calculate how much net effect the drug had. This is where delta-delta comes in. We use the Placebo delta as a reference for how much the mutant phenotype is supposed to be, and we subtract the Drug delta from it. The bootstrapped mean differences (delta-delta) between the Placebo and Drug group are plotted at the right bottom with a separate y-axis from other bootstrap plots. This effect size, at about -0.903 [95%CI -1.26, -0.535], is the net effect size of the drug treatment. That is to say that treatment with drug A reduced disease phenotype by 0.903.\nMean difference between mutants and wild types given the placebo treatment is:\n\\(\\Delta_{1} = \\overline{X}_{P, M} - \\overline{X}_{P, W}\\)\nMean difference between mutants and wild types given the drug treatment is:\n\\(\\Delta_{2} = \\overline{X}_{D, M} - \\overline{X}_{D, W}\\)\nThe net effect of the drug on mutants is:\n\\(\\Delta_{\\Delta} = \\Delta_{2} - \\Delta_{1}\\)\nwhere \\(\\overline{X}\\) is the sample mean, \\(\\Delta\\) is the mean difference."
  },
  {
    "objectID": "tutorials/05-deltadelta.html#specifying-grouping-for-comparisons",
    "href": "tutorials/05-deltadelta.html#specifying-grouping-for-comparisons",
    "title": "Delta - Delta",
    "section": "Specifying Grouping for Comparisons",
    "text": "Specifying Grouping for Comparisons\nIn the example above, we used the convention of “test - control’ but you can manipulate the orders of experiment groups as well as the horizontal axis variable by setting experiment_label and x1_level.\n\nunpaired_delta2_specified = dabest.load(data = df_delta2, \n                                            x = [\"Genotype\", \"Genotype\"], y = \"Y\", \n                                            delta2 = True, experiment = \"Treatment\",\n                                            experiment_label = [\"Drug\", \"Placebo\"],\n                                            x1_level = [\"M\", \"W\"])\n\nunpaired_delta2_specified.mean_diff.plot();"
  },
  {
    "objectID": "tutorials/05-deltadelta.html#paired-data",
    "href": "tutorials/05-deltadelta.html#paired-data",
    "title": "Delta - Delta",
    "section": "Paired Data",
    "text": "Paired Data\nThe delta - delta function also supports paired data, which is useful for us to visualise the data in an alternate way. Assuming that the placebo and drug treatment were done on the same subjects, our data is paired between the treatment conditions. We can specify this by using Treatment as x and Genotype as experiment, and we further specify that id_col is ID, linking data from the same subject with each other. Since we have done two replicates of the experiments, we can also colour the slope lines according to Rep.\n\npaired_delta2 = dabest.load(data = df_delta2, \n                                paired = \"baseline\", id_col=\"ID\",\n                                x = [\"Treatment\", \"Rep\"], y = \"Y\", \n                                delta2 = True, experiment = \"Genotype\")\npaired_delta2.mean_diff.plot();\n\n\n\n\nWe see that the drug had a non-specific effect of -0.321 [95%CI -0.498, -0.131] on wild type subjects even when they were not sick, and it had a bigger effect of -1.22 [95%CI -1.52, -0.906] in mutant subjects. In this visualisation, we can see the delta-delta value of -0.903 [95%CI -1.21, -0.587] as the net effect of the drug accounting for non-specific actions in healthy individuals.\nMean difference between drug and placebo treatments in wild type subjects is:\n\\[\\Delta_{1} = \\overline{X}_{D, W} - \\overline{X}_{P, W}\\]\nMean difference between drug and placebo treatments in mutant subjects is:\n\\[\\Delta_{2} = \\overline{X}_{D, M} - \\overline{X}_{P, M}\\]\nThe net effect of the drug on mutants is:\n\\[\\Delta_{\\Delta} = \\Delta_{2} - \\Delta_{1}\\]\nwhere \\(\\overline{X}\\) is the sample mean, \\(\\Delta\\) is the mean difference."
  },
  {
    "objectID": "tutorials/05-deltadelta.html#connection-to-anova",
    "href": "tutorials/05-deltadelta.html#connection-to-anova",
    "title": "Delta - Delta",
    "section": "Connection to ANOVA",
    "text": "Connection to ANOVA\nThe configuration of comparison we performed above is reminiscent of a two-way ANOVA. In fact, the delta - delta is an effect size estimated for the interaction term between Treatment and Genotype. Main effects of Treatment and Genotype, on the other hand, can be determined by simpler, univariate contrast plots."
  },
  {
    "objectID": "tutorials/05-deltadelta.html#omitting-delta-delta-plot",
    "href": "tutorials/05-deltadelta.html#omitting-delta-delta-plot",
    "title": "Delta - Delta",
    "section": "Omitting Delta-delta Plot",
    "text": "Omitting Delta-delta Plot\nIf for some reason you don’t want to display the delta-delta plot, you can easily do so by\n\nunpaired_delta2.mean_diff.plot(show_delta2=False);"
  },
  {
    "objectID": "tutorials/05-deltadelta.html#other-effect-sizes",
    "href": "tutorials/05-deltadelta.html#other-effect-sizes",
    "title": "Delta - Delta",
    "section": "Other Effect Sizes",
    "text": "Other Effect Sizes\nSince the delta-delta function is only applicable to mean differences, plots of other effect sizes will not include a delta-delta bootstrap plot.\n\nunpaired_delta2.cohens_d.plot();"
  },
  {
    "objectID": "tutorials/05-deltadelta.html#statistics",
    "href": "tutorials/05-deltadelta.html#statistics",
    "title": "Delta - Delta",
    "section": "Statistics",
    "text": "Statistics\nYou can find all outputs of the delta - delta calculation by assessing the attribute named delta_delta of the effect size object.\n\nunpaired_delta2.mean_diff.delta_delta\n\nDABEST v0.0.1\n=============\n             \nGood evening!\nThe current time is Sun Mar 12 00:55:42 2023.\n\nThe delta-delta between Placebo and Drug is -0.903 [95%CI -1.26, -0.535].\nThe p-value of the two-sided permutation t-test is 0.0, calculated for legacy purposes only. \n\n5000 bootstrap samples were taken; the confidence interval is bias-corrected and accelerated.\nAny p-value reported is the probability of observing theeffect size (or greater),\nassuming the null hypothesis ofzero difference is true.\nFor each p-value, 5000 reshuffles of the control and test labels were performed.\n\n\ndelta_delta has its own attributes, containing various information of delta - delta.\n\ndifference: the mean bootstrapped differences between the 2 groups of bootstrapped mean differences\nbootstraps: the 2 groups of bootstrapped mean differences\nbootstraps_delta_delta: the bootstrapped differences between the 2 groups of bootstrapped mean differences\npermutations: the mean difference between the two groups of bootstrapped mean differences calculated based on the permutation data\npermutations_var: the pooled group variances of two groups of bootstrapped mean differences calculated based on permutation data\npermutations_delta_delta: the delta-delta calculated based on the permutation data\n\ndelta_delta.to_dict() will return to you all the attributes in a dictionary format."
  },
  {
    "objectID": "tutorials/06-plotaesthetics.html",
    "href": "tutorials/06-plotaesthetics.html",
    "title": "Controlling Plot Aesthetics",
    "section": "",
    "text": "Changing the y-axes labels.\nimport numpy as np\nimport pandas as pd\nimport dabest\n\nprint(\"We're using DABEST v{}\".format(dabest.__version__))\n\nWe're using DABEST v2023.02.14\nfrom scipy.stats import norm # Used in generation of populations.\n\nnp.random.seed(9999) # Fix the seed so the results are replicable.\n# pop_size = 10000 # Size of each population.\nNs = 20 # The number of samples taken from each population\n\n# Create samples\nc1 = norm.rvs(loc=3, scale=0.4, size=Ns)\nc2 = norm.rvs(loc=3.5, scale=0.75, size=Ns)\nc3 = norm.rvs(loc=3.25, scale=0.4, size=Ns)\n\nt1 = norm.rvs(loc=3.5, scale=0.5, size=Ns)\nt2 = norm.rvs(loc=2.5, scale=0.6, size=Ns)\nt3 = norm.rvs(loc=3, scale=0.75, size=Ns)\nt4 = norm.rvs(loc=3.5, scale=0.75, size=Ns)\nt5 = norm.rvs(loc=3.25, scale=0.4, size=Ns)\nt6 = norm.rvs(loc=3.25, scale=0.4, size=Ns)\n\n\n# Add a `gender` column for coloring the data.\nfemales = np.repeat('Female', Ns/2).tolist()\nmales = np.repeat('Male', Ns/2).tolist()\ngender = females + males\n\n# Add an `id` column for paired data plotting.\nid_col = pd.Series(range(1, Ns+1))\n\n# Combine samples and gender into a DataFrame.\ndf = pd.DataFrame({'Control 1' : c1,     'Test 1' : t1,\n                 'Control 2' : c2,     'Test 2' : t2,\n                 'Control 3' : c3,     'Test 3' : t3,\n                 'Test 4'    : t4,     'Test 5' : t5, 'Test 6' : t6,\n                 'Gender'    : gender, 'ID'  : id_col\n                })\ntwo_groups_unpaired = dabest.load(df, idx=(\"Control 1\", \"Test 1\"), resamples=5000)\nChanging the y-axes labels.\ntwo_groups_unpaired.mean_diff.plot(swarm_label=\"This is my\\nrawdata\",\n                                       contrast_label=\"The bootstrap\\ndistribtions!\");\nColor the rawdata according to another column in the dataframe.\nmulti_2group = dabest.load(df, idx=((\"Control 1\", \"Test 1\",),\n                                         (\"Control 2\", \"Test 2\")\n                                       ))\nmulti_2group.mean_diff.plot(color_col=\"Gender\");\ntwo_groups_paired_baseline = dabest.load(df, idx=(\"Control 1\", \"Test 1\"),\n                                  paired=\"baseline\", id_col=\"ID\")\ntwo_groups_paired_baseline.mean_diff.plot(color_col=\"Gender\");\nChanging the palette used with custom_palette. Any valid matplotlib or seaborn color palette is accepted.\nmulti_2group.mean_diff.plot(color_col=\"Gender\", custom_palette=\"Dark2\");\nmulti_2group.mean_diff.plot(custom_palette=\"Paired\");\nYou can also create your own color palette. Create a dictionary where the keys are group names, and the values are valid matplotlib colors.\nYou can specify matplotlib colors in a variety of ways. Here, I demonstrate using named colors, hex strings (commonly used on the web), and RGB tuples.\nmy_color_palette = {\"Control 1\" : \"blue\",\n                        \"Test 1\"    : \"purple\",\n                        \"Control 2\" : \"#cb4b16\",     # This is a hex string.\n                        \"Test 2\"    : (0., 0.7, 0.2) # This is a RGB tuple.\n                       }\n\nmulti_2group.mean_diff.plot(custom_palette=my_color_palette);\nBy default, dabest.plot() will desaturate the color of the dots in the swarmplot by 50%. This draws attention to the effect size bootstrap curves.\nYou can alter the default values with the swarm_desat and halfviolin_desat keywords.\nmulti_2group.mean_diff.plot(custom_palette=my_color_palette,\n                                swarm_desat=0.75,\n                                halfviolin_desat=0.25);\nYou can also change the sizes of the dots used in the rawdata swarmplot, and those used to indicate the effect sizes.\nmulti_2group.mean_diff.plot(raw_marker_size=3,\n                                es_marker_size=12);\nChanging the y-limits for the rawdata axes, and for the contrast axes.\nmulti_2group.mean_diff.plot(swarm_ylim=(0, 5),\n                                contrast_ylim=(-2, 2));\nIf your effect size is qualitatively inverted (ie. a smaller value is a better outcome), you can simply invert the tuple passed to contrast_ylim.\nmulti_2group.mean_diff.plot(contrast_ylim=(2, -2),\n                                contrast_label=\"More negative is better!\");\nThe contrast axes share the same y-limits as that of the delta - delta plot and thus the y axis of the delta - delta plot changes as well.\nnp.random.seed(9999) # Fix the seed so the results are replicable.\n\n# Create samples\nN = 20\ny = norm.rvs(loc=3, scale=0.4, size=N*4)\ny[N:2*N] = y[N:2*N]+1\ny[2*N:3*N] = y[2*N:3*N]-0.5\n\n# Add a `Treatment` column\nt1 = np.repeat('Placebo', N*2).tolist()\nt2 = np.repeat('Drug', N*2).tolist()\ntreatment = t1 + t2 \n\n# Add a `Rep` column as the first variable for the 2 replicates of experiments done\nrep = []\nfor i in range(N*2):\n    rep.append('Rep1')\n    rep.append('Rep2')\n\n# Add a `Genotype` column as the second variable\nwt = np.repeat('W', N).tolist()\nmt = np.repeat('M', N).tolist()\nwt2 = np.repeat('W', N).tolist()\nmt2 = np.repeat('M', N).tolist()\n\n\ngenotype = wt + mt + wt2 + mt2\n\n# Add an `id` column for paired data plotting.\nid = list(range(0, N*2))\nid_col = id + id \n\n\n# Combine all columns into a DataFrame.\ndf_delta2 = pd.DataFrame({'ID'        : id_col,\n                  'Rep'      : rep,\n                   'Genotype'  : genotype, \n                   'Treatment': treatment,\n                   'Y'         : y\n                })\n\npaired_delta2 = dabest.load(data = df_delta2, \n                                paired = \"baseline\", id_col=\"ID\",\n                                x = [\"Treatment\", \"Rep\"], y = \"Y\", \n                                delta2 = True, experiment = \"Genotype\")\npaired_delta2.mean_diff.plot(contrast_ylim=(3, -3),\n                                 contrast_label=\"More negative is better!\");\nYou can also change the y-limits and y-label for the delta - delta plot.\npaired_delta2.mean_diff.plot(delta2_ylim=(3, -3),\n                                 delta2_label=\"More negative is better!\");\nYou can add minor ticks and also change the tick frequency by accessing the axes directly.\nEach estimation plot produced by dabest has 2 axes. The first one contains the rawdata swarmplot; the second one contains the bootstrap effect size differences.\nimport matplotlib.ticker as Ticker\n\nf = two_groups_unpaired.mean_diff.plot()\n\nrawswarm_axes = f.axes[0]\ncontrast_axes = f.axes[1]\n\nrawswarm_axes.yaxis.set_major_locator(Ticker.MultipleLocator(1))\nrawswarm_axes.yaxis.set_minor_locator(Ticker.MultipleLocator(0.5))\n\ncontrast_axes.yaxis.set_major_locator(Ticker.MultipleLocator(0.5))\ncontrast_axes.yaxis.set_minor_locator(Ticker.MultipleLocator(0.25))\nf = multi_2group.mean_diff.plot(swarm_ylim=(0,6),\n                                   contrast_ylim=(-3, 1))\n\nrawswarm_axes = f.axes[0]\ncontrast_axes = f.axes[1]\n\nrawswarm_axes.yaxis.set_major_locator(Ticker.MultipleLocator(2))\nrawswarm_axes.yaxis.set_minor_locator(Ticker.MultipleLocator(1))\n\ncontrast_axes.yaxis.set_major_locator(Ticker.MultipleLocator(0.5))\ncontrast_axes.yaxis.set_minor_locator(Ticker.MultipleLocator(0.25))\nFor mini-meta plots, you can hide the weighted avergae plot by setting show_mini_meta=False in the plot() function.\nnp.random.seed(9999) # Fix the seed so the results are replicable.\n# pop_size = 10000 # Size of each population.\nNs = 20 # The number of samples taken from each population\n\n# Create samples\nc1 = norm.rvs(loc=3, scale=0.4, size=Ns)\nc2 = norm.rvs(loc=3.5, scale=0.75, size=Ns)\nc3 = norm.rvs(loc=3.25, scale=0.4, size=Ns)\n\nt1 = norm.rvs(loc=3.5, scale=0.5, size=Ns)\nt2 = norm.rvs(loc=2.5, scale=0.6, size=Ns)\nt3 = norm.rvs(loc=3, scale=0.75, size=Ns)\n\n\n# Add a `gender` column for coloring the data.\nfemales = np.repeat('Female', Ns/2).tolist()\nmales = np.repeat('Male', Ns/2).tolist()\ngender = females + males\n\n# Add an `id` column for paired data plotting.\nid_col = pd.Series(range(1, Ns+1))\n\n# Combine samples and gender into a DataFrame.\ndf = pd.DataFrame({'Control 1' : c1,     'Test 1' : t1,\n                   'Control 2' : c2,     'Test 2' : t2,\n                   'Control 3' : c3,     'Test 3' : t3,\n                   'Gender'    : gender, 'ID'  : id_col\n                  })\nmini_meta_paired = dabest.load(df, idx=((\"Control 1\", \"Test 1\"), (\"Control 2\", \"Test 2\"), (\"Control 3\", \"Test 3\")), mini_meta=True, id_col=\"ID\", paired=\"baseline\")\nmini_meta_paired.mean_diff.plot(show_mini_meta=False);\nSimilarly, you can also hide the delta-delta plot by setting show_delta2=False in the plot() function.\npaired_delta2.mean_diff.plot(show_delta2=False);"
  },
  {
    "objectID": "tutorials/06-plotaesthetics.html#creating-estimation-plots-in-existing-axes",
    "href": "tutorials/06-plotaesthetics.html#creating-estimation-plots-in-existing-axes",
    "title": "Controlling Plot Aesthetics",
    "section": "Creating estimation plots in existing axes",
    "text": "Creating estimation plots in existing axes\nImplemented in v0.2.6 by Adam Nekimken.\ndabest.plot has an ax keyword that accepts any Matplotlib Axes. The entire estimation plot will be created in the specified Axes.\n\ntwo_groups_paired_baseline = dabest.load(df, idx=(\"Control 1\", \"Test 1\"),\n                                  paired=\"baseline\", id_col=\"ID\")\nmulti_2group_paired = dabest.load(df,\n                            idx=((\"Control 1\", \"Test 1\"),\n                                 (\"Control 2\", \"Test 2\")),\n                            paired=\"baseline\", id_col=\"ID\")\n\n\nfrom matplotlib import pyplot as plt\nf, axx = plt.subplots(nrows=2, ncols=2,\n                      figsize=(15, 15),\n                      gridspec_kw={'wspace': 0.25} # ensure proper width-wise spacing.\n                     )\n\ntwo_groups_unpaired.mean_diff.plot(ax=axx.flat[0]);\n\ntwo_groups_paired_baseline.mean_diff.plot(ax=axx.flat[1]);\n\nmulti_2group.mean_diff.plot(ax=axx.flat[2]);\n\nmulti_2group_paired.mean_diff.plot(ax=axx.flat[3]);\n\n\n\n\nIn this case, to access the individual rawdata axes, use name_of_axes to manipulate the rawdata swarmplot axes, and name_of_axes.contrast_axes to gain access to the effect size axes.\n\ntopleft_axes = axx.flat[0]\ntopleft_axes.set_ylabel(\"New y-axis label for rawdata\")\ntopleft_axes.contrast_axes.set_ylabel(\"New y-axis label for effect size\")\n\nText(638.7222222222223, 0.5, 'New y-axis label for effect size')"
  },
  {
    "objectID": "tutorials/04-mini_meta_delta.html",
    "href": "tutorials/04-mini_meta_delta.html",
    "title": "Mini-Meta Delta",
    "section": "",
    "text": "When scientists perform replicates of the same experiment, the effect size of each replicate often varies, which complicates interpretation of the results. As of v2023.02.14, DABEST can now compute the meta-analyzed weighted effect size given multiple replicates of the same experiment. This can help resolve differences between replicates and simplify interpretation.\nThis function uses the generic inverse-variance method to calculate the effect size, as follows:\n\\(\\theta_{\\text{weighted}} = \\frac{\\Sigma\\hat{\\theta_{i}}w_{i}}{{\\Sigma}w_{i}}\\)\nwhere:\n\\(\\hat{\\theta_{i}} = \\text{Mean difference for replicate }i\\)\n\\(w_{i} = \\text{Weight for replicate }i = \\frac{1}{s_{i}^2}\\)\n\\(s_{i}^2 = \\text{Pooled variance for replicate }i = \\frac{(n_{test}-1)s_{test}^2+(n_{control}-1)s_{control}^2}{n_{test}+n_{control}-2}\\)\n\\(n = \\text{sample size and }s^2 = \\text{variance for control/test.}\\)\nNote that this uses the fixed-effects model of meta-analysis, as opposed to the random-effects model; that is to say, all variation between the results of each replicate is assumed to be due solely to sampling error. We thus recommend that this function only be used for replications of the same experiment, i.e. situations where it can be safely assumed that each replicate estimates the same population mean \\(\\mu\\).\nAlso note that as of v2023.02.14, DABEST can only compute weighted effect size for mean difference only, and not standardized measures such as Cohen’s d.\nFor more information on meta-analysis, please refer to Chapter 10 of the Cochrane handbook: https://training.cochrane.org/handbook/current/chapter-10"
  },
  {
    "objectID": "tutorials/04-mini_meta_delta.html#load-libraries",
    "href": "tutorials/04-mini_meta_delta.html#load-libraries",
    "title": "Mini-Meta Delta",
    "section": "Load Libraries",
    "text": "Load Libraries\n\nimport numpy as np\nimport pandas as pd\nimport dabest\n\nprint(\"We're using DABEST v{}\".format(dabest.__version__))\n\nWe're using DABEST v2023.02.14"
  },
  {
    "objectID": "tutorials/04-mini_meta_delta.html#create-dataset-for-mini-meta-demo",
    "href": "tutorials/04-mini_meta_delta.html#create-dataset-for-mini-meta-demo",
    "title": "Mini-Meta Delta",
    "section": "Create dataset for mini-meta demo",
    "text": "Create dataset for mini-meta demo\nWe will now create a dataset to demonstrate the mini-meta function.\n\nfrom scipy.stats import norm # Used in generation of populations.\n\nnp.random.seed(9999) # Fix the seed so the results are replicable.\n# pop_size = 10000 # Size of each population.\nNs = 20 # The number of samples taken from each population\n\n# Create samples\nc1 = norm.rvs(loc=3, scale=0.4, size=Ns)\nc2 = norm.rvs(loc=3.5, scale=0.75, size=Ns)\nc3 = norm.rvs(loc=3.25, scale=0.4, size=Ns)\n\nt1 = norm.rvs(loc=3.5, scale=0.5, size=Ns)\nt2 = norm.rvs(loc=2.5, scale=0.6, size=Ns)\nt3 = norm.rvs(loc=3, scale=0.75, size=Ns)\n\n\n# Add a `gender` column for coloring the data.\nfemales = np.repeat('Female', Ns/2).tolist()\nmales = np.repeat('Male', Ns/2).tolist()\ngender = females + males\n\n# Add an `id` column for paired data plotting.\nid_col = pd.Series(range(1, Ns+1))\n\n# Combine samples and gender into a DataFrame.\ndf = pd.DataFrame({'Control 1' : c1,     'Test 1' : t1,\n                   'Control 2' : c2,     'Test 2' : t2,\n                   'Control 3' : c3,     'Test 3' : t3,\n                   'Gender'    : gender, 'ID'  : id_col\n                  })\n\nWe now have 3 Control and 3 Test groups, simulating 3 replicates of the same experiment. Our dataset also has a non-numerical column indicating gender, and another column indicating the identity of each observation.\nThis is known as a ‘wide’ dataset. See this writeup for more details.\n\ndf.head()\n\n\n\n\n\n\n\n\nControl 1\nTest 1\nControl 2\nTest 2\nControl 3\nTest 3\nGender\nID\n\n\n\n\n0\n2.793984\n3.420875\n3.324661\n1.707467\n3.816940\n1.796581\nFemale\n1\n\n\n1\n3.236759\n3.467972\n3.685186\n1.121846\n3.750358\n3.944566\nFemale\n2\n\n\n2\n3.019149\n4.377179\n5.616891\n3.301381\n2.945397\n2.832188\nFemale\n3\n\n\n3\n2.804638\n4.564780\n2.773152\n2.534018\n3.575179\n3.048267\nFemale\n4\n\n\n4\n2.858019\n3.220058\n2.550361\n2.796365\n3.692138\n3.276575\nFemale\n5"
  },
  {
    "objectID": "tutorials/04-mini_meta_delta.html#loading-data",
    "href": "tutorials/04-mini_meta_delta.html#loading-data",
    "title": "Mini-Meta Delta",
    "section": "Loading Data",
    "text": "Loading Data\nNext, we load data as we would normally using dabest.load(). This time, however, we also specify the argument mini_meta=True. As we are loading three experiments’ worth of data, idx is passed as a tuple of tuples, as follows:\n\nunpaired = dabest.load(df, idx=((\"Control 1\", \"Test 1\"), (\"Control 2\", \"Test 2\"), (\"Control 3\", \"Test 3\")), mini_meta=True)\n\nWhen this Dabest object is called, it should show that effect sizes will be calculated for each group, as well as the weighted delta. Note once again that weighted delta will only be calcuated for mean difference.\n\nunpaired\n\nDABEST v2023.02.14\n==================\n                  \nGood evening!\nThe current time is Sun Mar 19 22:59:33 2023.\n\nEffect size(s) with 95% confidence intervals will be computed for:\n1. Test 1 minus Control 1\n2. Test 2 minus Control 2\n3. Test 3 minus Control 3\n4. weighted delta (only for mean difference)\n\n5000 resamples will be used to generate the effect size bootstraps.\n\n\nBy calling the mean_diff attribute, you can view the mean differences for each group as well as the weighted delta.\n\nunpaired.mean_diff\n\nDABEST v2023.02.14\n==================\n                  \nGood evening!\nThe current time is Sun Mar 19 22:59:27 2023.\n\nThe unpaired mean difference between Control 1 and Test 1 is 0.48 [95%CI 0.221, 0.768].\nThe p-value of the two-sided permutation t-test is 0.001, calculated for legacy purposes only. \n\nThe unpaired mean difference between Control 2 and Test 2 is -1.38 [95%CI -1.93, -0.895].\nThe p-value of the two-sided permutation t-test is 0.0, calculated for legacy purposes only. \n\nThe unpaired mean difference between Control 3 and Test 3 is -0.255 [95%CI -0.717, 0.196].\nThe p-value of the two-sided permutation t-test is 0.293, calculated for legacy purposes only. \n\nThe weighted-average unpaired mean differences is -0.0104 [95%CI -0.222, 0.215].\nThe p-value of the two-sided permutation t-test is 0.937, calculated for legacy purposes only. \n\n5000 bootstrap samples were taken; the confidence interval is bias-corrected and accelerated.\nAny p-value reported is the probability of observing theeffect size (or greater),\nassuming the null hypothesis ofzero difference is true.\nFor each p-value, 5000 reshuffles of the control and test labels were performed.\n\nTo get the results of all valid statistical tests, use `.mean_diff.statistical_tests`\n\n\nYou can view the details of each experiment by accessing .mean_diff.results, as follows.\n\npd.options.display.max_columns = 50\nunpaired.mean_diff.results\n\n\n\n\n\n\n\n\ncontrol\ntest\ncontrol_N\ntest_N\neffect_size\nis_paired\ndifference\nci\nbca_low\nbca_high\nbca_interval_idx\npct_low\npct_high\npct_interval_idx\nbootstraps\nresamples\nrandom_seed\npermutations\npvalue_permutation\npermutation_count\npermutations_var\npvalue_welch\nstatistic_welch\npvalue_students_t\nstatistic_students_t\npvalue_mann_whitney\nstatistic_mann_whitney\n\n\n\n\n0\nControl 1\nTest 1\n20\n20\nmean difference\nNone\n0.480290\n95\n0.220869\n0.767721\n(140, 4889)\n0.215697\n0.761716\n(125, 4875)\n[0.6686169333655454, 0.4382051534234943, 0.665...\n5000\n12345\n[-0.17259843762502491, 0.03802293852634886, -0...\n0.0010\n5000\n[0.026356588154404337, 0.027102495439046997, 0...\n0.002094\n-3.308806\n0.002057\n-3.308806\n0.001625\n83.0\n\n\n1\nControl 2\nTest 2\n20\n20\nmean difference\nNone\n-1.381085\n95\n-1.925232\n-0.894537\n(108, 4857)\n-1.903964\n-0.875420\n(125, 4875)\n[-1.1603841133810318, -1.6359724856206515, -1....\n5000\n12345\n[0.015164519971271773, 0.017231919606192303, -...\n0.0000\n5000\n[0.12241741427801064, 0.12241565174150129, 0.1...\n0.000011\n5.138840\n0.000009\n5.138840\n0.000026\n356.0\n\n\n2\nControl 3\nTest 3\n20\n20\nmean difference\nNone\n-0.254831\n95\n-0.717337\n0.196121\n(115, 4864)\n-0.710346\n0.206131\n(125, 4875)\n[-0.09556572841011901, 0.35166073097757433, -0...\n5000\n12345\n[-0.05901068591042824, -0.13617667681797307, 0...\n0.2934\n5000\n[0.058358897501663703, 0.05796253365278035, 0....\n0.294766\n1.069798\n0.291459\n1.069798\n0.285305\n240.0\n\n\n\n\n\n\n\nNote, however, that this does not contain the relevant information for our weighted delta. The details of the weighted delta are stored as attributes of the mini_meta_delta object, for example:\n\ngroup_var: the pooled group variances of each set of 2 experiment groups\ndifference: the weighted mean difference calculated based on the raw data\nbootstraps: the deltas of each set of 2 experiment groups calculated based on the bootstraps\nbootstraps_weighted_delta: the weighted deltas calculated based on the bootstraps\npermutations: the deltas of each set of 2 experiment groups calculated based on the permutation data\npermutations_var: the pooled group variances of each set of 2 experiment groups calculated based on permutation data\npermutations_weighted_delta: the weighted deltas calculated based on the permutation data\n\nYou can call each of the above attributes on their own:\n\nunpaired.mean_diff.mini_meta_delta.difference\n\n-0.010352287701068538\n\n\nAttributes of the weighted delta can also be written to a dict by using the .to_dict() function. Below, we do this and subsequently convert the dict into a dataframe for better readability:\n\nweighted_delta_details = unpaired.mean_diff.mini_meta_delta.to_dict()\nweighted_delta_df = pd.DataFrame.from_dict(weighted_delta_details, orient = 'index')\nweighted_delta_df\n\n\n\n\n\n\n\n\n0\n\n\n\n\nacceleration_value\n0.000193\n\n\nalpha\n0.05\n\n\nbca_high\n0.215037\n\n\nbca_interval_idx\n(128, 4878)\n\n\nbca_low\n-0.221666\n\n\nbias_correction\n0.005013\n\n\nbootstraps\n[[0.6686169333655454, 0.4382051534234943, 0.66...\n\n\nbootstraps_weighted_delta\n[0.1771640316740503, 0.055052653330973, 0.1635...\n\n\nci\n95\n\n\ncontrol\n[Control 1, Control 2, Control 3]\n\n\ncontrol_N\n[20, 20, 20]\n\n\ncontrol_var\n[0.17628013404546256, 0.9584767911266554, 0.16...\n\n\ndifference\n-0.010352\n\n\ngroup_var\n[0.021070042637349427, 0.07222883451891535, 0....\n\n\njackknives\n[-0.008668330406027464, -0.008643903244926629,...\n\n\npct_high\n0.213769\n\n\npct_interval_idx\n(125, 4875)\n\n\npct_low\n-0.222307\n\n\npermutation_count\n5000\n\n\npermutations\n[[-0.17259843762502491, 0.03802293852634886, -...\n\n\npermutations_var\n[[0.026356588154404337, 0.027102495439046997, ...\n\n\npermutations_weighted_delta\n[-0.11757207833491819, -0.012928679700934625, ...\n\n\npvalue_permutation\n0.9374\n\n\ntest\n[Test 1, Test 2, Test 3]\n\n\ntest_N\n[20, 20, 20]\n\n\ntest_var\n[0.245120718701526, 0.4860998992516514, 0.9667..."
  },
  {
    "objectID": "tutorials/04-mini_meta_delta.html#producing-estimation-plots---unpaired-data",
    "href": "tutorials/04-mini_meta_delta.html#producing-estimation-plots---unpaired-data",
    "title": "Mini-Meta Delta",
    "section": "Producing estimation plots - unpaired data",
    "text": "Producing estimation plots - unpaired data\nSimply passing the .plot() method will produce a Cumming estimation plot showing the data for each experimental replicate as well as the calculated weighted delta.\n\nunpaired.mean_diff.plot()\n\n\n\n\n\n\n\nYou can also hide the weighted delta by passing the argument show_mini_meta=False. In this case, the resulting graph would be identical to a multiple two-groups plot:\n\nunpaired.mean_diff.plot(show_mini_meta=False)"
  },
  {
    "objectID": "tutorials/04-mini_meta_delta.html#producing-estimation-plots---paired-data",
    "href": "tutorials/04-mini_meta_delta.html#producing-estimation-plots---paired-data",
    "title": "Mini-Meta Delta",
    "section": "Producing estimation plots - paired data",
    "text": "Producing estimation plots - paired data\nThe tutorial up to this point has dealt with unpaired data. If your data is paired data, the process for loading, plotting and accessing the data is the same as for unpaired data, except the argument paired = \"sequential\" or \"baseline\" and an appropriate id_col are passed during the dabest.load() step, as follows:\n\npaired = dabest.load(df, idx=((\"Control 1\", \"Test 1\"), (\"Control 2\", \"Test 2\"), (\"Control 3\", \"Test 3\")), mini_meta=True, id_col=\"ID\", paired=\"baseline\")\npaired.mean_diff.plot()"
  },
  {
    "objectID": "tutorials/index.html",
    "href": "tutorials/index.html",
    "title": "Tutorials",
    "section": "",
    "text": "Click through to any of these tutorials to get started with dabest’s features.\n\n\n\n\n\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nBasics\n\n\nAn end-to-end tutorial on how to use the dabest.\n\n\n\n\nRepeated Measures\n\n\nExplanation of how to use dabest for repeated measures analysis.\n\n\n\n\nProportion Plots\n\n\nA guide to plot proportion plot with binary data.\n\n\n\n\nMini-Meta Delta\n\n\nExplanation of how to compute the meta-analyzed weighted effect size using dabest.\n\n\n\n\nControlling Plot Aesthetics\n\n\n\n\n\n\n\nDelta - Delta\n\n\nExplanation of how to calculate delta-delta using dabest.\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "tutorials/01-basics.html",
    "href": "tutorials/01-basics.html",
    "title": "Basics",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport dabest\n\nprint(\"We're using DABEST v{}\".format(dabest.__version__))\n\nWe're using DABEST v2023.02.14"
  },
  {
    "objectID": "tutorials/01-basics.html#load-libraries",
    "href": "tutorials/01-basics.html#load-libraries",
    "title": "Basics",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport dabest\n\nprint(\"We're using DABEST v{}\".format(dabest.__version__))\n\nWe're using DABEST v2023.02.14"
  },
  {
    "objectID": "tutorials/01-basics.html#create-dataset-for-demo",
    "href": "tutorials/01-basics.html#create-dataset-for-demo",
    "title": "Basics",
    "section": "Create dataset for demo",
    "text": "Create dataset for demo\nHere, we create a dataset to illustrate how dabest functions. In this dataset, each column corresponds to a group of observations.\n\nfrom scipy.stats import norm # Used in generation of populations.\n\nnp.random.seed(9999) # Fix the seed so the results are replicable.\n# pop_size = 10000 # Size of each population.\nNs = 20 # The number of samples taken from each population\n\n# Create samples\nc1 = norm.rvs(loc=3, scale=0.4, size=Ns)\nc2 = norm.rvs(loc=3.5, scale=0.75, size=Ns)\nc3 = norm.rvs(loc=3.25, scale=0.4, size=Ns)\n\nt1 = norm.rvs(loc=3.5, scale=0.5, size=Ns)\nt2 = norm.rvs(loc=2.5, scale=0.6, size=Ns)\nt3 = norm.rvs(loc=3, scale=0.75, size=Ns)\nt4 = norm.rvs(loc=3.5, scale=0.75, size=Ns)\nt5 = norm.rvs(loc=3.25, scale=0.4, size=Ns)\nt6 = norm.rvs(loc=3.25, scale=0.4, size=Ns)\n\n\n# Add a `gender` column for coloring the data.\nfemales = np.repeat('Female', Ns/2).tolist()\nmales = np.repeat('Male', Ns/2).tolist()\ngender = females + males\n\n# Add an `id` column for paired data plotting.\nid_col = pd.Series(range(1, Ns+1))\n\n# Combine samples and gender into a DataFrame.\ndf = pd.DataFrame({'Control 1' : c1,     'Test 1' : t1,\n                 'Control 2' : c2,     'Test 2' : t2,\n                 'Control 3' : c3,     'Test 3' : t3,\n                 'Test 4'    : t4,     'Test 5' : t5, 'Test 6' : t6,\n                 'Gender'    : gender, 'ID'  : id_col\n                })\n\nNote that we have 9 groups (3 Control samples and 6 Test samples). Our dataset also has a non-numerical column indicating gender, and another column indicating the identity of each observation.\nThis is known as a ‘wide’ dataset. See this writeup for more details.\n\ndf.head()\n\n\n\n\n\n\n\n\nControl 1\nTest 1\nControl 2\nTest 2\nControl 3\nTest 3\nTest 4\nTest 5\nTest 6\nGender\nID\n\n\n\n\n0\n2.793984\n3.420875\n3.324661\n1.707467\n3.816940\n1.796581\n4.440050\n2.937284\n3.486127\nFemale\n1\n\n\n1\n3.236759\n3.467972\n3.685186\n1.121846\n3.750358\n3.944566\n3.723494\n2.837062\n2.338094\nFemale\n2\n\n\n2\n3.019149\n4.377179\n5.616891\n3.301381\n2.945397\n2.832188\n3.214014\n3.111950\n3.270897\nFemale\n3\n\n\n3\n2.804638\n4.564780\n2.773152\n2.534018\n3.575179\n3.048267\n4.968278\n3.743378\n3.151188\nFemale\n4\n\n\n4\n2.858019\n3.220058\n2.550361\n2.796365\n3.692138\n3.276575\n2.662104\n2.977341\n2.328601\nFemale\n5"
  },
  {
    "objectID": "tutorials/01-basics.html#loading-data",
    "href": "tutorials/01-basics.html#loading-data",
    "title": "Basics",
    "section": "Loading Data",
    "text": "Loading Data\nBefore we create estimation plots and obtain confidence intervals for our effect sizes, we need to load the data and the relevant groups.\nWe simply supply the DataFrame to dabest.load(). We also must supply the two groups you want to compare in the idx argument as a tuple or list.\n\ntwo_groups_unpaired = dabest.load(df, idx=(\"Control 1\", \"Test 1\"), resamples=5000)\n\nCalling this Dabest object gives you a gentle greeting, as well as the comparisons that can be computed.\n\ntwo_groups_unpaired\n\nDABEST v2023.02.14\n==================\n                  \nGood evening!\nThe current time is Sun Mar 19 22:36:20 2023.\n\nEffect size(s) with 95% confidence intervals will be computed for:\n1. Test 1 minus Control 1\n\n5000 resamples will be used to generate the effect size bootstraps.\n\n\n\nChanging statistical parameters\nYou can change the width of the confidence interval that will be produced by manipulating the ci argument.\n\ntwo_groups_unpaired_ci90 = dabest.load(df, idx=(\"Control 1\", \"Test 1\"), ci=90)\n\n\ntwo_groups_unpaired_ci90\n\nDABEST v2023.02.14\n==================\n                  \nGood evening!\nThe current time is Sun Mar 19 22:36:23 2023.\n\nEffect size(s) with 90% confidence intervals will be computed for:\n1. Test 1 minus Control 1\n\n5000 resamples will be used to generate the effect size bootstraps."
  },
  {
    "objectID": "tutorials/01-basics.html#effect-sizes",
    "href": "tutorials/01-basics.html#effect-sizes",
    "title": "Basics",
    "section": "Effect sizes",
    "text": "Effect sizes\ndabest now features a range of effect sizes: - the mean difference (mean_diff) - the median difference (median_diff) - Cohen’s d ([cohens_d](https://ZHANGROU-99.github.io/DABEST-python/API/effsize.html#cohens_d)) - Hedges’ g ([hedges_g](https://ZHANGROU-99.github.io/DABEST-python/API/effsize.html#hedges_g)) - Cliff’s delta([cliffs_delta](https://ZHANGROU-99.github.io/DABEST-python/API/effsize.html#cliffs_delta))\nEach of these are attributes of the Dabest object.\n\ntwo_groups_unpaired.mean_diff\n\nDABEST v2023.02.14\n==================\n                  \nGood evening!\nThe current time is Sun Mar 19 22:36:25 2023.\n\nThe unpaired mean difference between Control 1 and Test 1 is 0.48 [95%CI 0.221, 0.768].\nThe p-value of the two-sided permutation t-test is 0.001, calculated for legacy purposes only. \n\n5000 bootstrap samples were taken; the confidence interval is bias-corrected and accelerated.\nAny p-value reported is the probability of observing theeffect size (or greater),\nassuming the null hypothesis ofzero difference is true.\nFor each p-value, 5000 reshuffles of the control and test labels were performed.\n\nTo get the results of all valid statistical tests, use `.mean_diff.statistical_tests`\n\n\nFor each comparison, the type of effect size is reported (here, it’s the “unpaired mean difference”). The confidence interval is reported as: [confidenceIntervalWidth LowerBound, UpperBound]\nThis confidence interval is generated through bootstrap resampling. See :doc:bootstraps for more details.\nSince v0.3.0, DABEST will report the p-value of the non-parametric two-sided approximate permutation t-test. This is also known as the Monte Carlo permutation test.\nFor unpaired comparisons, the p-values and test statistics of Welch’s t test, Student’s t test, and Mann-Whitney U test can be found in addition. For paired comparisons, the p-values and test statistics of the paired Student’s t and Wilcoxon tests are presented.\n\npd.options.display.max_columns = 50\ntwo_groups_unpaired.mean_diff.results\n\n\n\n\n\n\n\n\ncontrol\ntest\ncontrol_N\ntest_N\neffect_size\nis_paired\ndifference\nci\nbca_low\nbca_high\nbca_interval_idx\npct_low\npct_high\npct_interval_idx\nbootstraps\nresamples\nrandom_seed\npermutations\npvalue_permutation\npermutation_count\npermutations_var\npvalue_welch\nstatistic_welch\npvalue_students_t\nstatistic_students_t\npvalue_mann_whitney\nstatistic_mann_whitney\n\n\n\n\n0\nControl 1\nTest 1\n20\n20\nmean difference\nNone\n0.48029\n95\n0.220869\n0.767721\n(140, 4889)\n0.215697\n0.761716\n(125, 4875)\n[0.6686169333655454, 0.4382051534234943, 0.665...\n5000\n12345\n[-0.17259843762502491, 0.03802293852634886, -0...\n0.001\n5000\n[0.026356588154404337, 0.027102495439046997, 0...\n0.002094\n-3.308806\n0.002057\n-3.308806\n0.001625\n83.0\n\n\n\n\n\n\n\n\ntwo_groups_unpaired.mean_diff.statistical_tests\n\n\n\n\n\n\n\n\ncontrol\ntest\ncontrol_N\ntest_N\neffect_size\nis_paired\ndifference\nci\nbca_low\nbca_high\npvalue_permutation\npvalue_welch\nstatistic_welch\npvalue_students_t\nstatistic_students_t\npvalue_mann_whitney\nstatistic_mann_whitney\n\n\n\n\n0\nControl 1\nTest 1\n20\n20\nmean difference\nNone\n0.48029\n95\n0.220869\n0.767721\n0.001\n0.002094\n-3.308806\n0.002057\n-3.308806\n0.001625\n83.0\n\n\n\n\n\n\n\nLet’s compute the Hedges’ g for our comparison.\n\ntwo_groups_unpaired.hedges_g\n\nDABEST v2023.02.14\n==================\n                  \nGood evening!\nThe current time is Sun Mar 19 22:36:30 2023.\n\nThe unpaired Hedges' g between Control 1 and Test 1 is 1.03 [95%CI 0.349, 1.62].\nThe p-value of the two-sided permutation t-test is 0.001, calculated for legacy purposes only. \n\n5000 bootstrap samples were taken; the confidence interval is bias-corrected and accelerated.\nAny p-value reported is the probability of observing theeffect size (or greater),\nassuming the null hypothesis ofzero difference is true.\nFor each p-value, 5000 reshuffles of the control and test labels were performed.\n\nTo get the results of all valid statistical tests, use `.hedges_g.statistical_tests`\n\n\n\ntwo_groups_unpaired.hedges_g.results\n\n\n\n\n\n\n\n\ncontrol\ntest\ncontrol_N\ntest_N\neffect_size\nis_paired\ndifference\nci\nbca_low\nbca_high\nbca_interval_idx\npct_low\npct_high\npct_interval_idx\nbootstraps\nresamples\nrandom_seed\npermutations\npvalue_permutation\npermutation_count\npermutations_var\npvalue_welch\nstatistic_welch\npvalue_students_t\nstatistic_students_t\npvalue_mann_whitney\nstatistic_mann_whitney\n\n\n\n\n0\nControl 1\nTest 1\n20\n20\nHedges' g\nNone\n1.025525\n95\n0.349394\n1.618579\n(42, 4724)\n0.472844\n1.74166\n(125, 4875)\n[1.1337301267831184, 0.8311210968422604, 1.539...\n5000\n12345\n[-0.3295089865590538, 0.07158401210924781, -0....\n0.001\n5000\n[0.026356588154404337, 0.027102495439046997, 0...\n0.002094\n-3.308806\n0.002057\n-3.308806\n0.001625\n83.0"
  },
  {
    "objectID": "tutorials/01-basics.html#producing-estimation-plots",
    "href": "tutorials/01-basics.html#producing-estimation-plots",
    "title": "Basics",
    "section": "Producing estimation plots",
    "text": "Producing estimation plots\nTo produce a Gardner-Altman estimation plot, simply use the .plot() method. You can read more about its genesis and design inspiration at :doc:robust-beautiful.\nEvery effect size instance has access to the .plot() method. This means you can quickly create plots for different effect sizes easily.\n\ntwo_groups_unpaired.mean_diff.plot();\n\n\n\n\n\ntwo_groups_unpaired.hedges_g.plot();\n\n\n\n\nInstead of a Gardner-Altman plot, you can produce a Cumming estimation plot by setting float_contrast=False in the plot() method. This will plot the bootstrap effect sizes below the raw data, and also displays the the mean (gap) and ± standard deviation of each group (vertical ends) as gapped lines. This design was inspired by Edward Tufte’s dictum to maximise the data-ink ratio.\n\ntwo_groups_unpaired.hedges_g.plot(float_contrast=False);\n\n\n\n\nThe dabest package also implements a range of estimation plot designs aimed at depicting common experimental designs.\nThe multi-two-group estimation plot tiles two or more Cumming plots horizontally, and is created by passing a nested tuple to idx when dabest.load() is first invoked.\nThus, the lower axes in the Cumming plot is effectively a forest plot, used in meta-analyses to aggregate and compare data from different experiments.\n\nmulti_2group = dabest.load(df, idx=((\"Control 1\", \"Test 1\",),\n                                         (\"Control 2\", \"Test 2\")\n                                       ))\n\nmulti_2group.mean_diff.plot();\n\n\n\n\nThe shared control plot displays another common experimental paradigm, where several test samples are compared against a common reference sample.\nThis type of Cumming plot is automatically generated if the tuple passed to idx has more than two data columns.\n\nshared_control = dabest.load(df, idx=(\"Control 1\", \"Test 1\",\n                                          \"Test 2\", \"Test 3\",\n                                          \"Test 4\", \"Test 5\", \"Test 6\")\n                                 )\n\n\nshared_control\n\nDABEST v2023.02.14\n==================\n                  \nGood evening!\nThe current time is Sun Mar 19 22:36:43 2023.\n\nEffect size(s) with 95% confidence intervals will be computed for:\n1. Test 1 minus Control 1\n2. Test 2 minus Control 1\n3. Test 3 minus Control 1\n4. Test 4 minus Control 1\n5. Test 5 minus Control 1\n6. Test 6 minus Control 1\n\n5000 resamples will be used to generate the effect size bootstraps.\n\n\n\nshared_control.mean_diff\n\nDABEST v2023.02.14\n==================\n                  \nGood evening!\nThe current time is Sun Mar 19 22:36:48 2023.\n\nThe unpaired mean difference between Control 1 and Test 1 is 0.48 [95%CI 0.221, 0.768].\nThe p-value of the two-sided permutation t-test is 0.001, calculated for legacy purposes only. \n\nThe unpaired mean difference between Control 1 and Test 2 is -0.542 [95%CI -0.914, -0.211].\nThe p-value of the two-sided permutation t-test is 0.0042, calculated for legacy purposes only. \n\nThe unpaired mean difference between Control 1 and Test 3 is 0.174 [95%CI -0.295, 0.628].\nThe p-value of the two-sided permutation t-test is 0.479, calculated for legacy purposes only. \n\nThe unpaired mean difference between Control 1 and Test 4 is 0.79 [95%CI 0.306, 1.31].\nThe p-value of the two-sided permutation t-test is 0.0042, calculated for legacy purposes only. \n\nThe unpaired mean difference between Control 1 and Test 5 is 0.265 [95%CI 0.0137, 0.497].\nThe p-value of the two-sided permutation t-test is 0.0404, calculated for legacy purposes only. \n\nThe unpaired mean difference between Control 1 and Test 6 is 0.288 [95%CI -0.00441, 0.515].\nThe p-value of the two-sided permutation t-test is 0.0324, calculated for legacy purposes only. \n\n5000 bootstrap samples were taken; the confidence interval is bias-corrected and accelerated.\nAny p-value reported is the probability of observing theeffect size (or greater),\nassuming the null hypothesis ofzero difference is true.\nFor each p-value, 5000 reshuffles of the control and test labels were performed.\n\nTo get the results of all valid statistical tests, use `.mean_diff.statistical_tests`\n\n\n\nshared_control.mean_diff.plot();\n\n\n\n\ndabest thus empowers you to robustly perform and elegantly present complex visualizations and statistics.\n\nmulti_groups = dabest.load(df, idx=((\"Control 1\", \"Test 1\",),\n                                         (\"Control 2\", \"Test 2\",\"Test 3\"),\n                                         (\"Control 3\", \"Test 4\",\"Test 5\", \"Test 6\")\n                                       ))\n\n\nmulti_groups\n\nDABEST v2023.02.14\n==================\n                  \nGood evening!\nThe current time is Sun Mar 19 22:36:56 2023.\n\nEffect size(s) with 95% confidence intervals will be computed for:\n1. Test 1 minus Control 1\n2. Test 2 minus Control 2\n3. Test 3 minus Control 2\n4. Test 4 minus Control 3\n5. Test 5 minus Control 3\n6. Test 6 minus Control 3\n\n5000 resamples will be used to generate the effect size bootstraps.\n\n\n\nmulti_groups.mean_diff\n\nDABEST v2023.02.14\n==================\n                  \nGood evening!\nThe current time is Sun Mar 19 22:37:01 2023.\n\nThe unpaired mean difference between Control 1 and Test 1 is 0.48 [95%CI 0.221, 0.768].\nThe p-value of the two-sided permutation t-test is 0.001, calculated for legacy purposes only. \n\nThe unpaired mean difference between Control 2 and Test 2 is -1.38 [95%CI -1.93, -0.895].\nThe p-value of the two-sided permutation t-test is 0.0, calculated for legacy purposes only. \n\nThe unpaired mean difference between Control 2 and Test 3 is -0.666 [95%CI -1.3, -0.103].\nThe p-value of the two-sided permutation t-test is 0.0352, calculated for legacy purposes only. \n\nThe unpaired mean difference between Control 3 and Test 4 is 0.362 [95%CI -0.114, 0.887].\nThe p-value of the two-sided permutation t-test is 0.161, calculated for legacy purposes only. \n\nThe unpaired mean difference between Control 3 and Test 5 is -0.164 [95%CI -0.404, 0.0742].\nThe p-value of the two-sided permutation t-test is 0.208, calculated for legacy purposes only. \n\nThe unpaired mean difference between Control 3 and Test 6 is -0.14 [95%CI -0.398, 0.102].\nThe p-value of the two-sided permutation t-test is 0.282, calculated for legacy purposes only. \n\n5000 bootstrap samples were taken; the confidence interval is bias-corrected and accelerated.\nAny p-value reported is the probability of observing theeffect size (or greater),\nassuming the null hypothesis ofzero difference is true.\nFor each p-value, 5000 reshuffles of the control and test labels were performed.\n\nTo get the results of all valid statistical tests, use `.mean_diff.statistical_tests`\n\n\n\nmulti_groups.mean_diff.plot();\n\n\n\n\n\nUsing long (aka ‘melted’) data frames\ndabest can also work with ‘melted’ or ‘long’ data. This term is so used because each row will now correspond to a single datapoint, with one column carrying the value and other columns carrying ‘metadata’ describing that datapoint.\nMore details on wide vs long or ‘melted’ data can be found in this Wikipedia article. The pandas documentation gives recipes for melting dataframes.\n\nx='group'\ny='metric'\n\nvalue_cols = df.columns[:-2] # select all but the \"Gender\" and \"ID\" columns.\n\ndf_melted = pd.melt(df.reset_index(),\n                    id_vars=[\"Gender\", \"ID\"],\n                    value_vars=value_cols,\n                    value_name=y,\n                    var_name=x)\n\ndf_melted.head() # Gives the first five rows of `df_melted`.\n\n\n\n\n\n\n\n\nGender\nID\ngroup\nmetric\n\n\n\n\n0\nFemale\n1\nControl 1\n2.793984\n\n\n1\nFemale\n2\nControl 1\n3.236759\n\n\n2\nFemale\n3\nControl 1\n3.019149\n\n\n3\nFemale\n4\nControl 1\n2.804638\n\n\n4\nFemale\n5\nControl 1\n2.858019\n\n\n\n\n\n\n\nWhen your data is in this format, you will need to specify the x and y columns in dabest.load().\n\nanalysis_of_long_df = dabest.load(df_melted, idx=(\"Control 1\", \"Test 1\"),\n                                     x=\"group\", y=\"metric\")\n\nanalysis_of_long_df\n\nDABEST v2023.02.14\n==================\n                  \nGood evening!\nThe current time is Sun Mar 19 22:37:07 2023.\n\nEffect size(s) with 95% confidence intervals will be computed for:\n1. Test 1 minus Control 1\n\n5000 resamples will be used to generate the effect size bootstraps.\n\n\n\nanalysis_of_long_df.mean_diff.plot();"
  },
  {
    "objectID": "tutorials/02-repeated_measures.html",
    "href": "tutorials/02-repeated_measures.html",
    "title": "Repeated Measures",
    "section": "",
    "text": "DABEST version 2023.02.14 expands the repertoire of plots for experiments with repeated-measures designs. DABEST now allows the visualization of paired experiments with one control and multiple test groups, as well as repeated measurements of the same group. This is an improved version of paired data plotting in previous versions, which only supported computations involving one test group and one control group.\nThe repeated-measures function supports the calculation of effect sizes for paired data, either based on sequential comparisons (group i vs group i + 1) or baseline comparisons (control vs group i). To use these features, you can simply declare the argument paired = \"sequential\" or paired = \"baseline\" correspondingly while running dabest.load(). As in the previous version, you must also pass a column in the dataset that indicates the identity of each observation, using the id_col keyword.\n(Please note that paired = True and paired = False are no longer valid in v2023.02.14)"
  },
  {
    "objectID": "tutorials/02-repeated_measures.html#load-libraries",
    "href": "tutorials/02-repeated_measures.html#load-libraries",
    "title": "Repeated Measures",
    "section": "Load Libraries",
    "text": "Load Libraries\n\nimport numpy as np\nimport pandas as pd\nimport dabest\n\nprint(\"We're using DABEST v{}\".format(dabest.__version__))\n\nWe're using DABEST v2023.02.14"
  },
  {
    "objectID": "tutorials/02-repeated_measures.html#create-dataset-for-demo",
    "href": "tutorials/02-repeated_measures.html#create-dataset-for-demo",
    "title": "Repeated Measures",
    "section": "Create dataset for demo",
    "text": "Create dataset for demo\n\nfrom scipy.stats import norm # Used in generation of populations.\n\nnp.random.seed(9999) # Fix the seed so the results are replicable.\n# pop_size = 10000 # Size of each population.\nNs = 20 # The number of samples taken from each population\n\n# Create samples\nc1 = norm.rvs(loc=3, scale=0.4, size=Ns)\nc2 = norm.rvs(loc=3.5, scale=0.75, size=Ns)\nc3 = norm.rvs(loc=3.25, scale=0.4, size=Ns)\n\nt1 = norm.rvs(loc=3.5, scale=0.5, size=Ns)\nt2 = norm.rvs(loc=2.5, scale=0.6, size=Ns)\nt3 = norm.rvs(loc=3, scale=0.75, size=Ns)\nt4 = norm.rvs(loc=3.5, scale=0.75, size=Ns)\nt5 = norm.rvs(loc=3.25, scale=0.4, size=Ns)\nt6 = norm.rvs(loc=3.25, scale=0.4, size=Ns)\n\n\n# Add a `gender` column for coloring the data.\nfemales = np.repeat('Female', Ns/2).tolist()\nmales = np.repeat('Male', Ns/2).tolist()\ngender = females + males\n\n# Add an `id` column for paired data plotting.\nid_col = pd.Series(range(1, Ns+1))\n\n# Combine samples and gender into a DataFrame.\ndf = pd.DataFrame({'Control 1' : c1,     'Test 1' : t1,\n                   'Control 2' : c2,     'Test 2' : t2,\n                   'Control 3' : c3,     'Test 3' : t3,\n                   'Test 4'    : t4,     'Test 5' : t5, 'Test 6' : t6,\n                   'Gender'    : gender, 'ID'  : id_col\n                  })\n\n\ntwo_groups_paired_sequential = dabest.load(df, idx=(\"Control 1\", \"Test 1\"),\n                                               paired=\"sequential\", id_col=\"ID\")\n\n\ntwo_groups_paired_sequential\n\nDABEST v2023.02.14\n==================\n                  \nGood evening!\nThe current time is Sun Mar 19 22:39:03 2023.\n\nPaired effect size(s) for the sequential design of repeated-measures experiment \nwith 95% confidence intervals will be computed for:\n1. Test 1 minus Control 1\n\n5000 resamples will be used to generate the effect size bootstraps.\n\n\n\ntwo_groups_paired_baseline = dabest.load(df, idx=(\"Control 1\", \"Test 1\"),\n                                  paired=\"baseline\", id_col=\"ID\")\n\n\ntwo_groups_paired_baseline\n\nDABEST v2023.02.14\n==================\n                  \nGood evening!\nThe current time is Sun Mar 19 22:39:04 2023.\n\nPaired effect size(s) for repeated measures against baseline \nwith 95% confidence intervals will be computed for:\n1. Test 1 minus Control 1\n\n5000 resamples will be used to generate the effect size bootstraps.\n\n\nWhen only 2 paired data groups are involved, assigning either baseline or sequential to paired will give you the same numerical results.\n\ntwo_groups_paired_sequential.mean_diff\n\nDABEST v2023.02.14\n==================\n                  \nGood evening!\nThe current time is Sun Mar 19 22:39:08 2023.\n\nThe paired mean difference for the sequential design of repeated-measures experiment \nbetween Control 1 and Test 1 is 0.48 [95%CI 0.237, 0.73].\nThe p-value of the two-sided permutation t-test is 0.001, calculated for legacy purposes only. \n\n5000 bootstrap samples were taken; the confidence interval is bias-corrected and accelerated.\nAny p-value reported is the probability of observing theeffect size (or greater),\nassuming the null hypothesis ofzero difference is true.\nFor each p-value, 5000 reshuffles of the control and test labels were performed.\n\nTo get the results of all valid statistical tests, use `.mean_diff.statistical_tests`\n\n\n\ntwo_groups_paired_baseline.mean_diff\n\nDABEST v2023.02.14\n==================\n                  \nGood evening!\nThe current time is Sun Mar 19 22:39:09 2023.\n\nThe paired mean difference for repeated measures against baseline \nbetween Control 1 and Test 1 is 0.48 [95%CI 0.237, 0.73].\nThe p-value of the two-sided permutation t-test is 0.001, calculated for legacy purposes only. \n\n5000 bootstrap samples were taken; the confidence interval is bias-corrected and accelerated.\nAny p-value reported is the probability of observing theeffect size (or greater),\nassuming the null hypothesis ofzero difference is true.\nFor each p-value, 5000 reshuffles of the control and test labels were performed.\n\nTo get the results of all valid statistical tests, use `.mean_diff.statistical_tests`\n\n\nFor paired data, we use slopegraphs (another innovation from Edward Tufte) to connect paired observations. Both Gardner-Altman and Cumming plots support this.\n\ntwo_groups_paired_sequential.mean_diff.plot();\n\n\n\n\n\ntwo_groups_paired_sequential.mean_diff.plot(float_contrast=False);\n\n\n\n\n\ntwo_groups_paired_baseline.mean_diff.plot();\n\n\n\n\n\ntwo_groups_paired_baseline.mean_diff.plot(float_contrast=False);\n\n\n\n\nYou can also create repeated-measures plots with multiple test groups.In this case, declaring paired to be sequential or baseline will generate different results.\n\nsequential_repeated_measures = dabest.load(df, idx=(\"Control 1\", \"Test 1\", \"Test 2\", \"Test 3\"),\n                                               paired=\"sequential\", id_col=\"ID\")\n\n\nsequential_repeated_measures.mean_diff\n\nDABEST v2023.02.14\n==================\n                  \nGood evening!\nThe current time is Sun Mar 19 22:39:18 2023.\n\nThe paired mean difference for the sequential design of repeated-measures experiment \nbetween Control 1 and Test 1 is 0.48 [95%CI 0.237, 0.73].\nThe p-value of the two-sided permutation t-test is 0.001, calculated for legacy purposes only. \n\nThe paired mean difference for the sequential design of repeated-measures experiment \nbetween Test 1 and Test 2 is -1.02 [95%CI -1.36, -0.716].\nThe p-value of the two-sided permutation t-test is 0.0, calculated for legacy purposes only. \n\nThe paired mean difference for the sequential design of repeated-measures experiment \nbetween Test 2 and Test 3 is 0.716 [95%CI 0.14, 1.22].\nThe p-value of the two-sided permutation t-test is 0.022, calculated for legacy purposes only. \n\n5000 bootstrap samples were taken; the confidence interval is bias-corrected and accelerated.\nAny p-value reported is the probability of observing theeffect size (or greater),\nassuming the null hypothesis ofzero difference is true.\nFor each p-value, 5000 reshuffles of the control and test labels were performed.\n\nTo get the results of all valid statistical tests, use `.mean_diff.statistical_tests`\n\n\n\nsequential_repeated_measures.mean_diff.plot();\n\n\n\n\n\nbaseline_repeated_measures = dabest.load(df, idx=(\"Control 1\", \"Test 1\", \"Test 2\", \"Test 3\"),\n                                               paired=\"baseline\", id_col=\"ID\")\n\n\nbaseline_repeated_measures.mean_diff\n\nDABEST v2023.02.14\n==================\n                  \nGood evening!\nThe current time is Sun Mar 19 22:39:26 2023.\n\nThe paired mean difference for repeated measures against baseline \nbetween Control 1 and Test 1 is 0.48 [95%CI 0.237, 0.73].\nThe p-value of the two-sided permutation t-test is 0.001, calculated for legacy purposes only. \n\nThe paired mean difference for repeated measures against baseline \nbetween Control 1 and Test 2 is -0.542 [95%CI -0.975, -0.198].\nThe p-value of the two-sided permutation t-test is 0.014, calculated for legacy purposes only. \n\nThe paired mean difference for repeated measures against baseline \nbetween Control 1 and Test 3 is 0.174 [95%CI -0.297, 0.706].\nThe p-value of the two-sided permutation t-test is 0.505, calculated for legacy purposes only. \n\n5000 bootstrap samples were taken; the confidence interval is bias-corrected and accelerated.\nAny p-value reported is the probability of observing theeffect size (or greater),\nassuming the null hypothesis ofzero difference is true.\nFor each p-value, 5000 reshuffles of the control and test labels were performed.\n\nTo get the results of all valid statistical tests, use `.mean_diff.statistical_tests`\n\n\n\nbaseline_repeated_measures.mean_diff.plot();\n\n\n\n\nSame as that for unpaired data, DABEST empowers you to perform complex visualizations and statistics for paired data as well.\n\nmulti_baseline_repeated_measures = dabest.load(df, idx=((\"Control 1\", \"Test 1\", \"Test 2\", \"Test 3\"),\n                                                      (\"Control 2\", \"Test 4\", \"Test 5\", \"Test 6\")),\n                                               paired=\"baseline\", id_col=\"ID\")\nmulti_baseline_repeated_measures.mean_diff.plot();"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "DABEST Blog",
    "section": "",
    "text": "Bootstrap Confidence Intervals\n\n\n\n\n\nExplaination of the bootstrap method and its application in hypothesis testing using DABEST.\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nRobust and Beautiful Statistical Visualization\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/bootstraps/bootstraps.html",
    "href": "blog/posts/bootstraps/bootstraps.html",
    "title": "Bootstrap Confidence Intervals",
    "section": "",
    "text": "In a typical scientific experiment, we are interested in two populations (Control and Test), and whether there is a difference between their means \\((\\mu_{Test}-\\mu_{Control})\\)\n\nWe go about this by collecting observations from the control population, and from the test population.\n\nWe can easily compute the mean difference in our observed samples. This is our estimate of the population effect size that we are interested in.\nBut how do we obtain a measure of precision and confidence about our estimate? Can we get a sense of how it relates to the population mean difference?"
  },
  {
    "objectID": "blog/posts/bootstraps/bootstraps.html#sampling-from-populations",
    "href": "blog/posts/bootstraps/bootstraps.html#sampling-from-populations",
    "title": "Bootstrap Confidence Intervals",
    "section": "",
    "text": "In a typical scientific experiment, we are interested in two populations (Control and Test), and whether there is a difference between their means \\((\\mu_{Test}-\\mu_{Control})\\)\n\nWe go about this by collecting observations from the control population, and from the test population.\n\nWe can easily compute the mean difference in our observed samples. This is our estimate of the population effect size that we are interested in.\nBut how do we obtain a measure of precision and confidence about our estimate? Can we get a sense of how it relates to the population mean difference?"
  },
  {
    "objectID": "blog/posts/bootstraps/bootstraps.html#the-bootstrap-confidence-interval",
    "href": "blog/posts/bootstraps/bootstraps.html#the-bootstrap-confidence-interval",
    "title": "Bootstrap Confidence Intervals",
    "section": "The bootstrap confidence interval",
    "text": "The bootstrap confidence interval\nWe want to obtain a 95% confidence interval (95% CI) around the our estimate of the mean difference. The 95% indicates that any such confidence interval will capture the population mean difference 95% of the time.\nIn other words, if we repeated our experiment 100 times, gathering 100 independent sets of observations, and computing a 95% confidence interval for the mean difference each time, 95 of these intervals would capture the population mean difference. That is to say, we can be 95% confident the interval contains the true mean of the population.\nWe can calculate the 95% CI of the mean difference with bootstrap resampling\n\nThe bootstrap in action\nThe bootstrap[1] is a simple but powerful technique. It was [first described] (https://projecteuclid.org/euclid.aos/1176344552) by Bradley Efron.\nIt creates multiple resamples (with replacement) from a single set of observations, and computes the effect size of interest on each of these resamples. The bootstrap resamples of the effect size can then be used to determine the 95% CI.\nWith computers, we can perform 5000 resamples very easily.\n\nThe resampling distribution of the difference in means approaches a normal distribution. This is due to the Central Limit Theorem: a large number of independent random samples will approach a normal distribution even if the underlying population is not normally distributed.\nBootstrap resampling gives us two important benefits:\n\nNon-parametric statistical analysis. There is no need to assume that our observations, or the underlying populations, are normally distributed. Thanks to the Central Limit Theorem, the resampling distribution of the effect size will approach a normality.\nEasy construction of the 95% CI from the resampling distribution. For 1000 bootstrap resamples of the mean difference, one can use the 25th value and the 975th value of the ranked differences as boundaries of the 95% confidence interval. (This captures the central 95% of the distribution.) Such an interval construction is known as a percentile interval."
  },
  {
    "objectID": "blog/posts/bootstraps/bootstraps.html#adjusting-for-asymmetrical-resampling-distributions",
    "href": "blog/posts/bootstraps/bootstraps.html#adjusting-for-asymmetrical-resampling-distributions",
    "title": "Bootstrap Confidence Intervals",
    "section": "Adjusting for asymmetrical resampling distributions",
    "text": "Adjusting for asymmetrical resampling distributions\nWhile resampling distributions of the difference in means often have a normal distribution, it is not uncommon to encounter a skewed distribution. Thus, Efron developed the [bias-corrected and accelerated bootstrap] (https://en.wikipedia.org/wiki/Bootstrapping_(statistics)#History) (BCa bootstrap) to account for the skew, and still obtain the central 95% of the distribution.\nDABEST applies the BCa correction to the resampling bootstrap distributions of the effect size."
  },
  {
    "objectID": "blog/posts/bootstraps/bootstraps.html#estimation-plots-incorporate-bootstrap-resampling",
    "href": "blog/posts/bootstraps/bootstraps.html#estimation-plots-incorporate-bootstrap-resampling",
    "title": "Bootstrap Confidence Intervals",
    "section": "Estimation plots incorporate bootstrap resampling",
    "text": "Estimation plots incorporate bootstrap resampling\nThe estimation plot produced by DABEST presents the rawdata and the bootstrap confidence interval of the effect size (the difference in means) side-by-side as a single integrated plot.\n\nIt thus tightly couples visual presentation of the raw data with an indication of the population mean difference, and its confidence interval.\n [1]: The name is derived from the saying “pull oneself by one’s bootstraps”, often used as an exhortation to achieve success without external help."
  },
  {
    "objectID": "blog/posts/robust-beautiful/robust-beautiful.html",
    "href": "blog/posts/robust-beautiful/robust-beautiful.html",
    "title": "Robust and Beautiful Statistical Visualization",
    "section": "",
    "text": "What is data visualization? Battle-Baptiste and Rusert (2018) give a cogent and compelling definition:\nData visualization[1] is the rendering of information in a visual format to help communicate data while also generating new patterns and knowledge through the act of visualization itself.\nSadly, too many figures and visualizations in modern academic publications seemingly fail to “generate new patterns and knowledge through the act of visualization itself”. Here, we propose a solution: the estimation plot.\n\n\nBy only displaying the mean and standard deviation, barplots do not accurately represent the underlying distribution of the data.\n\nIn the above figure, four different samples with wildly different distributions–as seen in the swarmplot on the left panel–look exactly the same when visualized with a barplot on the right panel. (You can download the dataset to see for yourself.)\nWe’re not the first ones (see this, this, or that) to point out the barplot’s fatal flaws. Indeed, it is both sobering and fascinating to realise that the barplot is a 17th century invention initially used to compare single values, not to compare summarized and aggregated data.\n\n\n\nBoxplots are another widely used visualization tool. They arguably do include more information for each sample (medians, quartiles, maxima, minima, and outliers), but they do not convey to the viewer the size of each sample.\n\nThe figure above visualizes the same four samples as a swarmplot (left panel) and as a boxplot. If we did not label the x-axis with the sample size, it would be impossible to definitively distinguish the sample with 5 obesrvations from the sample with 50.\nEven if the world gets rid of barplots and boxplots, the problems plaguing statistical practices will remain unsolved. Null-hypothesis significance testing–the dominant statistical paradigm in basic research–does not indicate the effect size, or its confidence interval."
  },
  {
    "objectID": "blog/posts/robust-beautiful/robust-beautiful.html#current-plots-do-not-work",
    "href": "blog/posts/robust-beautiful/robust-beautiful.html#current-plots-do-not-work",
    "title": "Robust and Beautiful Statistical Visualization",
    "section": "",
    "text": "What is data visualization? Battle-Baptiste and Rusert (2018) give a cogent and compelling definition:\nData visualization[1] is the rendering of information in a visual format to help communicate data while also generating new patterns and knowledge through the act of visualization itself.\nSadly, too many figures and visualizations in modern academic publications seemingly fail to “generate new patterns and knowledge through the act of visualization itself”. Here, we propose a solution: the estimation plot.\n\n\nBy only displaying the mean and standard deviation, barplots do not accurately represent the underlying distribution of the data.\n\nIn the above figure, four different samples with wildly different distributions–as seen in the swarmplot on the left panel–look exactly the same when visualized with a barplot on the right panel. (You can download the dataset to see for yourself.)\nWe’re not the first ones (see this, this, or that) to point out the barplot’s fatal flaws. Indeed, it is both sobering and fascinating to realise that the barplot is a 17th century invention initially used to compare single values, not to compare summarized and aggregated data.\n\n\n\nBoxplots are another widely used visualization tool. They arguably do include more information for each sample (medians, quartiles, maxima, minima, and outliers), but they do not convey to the viewer the size of each sample.\n\nThe figure above visualizes the same four samples as a swarmplot (left panel) and as a boxplot. If we did not label the x-axis with the sample size, it would be impossible to definitively distinguish the sample with 5 obesrvations from the sample with 50.\nEven if the world gets rid of barplots and boxplots, the problems plaguing statistical practices will remain unsolved. Null-hypothesis significance testing–the dominant statistical paradigm in basic research–does not indicate the effect size, or its confidence interval."
  },
  {
    "objectID": "blog/posts/robust-beautiful/robust-beautiful.html#introducing-the-estimation-plot",
    "href": "blog/posts/robust-beautiful/robust-beautiful.html#introducing-the-estimation-plot",
    "title": "Robust and Beautiful Statistical Visualization",
    "section": "Introducing the Estimation Plot",
    "text": "Introducing the Estimation Plot\n\nhown above is a Gardner-Altman estimation plot. (The plot draws its name from [Martin J. Gardner] (https://www.independent.co.uk/news/people/obituary-professor-martin-gardner-1470261.html) and Douglas Altman, who are credited with [creating the design] (https://www.bmj.com/content/bmj/292/6522/746.full.pdf) in 1986).\nThis plot has two key features:\n\nIt presents all datapoints as a swarmplot, which orders each point to display the underlying distribution.\nIt presents the effect size as a bootstrap 95% confidence interval (95% CI) on a separate but aligned axes. where the effect size is displayed to the right of the war data, and the mean of the test group is aligned with the effect size.\n\nThus, estimation plots are robust, beautiful, and convey important statistical information elegantly and efficiently.\nAn estimation plot obtains and displays the 95% CI through nonparametric bootstrap resampling. This enables visualization of the confidence interval as a graded sampling distribution.\nThis is one important difference between estimation plots created by DABEST, and the original Gardner-Altman design. Here, the 95% CI is computed through parametric methods, and displayed as a vertical error bar.\nRead more about this technique at bootstraps.\n\nIntroducing Estimation Statistics\nEstimation plots emerge from estimation statistics, a simple framework that avoids the pitfalls of significance testing. It focuses on the effect sizes of one’s experiment/interventions, and uses familiar statistical concepts: means, mean differences, and error bars.\nSignificance testing calculates the probability (the P value) that the experimental data would be observed, if the intervention did not produce a change in the metric measured (i.e. the null hypothesis). This leads analysts to apply a false dichotomy on the experimental intervention.\nEstimation statistics, on the other hand, focuses on the magnitude of the effect (the effect size) and its precision. This encourages analysts to gain a deeper understanding of the metrics used, and how they relate to the natural processes being studied."
  },
  {
    "objectID": "blog/posts/robust-beautiful/robust-beautiful.html#an-estimation-plot-for-every-experimental-design",
    "href": "blog/posts/robust-beautiful/robust-beautiful.html#an-estimation-plot-for-every-experimental-design",
    "title": "Robust and Beautiful Statistical Visualization",
    "section": "An Estimation Plot For Every Experimental Design",
    "text": "An Estimation Plot For Every Experimental Design\nFor each of the most routine significance tests, there is an estimation replacement:\n\nUnpaired Student’s t-test –&gt; Two-group estimation plot\n\n\n\nPaired Student’s t-test –&gt; Paired estimation plot\nThe Gardner-Altman estimation plot can also display effect sizes for repeated measures (aka a paired experimental design) using a Tufte slopegraph instead of a swarmplot.\n\n\n\nOne-way ANOVA + multiple comparisons –&gt; Multi two-group estimation plot\nFor comparisons between 3 or more groups that typically employ analysis of variance (ANOVA) methods, one can use the Cumming estimation plot, named after Geoff Cumming, and draws its design heavily from his 2012 textbook Understanding the New Statistics. This estimation plot design can be considered a variant of the Gardner-Altman plot.\n\nThe effect size and 95% CIs are still plotted a separate axes, but unlike the Gardner-Altman plot, this axes is positioned beneath the raw data.\nSuch a design frees up visual space in the upper panel, allowing the display of summary measurements (mean ± standard deviation) for each group. These are shown as gapped lines to the right of each group. The mean of each group is indicated as a gap in the line, adhering to Edward Tufte’s dictum to keep the data-ink ratio low.\n\n\nRepeated measures ANOVA –&gt; Multi paired estimation plot\n\n\n\nOrdered groups ANOVA –&gt; Shared-control estimation plot\n\n\n\nEstimation Plots: The Way Forward\nIn summary, estimation plots offer five key benefits relative to conventional plots:\n\n\n\n\n\n\n\n\n\n\nBarplot\nBoxplot\nEstimation Plot\n\n\n\n\nDisplays all observed values\nNO\nNO\nYes\n\n\nAvoids false dichotomy\nNO\nNO\nYes\n\n\nFocusses on effect size\nNO\nNO\nYes\n\n\nVisualizes effect size precision\nNO\nNO\nYes\n\n\nShows mean difference distribution\nNO\nNO\nYes\n\n\n\nYou can create estimation plots using the DABEST (Data Analysis with Bootstrap Estimation) packages, which are available in Matlab, Python, and R.\n [1]:W. E. B. Du Bois’s Data Portraits: Visualizing Black America. Edited by Whitney Battle-Baptiste and Britt Rusert, Princeton Architectural Press, 2018"
  },
  {
    "objectID": "03-citation.html",
    "href": "03-citation.html",
    "title": "Citing DABEST",
    "section": "",
    "text": "If your publication features a graphic generated with this software library, please cite the following publication.\nMoving beyond P values: Everyday data analysis with estimation plots Joses Ho, Tayfun Tumkaya, Sameer Aryal, Hyungwon Choi, Adam Claridge-Chang\nNature Methods 2019, 1548-7105. doi:10.1038/s41592-019-0470-3\nFree-to-view PDF\nPaywalled publisher site"
  },
  {
    "objectID": "tests/test_08_mini_meta_pvals.html",
    "href": "tests/test_08_mini_meta_pvals.html",
    "title": "dabest",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport pytest\n\n\n\nfrom dabest._stats_tools import effsize\nfrom dabest._stats_tools import confint_2group_diff as ci2g\nfrom dabest._classes import PermutationTest, Dabest\n\n\n# Data for tests.\n# See Oehlert, G. W. (2000). A First Course in Design \n# and Analysis of Experiments (1st ed.). W. H. Freeman.\n# from Problem 16.3 Pg 444.\n\nrep1_yes = [53.4,54.3,55.9,53.8,56.3,58.6]\nrep1_no = [58.2,60.4,62.4,59.5,64.5,64.5]\nrep2_yes = [46.5,57.2,57.4,51.1,56.9,60.2]\nrep2_no = [49.2,61.6,57.2,51.3,66.8,62.7]\ndf_mini_meta = pd.DataFrame({\n    \"Rep1_Yes\":rep1_yes,\n    \"Rep1_No\" :rep1_no,\n    \"Rep2_Yes\":rep2_yes,\n    \"Rep2_No\" :rep2_no\n})\nN=6 # Size of each group\n\n\n# kwargs for Dabest class init.\ndabest_default_kwargs = dict(x=None, y=None, ci=95, \n                            resamples=5000, random_seed=12345,\n                            proportional=False, delta2=False, experiment=None, \n                            experiment_label=None, x1_level=None, paired=None,\n                            id_col=None\n                            )\n\n\nunpaired = Dabest(data = df_mini_meta, idx =((\"Rep1_No\", \"Rep1_Yes\"), \n                                             (\"Rep2_No\", \"Rep2_Yes\")), \n                                             mini_meta=True,\n                                             **dabest_default_kwargs)\n\ntest_mean_diff\n\nmean_diff = unpaired.mean_diff.results['difference'].to_list()\nnp_result = [np.mean(rep1_yes)-np.mean(rep1_no), \n             np.mean(rep2_yes)-np.mean(rep2_no)]\nassert mean_diff == pytest.approx(np_result)\n\ntest_variances\n\nmini_meta_delta = unpaired.mean_diff.mini_meta_delta\n\ncontrol_var    = mini_meta_delta.control_var\nnp_control_var = [np.var(rep1_no, ddof=1),\n                  np.var(rep2_no, ddof=1)]\nassert control_var == pytest.approx(np_control_var)\n\ntest_var    = mini_meta_delta.test_var\nnp_test_var = [np.var(rep1_yes, ddof=1),\n               np.var(rep2_yes, ddof=1)]\nassert test_var == pytest.approx(np_test_var)\n\ngroup_var    = mini_meta_delta.group_var\nnp_group_var = [ci2g.calculate_group_var(control_var[i], N,\n                                         test_var[i], N)\n                for i in range(0, 2)]\nassert group_var == pytest.approx(np_group_var)\n\ntest_weighted_mean_delta\n\ndifference = unpaired.mean_diff.mini_meta_delta.difference\n\nnp_means = [np.mean(rep1_yes)-np.mean(rep1_no), \n            np.mean(rep2_yes)-np.mean(rep2_no)]\nnp_var   = [np.var(rep1_yes, ddof=1)/N+np.var(rep1_no, ddof=1)/N,\n            np.var(rep2_yes, ddof=1)/N+np.var(rep2_no, ddof=1)/N]\n\nnp_difference = effsize.weighted_delta(np_means, np_var)\n\nassert difference == pytest.approx(np_difference)\n\ntest_unpaired_permutation_test\n\nmini_meta_delta   = unpaired.mean_diff.mini_meta_delta\npvalue             = mini_meta_delta.pvalue_permutation\npermutations_delta = mini_meta_delta.permutations_weighted_delta\n\nperm_test_1 = PermutationTest(rep1_no, rep1_yes, \n                            effect_size=\"mean_diff\", \n                            is_paired=False)\nperm_test_2 = PermutationTest(rep2_no, rep2_yes, \n                            effect_size=\"mean_diff\", \n                            is_paired=False)\npermutations_1 = perm_test_1.permutations\npermutations_2 = perm_test_2.permutations\npermutations_1_var = perm_test_1.permutations_var\npermutations_2_var = perm_test_2.permutations_var\n\nweight_1 = np.true_divide(1,permutations_1_var)\nweight_2 = np.true_divide(1,permutations_2_var)\n\nweighted_deltas = (weight_1*permutations_1 + weight_2*permutations_2)/(weight_1+weight_2)\nassert permutations_delta == pytest.approx(weighted_deltas)\n\n\nnp_means = [np.mean(rep1_yes)-np.mean(rep1_no), \n            np.mean(rep2_yes)-np.mean(rep2_no)]\nnp_var   = [np.var(rep1_yes, ddof=1)/N+np.var(rep1_no, ddof=1)/N,\n            np.var(rep2_yes, ddof=1)/N+np.var(rep2_no, ddof=1)/N]\nnp_weight= np.true_divide(1, np_var)\n\nnp_difference = np.sum(np_means*np_weight)/np.sum(np_weight)\n\nnp_pvalues = len(list(filter(lambda x: np.abs(x)&gt;np.abs(np_difference), \n                            weighted_deltas)))/len(weighted_deltas)\n\nassert pvalue == pytest.approx(np_pvalues)"
  },
  {
    "objectID": "tests/test_04_repeated_measures_effsizes_pvals.html",
    "href": "tests/test_04_repeated_measures_effsizes_pvals.html",
    "title": "dabest",
    "section": "",
    "text": "import pytest\nimport lqrt\nimport numpy as np\nimport scipy as sp\nimport pandas as pd\n\n\n\nfrom dabest._stats_tools import effsize\nfrom dabest._classes import TwoGroupsEffectSize, PermutationTest, Dabest, EffectSizeDataFrame\n\n\n# Data for tests\n# See Der, G., &amp; Everitt, B. S. (2009). A handbook\n# of statistical analyses using SAS, from Display 11.1\ngroup = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\nfirst = [20, 14, 7, 6, 9, 9, 7, 18, 6, 10, 5, 11, 10, 17, 16, 7, 5, 16, 2, 7, 9, 2, 7, 19,\n         7, 9, 6, 13, 9, 6, 11, 7, 8, 3, 4, 11, 1, 6, 0, 18, 15, 10,  6,  9,  4,  4, 10]\nsecond = [15, 12, 5, 10, 7, 9, 3, 17, 9, 15, 9, 11, 2, 12, 15, 10, 0, 7, 1, 11, 16,\n        5, 3, 13, 5, 12, 7, 18, 10, 7, 11, 10, 18, 3, 10, 10, 3, 7, 3, 18, 15, 14, 6, 9, 3, 13, 11]\nthird = [14, 12, 5, 9, 9, 9, 7, 16, 9, 12, 7, 8, 9, 14, 12, 4, 5, 7, 1, 7, 14, 6, 5, 14, 8, 16, 10,\n         14, 12, 8, 12, 11, 19, 3, 11, 10, 2, 7, 3, 19, 15, 16, 7, 13, 4, 13, 13]\nfourth = [13, 10, 6, 8, 5, 11, 6, 14, 9, 12, 3, 8, 3, 10, 7, 7, 0, 6, 2, 5, 10, 7, 5, 12, 8, 17, 15,\n          21, 14, 9, 14, 12, 19, 7, 17, 15, 4, 9, 4, 22, 18, 17, 9, 16, 7, 16, 17]\nfifth = [13, 10, 5, 7, 4, 8, 5, 12, 9, 11, 5, 9, 5, 9, 9, 5, 0, 4, 2, 8, 6, 6, 5, 10, 6, 18, 16, 21,\n         15, 12, 16, 14, 22, 8, 18, 16, 5, 10, 6, 22, 19, 19, 10, 20, 9, 19, 21] \n\ndf = pd.DataFrame({'Group' : group,\n                   'First' : first,\n                   'Second': second,\n                   'Third' : third,\n                   'Fourth': fourth,\n                   'Fifth' : fifth,\n                   'ID': np.arange(0, 47)\n                    })\n\n# kwargs for Dabest class init.\ndabest_default_kwargs = dict(x=None, y=None, ci=95, \n                            resamples=5000, random_seed=12345, proportional=False,\n                            delta2 = False, experiment=None, \n                            experiment_label=None, x1_level=None, mini_meta=False)\n\n# example of sequential repeated measures\nsequential = Dabest(df, id_col = \"ID\",\n                         idx=(\"First\", \"Second\", \"Third\", \"Fourth\", \"Fifth\"),\n                         paired = \"sequential\",\n                         **dabest_default_kwargs)\n\n# example of baseline repeated measures\nbaseline = Dabest(df, id_col = \"ID\",\n                       idx=(\"First\", \"Second\", \"Third\", \"Fourth\", \"Fifth\"),\n                       paired = \"baseline\",\n                       **dabest_default_kwargs)\n\ntest_mean_diff_sequential\n\nmean_diff = sequential.mean_diff.results['difference'].to_list()\nnp_result = [np.mean(df.iloc[:,i+1]-df.iloc[:,i]) for i in range(1,5)]\nassert mean_diff == pytest.approx(np_result)\n\ntest_median_diff_sequential\n\nmedian_diff = sequential.median_diff.results['difference'].to_list()\nnp_result = [np.median(df.iloc[:,i+1]-df.iloc[:,i]) for i in range(1,5)]\nassert median_diff == pytest.approx(np_result)\n\ntest_mean_diff_baseline\n\nmean_diff = baseline.mean_diff.results['difference'].to_list()\nnp_result = [np.mean(df.iloc[:,i]-df.iloc[:,1]) for i in range(2,6)]\nassert mean_diff == pytest.approx(np_result)\n\ntest_median_diff_baseline\n\nmedian_diff = baseline.median_diff.results['difference'].to_list()\nnp_result = [np.median(df.iloc[:,i]-df.iloc[:,1]) for i in range(2,6)]\nassert median_diff == pytest.approx(np_result)\n\ntest_cohens_d_sequential\n\ncohens_d = sequential.cohens_d.results['difference'].to_list()\nnp_result = [np.mean(df.iloc[:,i+1]-df.iloc[:,i])\n                /np.sqrt((np.var(df.iloc[:,i+1], ddof=1)+np.var(df.iloc[:,i], ddof=1))/2) \n            for i in range(1,5)]\nassert cohens_d == pytest.approx(np_result)\n\ntest_hedges_g_sequential\n\nfrom math import gamma\nhedges_g = sequential.hedges_g.results['difference'].to_list()\na = 47*2-2\nfac = gamma(a/2)/(np.sqrt(a/2)*gamma((a-1)/2))\nnp_result = [np.mean(df.iloc[:,i+1]-df.iloc[:,i])*fac\n                /np.sqrt((np.var(df.iloc[:,i+1], ddof=1)+np.var(df.iloc[:,i], ddof=1))/2) \n            for i in range(1,5)] \nassert hedges_g == pytest.approx(np_result)\n\ntest_cohens_d_baseline\n\ncohens_d = baseline.cohens_d.results['difference'].to_list()\nnp_result = [np.mean(df.iloc[:,i]-df.iloc[:,1])\n                /np.sqrt((np.var(df.iloc[:,i], ddof=1)+np.var(df.iloc[:,1], ddof=1))/2) \n            for i in range(2,6)]\nassert cohens_d == pytest.approx(np_result)\n\ntest_hedges_g_baseline\n\nfrom math import gamma\nhedges_g = baseline.hedges_g.results['difference'].to_list()\na = 47*2-2\nfac = gamma(a/2)/(np.sqrt(a/2)*gamma((a-1)/2))\nnp_result = [np.mean(df.iloc[:,i]-df.iloc[:,1])*fac\n                /np.sqrt((np.var(df.iloc[:,i], ddof=1)+np.var(df.iloc[:,1], ddof=1))/2) \n            for i in range(2,6)]\nassert hedges_g == pytest.approx(np_result)\n\ntest_paired_stats_sequential\n\nnp_result = sequential.mean_diff.results\n    \np1 = [sp.stats.ttest_rel(df.iloc[:,i], df.iloc[:,i+1], nan_policy='omit').pvalue\n            for i in range(1,5)] \nassert np_result[\"pvalue_paired_students_t\"].to_list() == pytest.approx(p1)\n\np2 = [sp.stats.wilcoxon(df.iloc[:,i], df.iloc[:,i+1]).pvalue\n            for i in range(1,5)] \nassert np_result[\"pvalue_wilcoxon\"].to_list() == pytest.approx(p2)\n\ntest_paired_stats_baseline\n\nnp_result = baseline.mean_diff.results\n    \np1 = [sp.stats.ttest_rel(df.iloc[:,1], df.iloc[:,i], nan_policy='omit').pvalue\n            for i in range(2,6)] \nassert np_result[\"pvalue_paired_students_t\"].to_list() == pytest.approx(p1)\n\np2 = [sp.stats.wilcoxon(df.iloc[:,1], df.iloc[:,i]).pvalue\n            for i in range(2,6)] \nassert np_result[\"pvalue_wilcoxon\"].to_list() == pytest.approx(p2)\n\ntest_lqrt_paired_sequential\n\nlqrt_result = sequential.mean_diff.lqrt[\"pvalue_paired_lqrt\"].to_list()\n                             \np1 = [lqrt.lqrtest_rel(df.iloc[:,i], df.iloc[:,i+1], random_state=12345).pvalue\n            for i in range(1,5)] \n\nassert lqrt_result == pytest.approx(p1)\n\ntest_lqrt_paired_baseline\n\nlqrt_result = baseline.mean_diff.lqrt[\"pvalue_paired_lqrt\"].to_list()\n                             \np1 = [lqrt.lqrtest_rel(df.iloc[:,1], df.iloc[:,i], random_state=12345).pvalue\n            for i in range(2,6)] \n\nassert lqrt_result == pytest.approx(p1)"
  },
  {
    "objectID": "tests/test_06_delta-delta_effsize_pvals.html",
    "href": "tests/test_06_delta-delta_effsize_pvals.html",
    "title": "dabest",
    "section": "",
    "text": "import pytest\nimport lqrt\nimport numpy as np\nimport scipy as sp\nimport pandas as pd\n\n\n\nfrom dabest._stats_tools import effsize\nfrom dabest._classes import TwoGroupsEffectSize, PermutationTest, Dabest\n\n\n# Data for tests.\n# See: Asheber Abebe. Introduction to Design and Analysis of Experiments \n# with the SAS, from Example: Two-way RM Design Pg 137.\nhr = [72, 78, 71, 72, 66, 74, 62, 69, 69, 66, 84, 80, 72, 65, 75, 71, \n      86, 83, 82, 83, 79, 83, 73, 75, 73, 62, 90, 81, 72, 62, 69, 70]\n\n# Add experiment column\ne1 = np.repeat('Treatment1', 8).tolist()\ne2 = np.repeat('Control', 8).tolist()\nexperiment = e1 + e2 + e1 + e2\n\n# Add a `Drug` column as the first variable\nd1 = np.repeat('AX23', 8).tolist()\nd2 = np.repeat('CONTROL', 8).tolist()\ndrug = d1 + d2 + d1 + d2\n\n# Add a `Time` column as the second variable\nt1 = np.repeat('T1', 16).tolist()\nt2 = np.repeat('T2', 16).tolist()\ntime = t1 + t2\n\n# Add an `id` column for paired data plotting.\nid_col = []\nfor i in range(1, 9):\n    id_col.append(str(i)+\"a\")\nfor i in range(1, 9):\n    id_col.append(str(i)+\"c\")\nid_col.extend(id_col)\n\n# Combine samples and gender into a DataFrame.\ndf_test = pd.DataFrame({'ID'   : id_col,\n                   'Drug'      : drug,\n                   'Time'      : time, \n                   'Experiment': experiment,\n                   'Heart Rate': hr\n                    })\n\n\ndf_test_control = df_test[df_test[\"Experiment\"]==\"Control\"]\ndf_test_control = df_test_control.pivot(index=\"ID\", columns=\"Time\", values=\"Heart Rate\")\n\n\ndf_test_treatment1 = df_test[df_test[\"Experiment\"]==\"Treatment1\"]\ndf_test_treatment1 = df_test_treatment1.pivot(index=\"ID\", columns=\"Time\", values=\"Heart Rate\")\n\n\n# kwargs for Dabest class init.\ndabest_default_kwargs = dict(ci=95, \n                            resamples=5000, random_seed=12345,\n                            idx=None, proportional=False, mini_meta=False\n                            )\n\n# example of unpaired delta-delta calculation\nunpaired = Dabest(data = df_test, x = [\"Time\", \"Drug\"], y = \"Heart Rate\", \n                  delta2 = True, experiment = \"Experiment\",\n                  experiment_label=None, x1_level=None, paired=None, id_col=None,\n                  **dabest_default_kwargs)\n\n\n# example of paired delta-delta calculation\npaired = Dabest(data = df_test, x = [\"Time\", \"Drug\"], y = \"Heart Rate\", \n                  delta2 = True, experiment = \"Experiment\", paired=\"sequential\", id_col=\"ID\",\n                  experiment_label=None, x1_level=None,\n                  **dabest_default_kwargs)\n\n\n# example of paired data with specified experiment/x1 level\npaired_specified_level = Dabest(data = df_test, x = [\"Time\", \"Drug\"], y = \"Heart Rate\", \n                  delta2 = True, experiment = \"Experiment\", paired=\"sequential\", id_col=\"ID\",\n                  experiment_label=[\"Control\", \"Treatment1\"], x1_level=[\"T2\", \"T1\"],\n                  **dabest_default_kwargs)\n\ntest_mean_diff_delta_unpaired\n\nmean_diff_results = unpaired.mean_diff.results\nall_mean_diff = mean_diff_results['difference'].to_list()\ndiff1 = np.mean(df_test_treatment1[\"T2\"])-np.mean(df_test_treatment1[\"T1\"])\ndiff2 = np.mean(df_test_control[\"T2\"])-np.mean(df_test_control[\"T1\"])\nnp_result = [diff1, diff2]\nassert all_mean_diff == pytest.approx(np_result)\n\ntest_mean_diff_delta_paired\n\nmean_diff_results = paired.mean_diff.results\nall_mean_diff = mean_diff_results['difference'].to_list()\ndiff1 = np.mean(df_test_treatment1[\"T2\"]-df_test_treatment1[\"T1\"])\ndiff2 = np.mean(df_test_control[\"T2\"]-df_test_control[\"T1\"])\nnp_result = [diff1, diff2]\nassert all_mean_diff == pytest.approx(np_result)\n\ntest_mean_diff_delta_paired_specified_level\n\nmean_diff_results = paired_specified_level.mean_diff.results\nall_mean_diff = mean_diff_results['difference'].to_list()\ndiff1 = np.mean(df_test_control[\"T1\"]-df_test_control[\"T2\"])\ndiff2 = np.mean(df_test_treatment1[\"T1\"]-df_test_treatment1[\"T2\"])\nnp_result = [diff1, diff2]\nassert all_mean_diff == pytest.approx(np_result)\n\ntest_median_diff_unpaired\n\nall_median_diff = unpaired.median_diff.results\nmedian_diff = all_median_diff['difference'].to_list()\ndiff1 = np.median(df_test_treatment1[\"T2\"])-np.median(df_test_treatment1[\"T1\"])\ndiff2 = np.median(df_test_control[\"T2\"])-np.median(df_test_control[\"T1\"])\nnp_result = [diff1, diff2]\nassert median_diff == pytest.approx(np_result)\n\ntest_median_diff_paired\n\nall_median_diff = paired.median_diff.results\nmedian_diff = all_median_diff['difference'].to_list()\ndiff1 = np.median(df_test_treatment1[\"T2\"]-df_test_treatment1[\"T1\"])\ndiff2 = np.median(df_test_control[\"T2\"]-df_test_control[\"T1\"])\nnp_result = [diff1, diff2]\nassert median_diff == pytest.approx(np_result)\n\ntest_median_diff_paired_specified_level\n\nall_median_diff = paired_specified_level.median_diff.results\nmedian_diff = all_median_diff['difference'].to_list()\ndiff1 = np.median(df_test_control[\"T1\"]-df_test_control[\"T2\"])\ndiff2 = np.median(df_test_treatment1[\"T1\"]-df_test_treatment1[\"T2\"])\nnp_result = [diff1, diff2]\nassert median_diff == pytest.approx(np_result)\n\ntest_cohens_d_unpaired\n\nall_cohens_d = unpaired.cohens_d.results\ncohens_d = all_cohens_d['difference'].to_list()\ndiff1 = np.mean(df_test_treatment1[\"T2\"])-np.mean(df_test_treatment1[\"T1\"])\ndiff1 = diff1/np.sqrt((np.var(df_test_treatment1[\"T2\"], ddof=1)+np.var(df_test_treatment1[\"T1\"], ddof=1))/2) \ndiff2 = np.mean(df_test_control[\"T2\"])-np.mean(df_test_control[\"T1\"])\ndiff2 = diff2/np.sqrt((np.var(df_test_control[\"T2\"], ddof=1)+np.var(df_test_control[\"T1\"], ddof=1))/2) \nnp_result = [diff1, diff2]          \nassert cohens_d == pytest.approx(np_result)\n\ntest_cohens_d_paired\n\nall_cohens_d = paired.cohens_d.results\ncohens_d = all_cohens_d['difference'].to_list()\ndiff1 = np.mean(df_test_treatment1[\"T2\"]-df_test_treatment1[\"T1\"])\ndiff1 = diff1/np.sqrt((np.var(df_test_treatment1[\"T2\"], ddof=1)+np.var(df_test_treatment1[\"T1\"], ddof=1))/2) \ndiff2 = np.mean(df_test_control[\"T2\"]-df_test_control[\"T1\"])\ndiff2 = diff2/np.sqrt((np.var(df_test_control[\"T2\"], ddof=1)+np.var(df_test_control[\"T1\"], ddof=1))/2) \nnp_result = [diff1, diff2]          \nassert cohens_d == pytest.approx(np_result)\n\ntest_cohens_d_paired_specified_level\n\nall_cohens_d = paired_specified_level.cohens_d.results\ncohens_d = all_cohens_d['difference'].to_list()\ndiff1 = np.mean(df_test_control[\"T1\"])-np.mean(df_test_control[\"T2\"])\ndiff1 = diff1/np.sqrt((np.var(df_test_control[\"T2\"], ddof=1)+np.var(df_test_control[\"T1\"], ddof=1))/2)\ndiff2 = np.mean(df_test_treatment1[\"T1\"])-np.mean(df_test_treatment1[\"T2\"])\ndiff2 = diff2/np.sqrt((np.var(df_test_treatment1[\"T2\"], ddof=1)+np.var(df_test_treatment1[\"T1\"], ddof=1))/2)  \nnp_result = [diff1, diff2]     \nassert cohens_d == pytest.approx(np_result)\n\ntest_hedges_g_unpaired\n\nfrom math import gamma\nhedges_g = unpaired.hedges_g.results['difference'].to_list()\na = 8*2-2\nfac = gamma(a/2)/(np.sqrt(a/2)*gamma((a-1)/2))\ndiff1 = (np.mean(df_test_treatment1[\"T2\"])-np.mean(df_test_treatment1[\"T1\"]))*fac\ndiff1 = diff1/np.sqrt((np.var(df_test_treatment1[\"T2\"], ddof=1)+np.var(df_test_treatment1[\"T1\"], ddof=1))/2) \ndiff2 = (np.mean(df_test_control[\"T2\"])-np.mean(df_test_control[\"T1\"]))*fac\ndiff2 = diff2/np.sqrt((np.var(df_test_control[\"T2\"], ddof=1)+np.var(df_test_control[\"T1\"], ddof=1))/2) \nnp_result=[diff1, diff2]\nassert hedges_g == pytest.approx(np_result)\n\ntest_hedges_g_paired\n\nfrom math import gamma\nhedges_g = paired.hedges_g.results['difference'].to_list()\na = 8*2-2\nfac = gamma(a/2)/(np.sqrt(a/2)*gamma((a-1)/2))\ndiff1 = (np.mean(df_test_treatment1[\"T2\"]-df_test_treatment1[\"T1\"]))*fac\ndiff1 = diff1/np.sqrt((np.var(df_test_treatment1[\"T2\"], ddof=1)+np.var(df_test_treatment1[\"T1\"], ddof=1))/2) \ndiff2 = (np.mean(df_test_control[\"T2\"]-df_test_control[\"T1\"]))*fac\ndiff2 = diff2/np.sqrt((np.var(df_test_control[\"T2\"], ddof=1)+np.var(df_test_control[\"T1\"], ddof=1))/2) \nnp_result=[diff1, diff2]\nassert hedges_g == pytest.approx(np_result)\n\ntest_hedges_g_paired_specified_level\n\nfrom math import gamma\nhedges_g = paired_specified_level.hedges_g.results['difference'].to_list()\na = 8*2-2\nfac = gamma(a/2)/(np.sqrt(a/2)*gamma((a-1)/2))\ndiff1 = (np.mean(df_test_control[\"T1\"]-df_test_control[\"T2\"]))*fac\ndiff1 = diff1/np.sqrt((np.var(df_test_control[\"T2\"], ddof=1)+np.var(df_test_control[\"T1\"], ddof=1))/2) \ndiff2 = (np.mean(df_test_treatment1[\"T1\"]-df_test_treatment1[\"T2\"]))*fac\ndiff2 = diff2/np.sqrt((np.var(df_test_treatment1[\"T2\"], ddof=1)+np.var(df_test_treatment1[\"T1\"], ddof=1))/2) \nnp_result=[diff1, diff2]\nassert hedges_g == pytest.approx(np_result)\n\ntest_unpaired_delta_delta\n\ndelta_delta = unpaired.mean_diff.delta_delta.difference\n\ndiff1 = np.mean(df_test_treatment1[\"T2\"])-np.mean(df_test_treatment1[\"T1\"])\ndiff2 = np.mean(df_test_control[\"T2\"])-np.mean(df_test_control[\"T1\"])\nnp_result = diff2-diff1\n\nassert delta_delta == pytest.approx(np_result)\n\ntest_paired_delta_delta\n\ndelta_delta = paired.mean_diff.delta_delta.difference\n\ndiff1 = np.mean(df_test_treatment1[\"T2\"] - df_test_treatment1[\"T1\"])\ndiff2 = np.mean(df_test_control[\"T2\"] - df_test_control[\"T1\"])\nnp_result = diff2-diff1\n\nassert delta_delta == pytest.approx(np_result)\n\ntest_paired_specified_level_delta_delta\n\ndelta_delta = paired_specified_level.mean_diff.delta_delta.difference\n\ndiff1 = np.mean(df_test_control[\"T1\"] - df_test_control[\"T2\"])\ndiff2 = np.mean(df_test_treatment1[\"T1\"] - df_test_treatment1[\"T2\"])\nnp_result = diff2-diff1\n\nassert delta_delta == pytest.approx(np_result)\n\ntest_unpaired_permutation_test\n\ndelta_delta              = unpaired.mean_diff.delta_delta\npvalue                   = delta_delta.pvalue_permutation\npermutations_delta_delta = delta_delta.permutations_delta_delta\n\nperm_test_1 = PermutationTest(df_test_treatment1[\"T1\"], \n                              df_test_treatment1[\"T2\"], \n                              effect_size=\"mean_diff\", \n                              is_paired=False)\nperm_test_2 = PermutationTest(df_test_control[\"T1\"], \n                              df_test_control[\"T2\"], \n                              effect_size=\"mean_diff\", \n                              is_paired=False)\npermutations_1 = perm_test_1.permutations\npermutations_2 = perm_test_2.permutations\n\ndelta_deltas = permutations_2-permutations_1\nassert permutations_delta_delta == pytest.approx(delta_deltas)\n\ndiff1 = np.mean(df_test_treatment1[\"T2\"])-np.mean(df_test_treatment1[\"T1\"])\ndiff2 = np.mean(df_test_control[\"T2\"])-np.mean(df_test_control[\"T1\"])\nnp_diff = diff2-diff1\n\nnp_pvalues = len(list(filter(lambda x: np.abs(x)&gt;np.abs(np_diff), \n                            delta_deltas)))/len(delta_deltas)\n\nassert pvalue == pytest.approx(np_pvalues)\n\ntest_paired_permutation_test\n\ndelta_delta              = paired.mean_diff.delta_delta\npvalue                   = delta_delta.pvalue_permutation\npermutations_delta_delta = delta_delta.permutations_delta_delta\n\nperm_test_1 = PermutationTest(df_test_treatment1[\"T1\"], \n                              df_test_treatment1[\"T2\"], \n                              effect_size=\"mean_diff\", \n                              is_paired=\"sequential\")\nperm_test_2 = PermutationTest(df_test_control[\"T1\"], \n                              df_test_control[\"T2\"], \n                              effect_size=\"mean_diff\", \n                              is_paired=\"sequential\")\npermutations_1 = perm_test_1.permutations\npermutations_2 = perm_test_2.permutations\n\ndelta_deltas = permutations_2-permutations_1\nassert permutations_delta_delta == pytest.approx(delta_deltas)\n\ndiff1 = np.mean(df_test_treatment1[\"T2\"]-df_test_treatment1[\"T1\"])\ndiff2 = np.mean(df_test_control[\"T2\"]-df_test_control[\"T1\"])\nnp_diff = diff2-diff1\n\nnp_pvalues = len(list(filter(lambda x: np.abs(x)&gt;np.abs(np_diff), \n                            delta_deltas)))/len(delta_deltas)\n\nassert pvalue == pytest.approx(np_pvalues)\n\ntest_paired_specified_level_permutation_test\n\ndelta_delta              = paired_specified_level.mean_diff.delta_delta\npvalue                   = delta_delta.pvalue_permutation\npermutations_delta_delta = delta_delta.permutations_delta_delta\n\nperm_test_1 = PermutationTest(df_test_control[\"T2\"], \n                              df_test_control[\"T1\"], \n                              effect_size=\"mean_diff\", \n                              is_paired=\"sequential\")\nperm_test_2 = PermutationTest(df_test_treatment1[\"T2\"], \n                              df_test_treatment1[\"T1\"], \n                              effect_size=\"mean_diff\", \n                              is_paired=\"sequential\")\npermutations_1 = perm_test_1.permutations\npermutations_2 = perm_test_2.permutations\n\ndelta_deltas = permutations_2-permutations_1\nassert permutations_delta_delta == pytest.approx(delta_deltas)\n\ndiff1 = np.mean(df_test_control[\"T1\"]-df_test_control[\"T2\"])\ndiff2 = np.mean(df_test_treatment1[\"T1\"]-df_test_treatment1[\"T2\"])\nnp_diff = diff2-diff1\n\nnp_pvalues = len(list(filter(lambda x: np.abs(x)&gt;np.abs(np_diff), \n                            delta_deltas)))/len(delta_deltas)\n\nassert pvalue == pytest.approx(np_pvalues)"
  },
  {
    "objectID": "tests/test_99_confint.html",
    "href": "tests/test_99_confint.html",
    "title": "dabest",
    "section": "",
    "text": "import numpy as np\nfrom scipy.stats import norm\nfrom scipy.stats import skewnorm\nimport pandas as pd\nimport pytest\n\n\n\nfrom dabest._api import load\n\ntest_paired_mean_diff_ci\n\n# See Altman et al., Statistics with Confidence: \n# Confidence Intervals and Statistical Guidelines (Second Edition). Wiley, 2000.\n# Pg 31.\n# Added in v0.2.5.\nblood_pressure = {\"before\": [148, 142, 136, 134, 138, 140, 132, 144,\n                            128, 170, 162, 150, 138, 154, 126, 116],\n                  \"after\" : [152, 152, 134, 148, 144, 136, 144, 150, \n                            146, 174, 162, 162, 146, 156, 132, 126],\n                 \"subject_id\" : np.arange(1, 17)}\nexercise_bp = pd.DataFrame(blood_pressure)\n\n\nex_bp = load(data=exercise_bp, idx=(\"before\", \"after\"), \n             paired=\"baseline\", id_col=\"subject_id\")\npaired_mean_diff = ex_bp.mean_diff.results\n\nassert pytest.approx(3.875) == paired_mean_diff.bca_low[0]\nassert pytest.approx(9.5) == paired_mean_diff.bca_high[0]\n\nC:\\Users\\zhang\\anaconda3\\lib\\site-packages\\scipy\\stats\\_morestats.py:3337: UserWarning: Exact p-value calculation does not work if there are zeros. Switching to normal approximation.\n  warnings.warn(\"Exact p-value calculation does not work if there are \"\n\n\ntest_unpaired_ci\n\n# Dropped to 30 reps to save time. v0.2.5.\nreps=30\nci=95\nPOPULATION_N = 10000\nSAMPLE_N = 10\n\n# Create data for hedges g and cohens d.\nCONTROL_MEAN = np.random.randint(1, 1000)\nPOP_SD       = np.random.randint(1, 15)\nPOP_D        = np.round(np.random.uniform(-2, 2, 1)[0], 2)\n\nTRUE_STD_DIFFERENCE = CONTROL_MEAN + (POP_D * POP_SD)\nnorm_sample_kwargs = dict(scale=POP_SD, size=SAMPLE_N)\nc1 = norm.rvs(loc=CONTROL_MEAN, **norm_sample_kwargs)\nt1 = norm.rvs(loc=CONTROL_MEAN+TRUE_STD_DIFFERENCE, **norm_sample_kwargs)\n\nstd_diff_df = pd.DataFrame({'Control' : c1, 'Test': t1})\n\n\n\n# Create mean_diff data\nCONTROL_MEAN = np.random.randint(1, 1000)\nPOP_SD       = np.random.randint(1, 15)\nTRUE_DIFFERENCE = np.random.randint(-POP_SD*5, POP_SD*5)\n\nc1 = norm.rvs(loc=CONTROL_MEAN, **norm_sample_kwargs)\nt1 = norm.rvs(loc=CONTROL_MEAN+TRUE_DIFFERENCE, **norm_sample_kwargs)\n\nmean_df = pd.DataFrame({'Control' : c1, 'Test': t1})\n\n\n\n# Create median_diff data\nMEDIAN_DIFFERENCE = np.random.randint(-5, 5)\nA = np.random.randint(-7, 7)\n\nskew_kwargs = dict(a=A, scale=5, size=POPULATION_N)\nskewpop1 = skewnorm.rvs(**skew_kwargs, loc=100)\nskewpop2 = skewnorm.rvs(**skew_kwargs, loc=100+MEDIAN_DIFFERENCE)\n\nsample_kwargs = dict(replace=False, size=SAMPLE_N)\nskewsample1 = np.random.choice(skewpop1, **sample_kwargs)\nskewsample2 = np.random.choice(skewpop2, **sample_kwargs)\n\nmedian_df = pd.DataFrame({'Control' : skewsample1, 'Test': skewsample2})\n\n\n\n# Create two populations with a 50% overlap.\nCD_DIFFERENCE = np.random.randint(1, 10)\nSD = np.abs(CD_DIFFERENCE)\n\npop_kwargs = dict(scale=SD, size=POPULATION_N)\npop1 = norm.rvs(loc=100, **pop_kwargs)\npop2 = norm.rvs(loc=100+CD_DIFFERENCE, **pop_kwargs)\n\nsample_kwargs = dict(replace=False, size=SAMPLE_N)\nsample1 = np.random.choice(pop1, **sample_kwargs)\nsample2 = np.random.choice(pop2, **sample_kwargs)\n\ncd_df = pd.DataFrame({'Control' : sample1, 'Test': sample2})\n\n\n\n# Create several CIs and see if the true population difference lies within.\nerror_count_cohens_d     = 0\nerror_count_hedges_g     = 0\nerror_count_mean_diff    = 0\nerror_count_median_diff  = 0\nerror_count_cliffs_delta = 0\n\nfor i in range(0, reps):\n    # print(i) # for debug.\n    # pick a random seed\n    rnd_sd = np.random.randint(0, 999999)\n    load_kwargs = dict(ci=ci, random_seed=rnd_sd)\n\n    std_diff_data = load(data=std_diff_df, idx=(\"Control\", \"Test\"), **load_kwargs)\n    cd = std_diff_data.cohens_d.results\n    # print(\"cohen's d\")  # for debug.\n    cd_low, cd_high = float(cd.bca_low), float(cd.bca_high)\n    if cd_low &lt; POP_D &lt; cd_high is False:\n        error_count_cohens_d += 1\n\n    hg = std_diff_data.hedges_g.results\n    # print(\"hedges' g\") # for debug.\n    hg_low, hg_high = float(hg.bca_low), float(hg.bca_high)\n    if hg_low &lt; POP_D &lt; hg_high is False:\n        error_count_hedges_g += 1\n\n\n    mean_diff_data = load(data=mean_df, idx=(\"Control\", \"Test\"), **load_kwargs)\n    mean_d = mean_diff_data.mean_diff.results\n    # print(\"mean diff\") # for debug.\n    mean_d_low, mean_d_high = float(mean_d.bca_low), float(mean_d.bca_high)\n    if mean_d_low &lt; TRUE_DIFFERENCE &lt; mean_d_high is False:\n        error_count_mean_diff += 1\n\n\n    median_diff_data = load(data=median_df, idx=(\"Control\", \"Test\"),\n                         **load_kwargs)\n    median_d = median_diff_data.median_diff.results\n    # print(\"median diff\") # for debug.\n    median_d_low, median_d_high = float(median_d.bca_low), float(median_d.bca_high)\n    if median_d_low &lt; MEDIAN_DIFFERENCE &lt; median_d_high is False:\n        error_count_median_diff += 1\n\n\n    cd_data = load(data=cd_df, idx=(\"Control\", \"Test\"), **load_kwargs)\n    cliffs = cd_data.cliffs_delta.results\n    # print(\"cliff's delta\") # for debug.\n    low, high = float(cliffs.bca_low), float(cliffs.bca_high)\n    if low &lt; 0.5 &lt; high is False:\n        error_count_cliffs_delta += 1\n\n\nmax_errors = int(np.ceil(reps * (100 - ci) / 100))\n\nassert error_count_cohens_d     &lt;= max_errors\nassert error_count_hedges_g     &lt;= max_errors\nassert error_count_mean_diff    &lt;= max_errors\nassert error_count_median_diff  &lt;= max_errors\nassert error_count_cliffs_delta &lt;= max_errors\n\nc:\\users\\zhang\\desktop\\vnbdev-dabest\\dabest-python\\dabest\\_classes.py:1663: UserWarning: The lower limit of the interval was in the bottom 10 values. The result should be considered unstable.\n  warnings.warn(err_temp.substitute(lim_type=\"lower\",\n\n\nModuleNotFoundError: No module named 'dabest.effsize'"
  },
  {
    "objectID": "tests/test_02_edge_cases.html",
    "href": "tests/test_02_edge_cases.html",
    "title": "test_unrelated_columns",
    "section": "",
    "text": "import numpy as np\nfrom numpy.random import PCG64, RandomState\nimport scipy as sp\nimport pytest\nimport pandas as pd\n\n\n\nfrom dabest._api import load\n\nTest to see if 'unrelated' columns jam up the analysis.\nSee Github Issue 43.\nhttps://github.com/ACCLAB/DABEST-python/issues/44.\n\nAdded in v0.2.5.\n\nN=60\nrandom_seed=12345\n\n# rng = RandomState(MT19937(random_seed))\nrng = RandomState(PCG64(12345))\n# rng = np.random.default_rng(seed=random_seed)\n\ndf = pd.DataFrame(\n    {'groups': rng.choice(['Group 1', 'Group 2', 'Group 3'], size=(N,)),\n     'color' : rng.choice(['green', 'red', 'purple'], size=(N,)),\n     'value':  rng.random(size=(N,))})\n\ndf['unrelated'] = np.nan\n\ntest = load(data=df, x='groups', y='value', \n            idx=['Group 1', 'Group 2'])\n\nmd = test.mean_diff.results\n\nassert md.difference[0] == pytest.approx(-0.0322, abs=1e-4)\nassert md.bca_low[0]    == pytest.approx(-0.2279, abs=1e-4)\nassert md.bca_high[0]   == pytest.approx(0.1613, abs=1e-4)"
  },
  {
    "objectID": "tests/test_01_effsizes_pvals.html",
    "href": "tests/test_01_effsizes_pvals.html",
    "title": "dabest",
    "section": "",
    "text": "import pytest\nimport lqrt\nimport numpy as np\nfrom numpy import median as npmedian\nfrom numpy import mean as npmean\nimport scipy as sp\nimport pandas as pd\n\n\n\nfrom dabest._stats_tools import effsize\nfrom dabest._classes import TwoGroupsEffectSize, PermutationTest, Dabest\n\n\n# Data for tests.\n# See Cumming, G. Understanding the New Statistics:\n# Effect Sizes, Confidence Intervals, and Meta-Analysis. Routledge, 2012,\n# from Cumming 2012 Table 11.1 Pg 287.\nwb = {\"control\": [34, 54, 33, 44, 45, 53, 37, 26, 38, 58],\n      \"expt\":    [66, 38, 35, 55, 48, 39, 65, 32, 57, 41]}\nwellbeing = pd.DataFrame(wb)\n\n\n\n# from Cumming 2012 Table 11.2 Page 291\npaired_wb = {\"pre\":   [43, 28, 54, 36, 31, 48, 50, 69, 29, 40],\n             \"post\":  [51, 33, 58, 42, 39, 45, 54, 68, 35, 44],\n             \"ID\":    np.arange(10)}\npaired_wellbeing = pd.DataFrame(paired_wb)\n\n\n\n# Data for testing Cohen's calculation.\n# Only work with binary data.\n# See Venables, W. N. and Ripley, B. D. (2002) Modern Applied Statistics with S. Fourth edition. Springer.\n# Make two groups of `smoke` by choosing `low` as a standard, and the data is trimed from the back.\nsk = {  \"low\":  [0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, \n                 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0],\n        \"high\": [1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, \n                 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1]}\nsmoke = pd.DataFrame(sk)\n\n\n\n# Data from Hogarty and Kromrey (1999)\n# Kromrey, Jeffrey D., and Kristine Y. Hogarty. 1998.\n# \"Analysis Options for Testing Group Differences on Ordered Categorical\n# Variables: An Empirical Investigation of Type I Error Control\n# Statistical Power.\"\n# Multiple Linear Regression Viewpoints 25 (1): 70 - 82.\nlikert_control   = [1, 1, 2, 2, 2, 3, 3, 3, 4, 5]\nlikert_treatment = [1, 2, 3, 4, 4, 5]\n\n\n\n# Data from Cliff (1993)\n# Cliff, Norman. 1993. \"Dominance Statistics: Ordinal Analyses to Answer\n# Ordinal Questions.\"\n# Psychological Bulletin 114 (3): 494-509.\na_scores = [6, 7, 9, 10]\nb_scores = [1, 3, 4, 7, 8]\n\n\n\n# kwargs for Dabest class init.\ndabest_default_kwargs = dict(x=None, y=None, ci=95, \n                            resamples=5000, random_seed=12345,\n                            proportional=False, delta2=False, experiment=None, \n                            experiment_label=None, x1_level=None, mini_meta=False\n                            )\n\ntest_mean_diff_unpaired\n\nmean_diff = effsize.func_difference(wellbeing.control, wellbeing.expt,\n                                    np.mean, is_paired=False)\nassert mean_diff == pytest.approx(5.4)\n\ntest_median_diff_unpaired\n\nmedian_diff = effsize.func_difference(wellbeing.control, wellbeing.expt,\n                                    npmedian, is_paired=False)\nassert median_diff == pytest.approx(3.5)\n\ntest_mean_diff_paired\n\nmean_diff = effsize.func_difference(paired_wellbeing.pre,\n                                    paired_wellbeing.post,\n                                    npmean, is_paired=\"baseline\")\nassert mean_diff == pytest.approx(4.10)\n\ntest_median_diff_paired\n\nmedian_diff = effsize.func_difference(paired_wellbeing.pre,\n                                      paired_wellbeing.post,\n                                      npmedian, is_paired=\"baseline\")\nassert median_diff == pytest.approx(4.5)\n\ntest_cohens_d_unpaired\n\ncohens_d = effsize.cohens_d(wellbeing.control, wellbeing.expt,\n                            is_paired=False)\nassert np.round(cohens_d, 2) == pytest.approx(0.47)\n\ntest_hedges_g_unpaired\n\nhedges_g = effsize.hedges_g(wellbeing.control, wellbeing.expt,\n                                is_paired=False)\nassert np.round(hedges_g, 2) == pytest.approx(0.45)\n\ntest_cohens_d_paired\n\ncohens_d = effsize.cohens_d(paired_wellbeing.pre, paired_wellbeing.post,\n                                is_paired=\"baseline\")\nassert np.round(cohens_d, 2) == pytest.approx(0.34)\n\ntest_hedges_g_paired\n\nhedges_g = effsize.hedges_g(paired_wellbeing.pre, paired_wellbeing.post,\n                            is_paired=\"baseline\")\nassert np.round(hedges_g, 2) == pytest.approx(0.33)\n\ntest_cohens_h\n\ncohens_h = effsize.cohens_h(smoke.low, smoke.high)\nassert np.round(cohens_h, 2) == pytest.approx(0.17)\n\ntest_cliffs_delta\n\nlikert_delta = effsize.cliffs_delta(likert_treatment, likert_control)\nassert likert_delta == pytest.approx(-0.25)\n\nscores_delta = effsize.cliffs_delta(b_scores, a_scores)\nassert scores_delta == pytest.approx(0.65)\n\ntest_unpaired_stats\n\nc = wellbeing.control\nt = wellbeing.expt\n\nunpaired_es = TwoGroupsEffectSize(c, t, \"mean_diff\", is_paired=False, proportional=False)\n\np1 = sp.stats.mannwhitneyu(c, t, alternative=\"two-sided\").pvalue\nassert unpaired_es.pvalue_mann_whitney == pytest.approx(p1)\n\np2 = sp.stats.ttest_ind(c, t, nan_policy='omit').pvalue\nassert unpaired_es.pvalue_students_t == pytest.approx(p2)\n\np3 = sp.stats.ttest_ind(c, t, equal_var=False, nan_policy='omit').pvalue\nassert unpaired_es.pvalue_welch == pytest.approx(p3)\n\ntest_paired_stats\n\nbefore = paired_wellbeing.pre\nafter = paired_wellbeing.post\n\npaired_es = TwoGroupsEffectSize(before, after, \"mean_diff\", is_paired=\"baseline\", proportional=False)\n\np1 = sp.stats.ttest_rel(before, after, nan_policy='omit').pvalue\nassert paired_es.pvalue_paired_students_t == pytest.approx(p1)\n\np2 = sp.stats.wilcoxon(before, after).pvalue\nassert paired_es.pvalue_wilcoxon == pytest.approx(p2)\n\ntest_median_diff_stats\n\nc = wellbeing.control\nt = wellbeing.expt\n\nes = TwoGroupsEffectSize(c, t, \"median_diff\", is_paired=False, proportional=False)\n\np1 = sp.stats.kruskal(c, t, nan_policy='omit').pvalue\nassert es.pvalue_kruskal == pytest.approx(p1)\n\nC:\\Users\\zhang\\Desktop\\vnbdev-dabest\\DABEST-python\\dabest\\effsize.py:77: UserWarning: Using median as the statistic in bootstrapping may result in a biased estimate and cause problems with BCa confidence intervals. Consider using a different statistic, such as the mean.\nWhen plotting, please consider using percetile confidence intervals by specifying `ci_type='percentile'`. For detailed information, refer to https://github.com/ACCLAB/DABEST-python/issues/129 \n\n  warnings.warn(message=mes1+mes2, category=UserWarning)\n\n\ntest_ordinal_dominance\n\nes = TwoGroupsEffectSize(likert_control, likert_treatment, \n                             \"cliffs_delta\", is_paired=False, proportional=False)\n                             \np1 = sp.stats.brunnermunzel(likert_control, likert_treatment).pvalue\nassert es.pvalue_brunner_munzel == pytest.approx(p1)\n\ntest_unpaired_permutation_test\n\nperm_test = PermutationTest(wellbeing.control, wellbeing.expt, \n                                effect_size=\"mean_diff\", \n                                is_paired=False)\nassert perm_test.pvalue == pytest.approx(0.2976)\n\ntest_paired_permutation_test\n\nperm_test = PermutationTest(paired_wellbeing.pre, \n                                paired_wellbeing.post, \n                                effect_size=\"mean_diff\", \n                                is_paired=\"baseline\")\nassert perm_test.pvalue == pytest.approx(0.0124)\n\ntest_lqrt_unpaired\n\nunpaired_dabest = Dabest(wellbeing, idx=(\"control\", \"expt\"), \n                             paired=None, id_col=None, \n                             **dabest_default_kwargs)\nlqrt_result = unpaired_dabest.mean_diff.lqrt\n\np1 = lqrt.lqrtest_ind(wellbeing.control, wellbeing.expt,\n                      equal_var=True,\n                      random_state=12345)\n\np2 = lqrt.lqrtest_ind(wellbeing.control, wellbeing.expt,\n                      equal_var=False,\n                      random_state=12345)\n\nassert lqrt_result.pvalue_lqrt_equal_var[0] == pytest.approx(p1.pvalue)\nassert lqrt_result.pvalue_lqrt_unequal_var[0] == pytest.approx(p2.pvalue)\n\ntest_lqrt_paired\n\npaired_dabest = Dabest(paired_wellbeing, idx=(\"pre\", \"post\"),\n                           paired=\"baseline\", id_col=\"ID\",\n                           **dabest_default_kwargs)\nlqrt_result = paired_dabest.mean_diff.lqrt\n\np1 = lqrt.lqrtest_rel(paired_wellbeing.pre, paired_wellbeing.post, \n             random_state=12345)\n\nassert lqrt_result.pvalue_paired_lqrt[0] == pytest.approx(p1.pvalue)"
  },
  {
    "objectID": "read_me.html",
    "href": "read_me.html",
    "title": "DABEST-Python",
    "section": "",
    "text": "On 20 March 2023, we officially released DABEST v2023.02.14 for Python. This new version provided the following new features:\n\nRepeated measures. Augments the prior function for plotting (independent) multiple test groups versus a shared control; it can now do the same for repeated-measures experimental designs. Thus, together, these two methods can be used to replace both flavors of the 1-way ANOVA with an estimation analysis.\nProportional data. Generates proportional bar plots, proportional differences, and calculates Cohen’s h. Also enables plotting Sankey diagrams for paired binary data. This is the estimation equivalent to a bar chart with Fischer’s exact test.\nThe \\(\\Delta\\Delta\\) plot. Calculates the delta-delta (\\(\\Delta\\Delta\\)) for 2 × 2 experimental designs and plots the four groups with their relevant effect sizes. This design can be used as a replacement for the 2 × 2 ANOVA.\nMini-meta. Calculates and plots a weighted delta (\\(\\Delta\\)) for meta-analysis of experimental replicates. Useful for summarizing data from multiple replicated experiments, for example by different scientists in the same lab, or the same scientist at different times. When the observed values are known (and share a common metric), this makes meta-analysis available as a routinely accessible tool."
  },
  {
    "objectID": "read_me.html#recent-version-update",
    "href": "read_me.html#recent-version-update",
    "title": "DABEST-Python",
    "section": "",
    "text": "On 20 March 2023, we officially released DABEST v2023.02.14 for Python. This new version provided the following new features:\n\nRepeated measures. Augments the prior function for plotting (independent) multiple test groups versus a shared control; it can now do the same for repeated-measures experimental designs. Thus, together, these two methods can be used to replace both flavors of the 1-way ANOVA with an estimation analysis.\nProportional data. Generates proportional bar plots, proportional differences, and calculates Cohen’s h. Also enables plotting Sankey diagrams for paired binary data. This is the estimation equivalent to a bar chart with Fischer’s exact test.\nThe \\(\\Delta\\Delta\\) plot. Calculates the delta-delta (\\(\\Delta\\Delta\\)) for 2 × 2 experimental designs and plots the four groups with their relevant effect sizes. This design can be used as a replacement for the 2 × 2 ANOVA.\nMini-meta. Calculates and plots a weighted delta (\\(\\Delta\\)) for meta-analysis of experimental replicates. Useful for summarizing data from multiple replicated experiments, for example by different scientists in the same lab, or the same scientist at different times. When the observed values are known (and share a common metric), this makes meta-analysis available as a routinely accessible tool."
  },
  {
    "objectID": "read_me.html#contents",
    "href": "read_me.html#contents",
    "title": "DABEST-Python",
    "section": "Contents",
    "text": "Contents\n\n\nAbout\nInstallation\nUsage\nHow to cite\nBugs\nContributing\nAcknowledgements\nTesting\nDABEST in other languages"
  },
  {
    "objectID": "read_me.html#about",
    "href": "read_me.html#about",
    "title": "DABEST-Python",
    "section": "About",
    "text": "About\nDABEST is a package for Data Analysis using Bootstrap-Coupled ESTimation.\nEstimation statistics is a simple framework that avoids the pitfalls of significance testing. It uses familiar statistical concepts: means, mean differences, and error bars. More importantly, it focuses on the effect size of one’s experiment/intervention, as opposed to a false dichotomy engendered by P values.\nAn estimation plot has two key features.\n\nIt presents all datapoints as a swarmplot, which orders each point to display the underlying distribution.\nIt presents the effect size as a bootstrap 95% confidence interval on a separate but aligned axes.\n\n\n\n\nThe five kinds of estimation plots\n\n\nDABEST powers estimationstats.com, allowing everyone access to high-quality estimation plots."
  },
  {
    "objectID": "read_me.html#installation",
    "href": "read_me.html#installation",
    "title": "DABEST-Python",
    "section": "Installation",
    "text": "Installation\nThis package is tested on Python 3.6, 3.7, and 3.8. It is highly recommended to download the Anaconda distribution of Python in order to obtain the dependencies easily.\nYou can install this package via pip.\nTo install, at the command line run \npip install --upgrade dabest\nYou can also clone this repo locally.\nThen, navigate to the cloned repo in the command line and run\npip install ."
  },
  {
    "objectID": "read_me.html#usage",
    "href": "read_me.html#usage",
    "title": "DABEST-Python",
    "section": "Usage",
    "text": "Usage\nimport pandas as pd\nimport dabest\n\n# Load the iris dataset. Requires internet access.\niris = pd.read_csv(\"https://github.com/mwaskom/seaborn-data/raw/master/iris.csv\")\n\n# Load the above data into `dabest`.\niris_dabest = dabest.load(data=iris, x=\"species\", y=\"petal_width\",\n                          idx=(\"setosa\", \"versicolor\", \"virginica\"))\n\n# Produce a Cumming estimation plot.\niris_dabest.mean_diff.plot();\n\n\n\nA Cumming estimation plot of petal width from the iris dataset\n\n\nPlease refer to the official tutorial for more useful code snippets."
  },
  {
    "objectID": "read_me.html#how-to-cite",
    "href": "read_me.html#how-to-cite",
    "title": "DABEST-Python",
    "section": "How to cite",
    "text": "How to cite\nMoving beyond P values: Everyday data analysis with estimation plots\nJoses Ho, Tayfun Tumkaya, Sameer Aryal, Hyungwon Choi, Adam Claridge-Chang\nNature Methods 2019, 1548-7105. 10.1038/s41592-019-0470-3\nPaywalled publisher site; Free-to-view PDF"
  },
  {
    "objectID": "read_me.html#bugs",
    "href": "read_me.html#bugs",
    "title": "DABEST-Python",
    "section": "Bugs",
    "text": "Bugs\nPlease report any bugs on the Github issue tracker."
  },
  {
    "objectID": "read_me.html#contributing",
    "href": "read_me.html#contributing",
    "title": "DABEST-Python",
    "section": "Contributing",
    "text": "Contributing\nAll contributions are welcome; please read the Guidelines for contributing first.\nWe also have a Code of Conduct to foster an inclusive and productive space.\n\nA wish list for new features\nCurrently, DABEST offers functions to handle data traditionally analyzed with Student’s paired and unpaired t-tests. It also offers plots for multiplexed versions of these, and the estimation counterpart to a 1-way analysis of variance (ANOVA), the shared-control design. While these five functions execute a large fraction of common biomedical data analyses, there remain three others: 2-way data, time-series group data, and proportional data. We aim to add these new functions to both the R and Python libraries.\n\nIn many experiments, four groups are investigate to isolate an interaction, for example: a genotype × drug effect. Here, wild-type and mutant animals are each subjected to drug or sham treatments; the data are traditionally analysed with a 2×2 ANOVA. We have received requests by email, Twitter, and GitHub to implement an estimation counterpart to the 2-way ANOVA. To do this, we will implement \\(\\Delta\\Delta\\) plots, in which the difference of means (\\(\\Delta\\)) of two groups is subtracted from a second two-group \\(\\Delta\\). Implemented in v2023.02.14.\nCurrently, DABEST can analyse multiple paired data in a single plot, and multiple groups with a common, shared control. However, a common design in biomedical science is to follow the same group of subjects over multiple, successive time points. An estimation plot for this would combine elements of the two other designs, and could be used in place of a repeated-measures ANOVA. Implemented in v2023.02.14\nWe have observed that proportional data are often analyzed in neuroscience and other areas of biomedical research. However, compared to other data types, the charts are frequently impoverished: often, they omit error bars, sample sizes, and even P values—let alone effect sizes. We would like DABEST to feature proportion charts, with error bars and a curve for the distribution of the proportional differences. Implemented in v2023.02.14\n\nWe encourage contributions for the above features."
  },
  {
    "objectID": "read_me.html#acknowledgements",
    "href": "read_me.html#acknowledgements",
    "title": "DABEST-Python",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe would like to thank alpha testers from the Claridge-Chang lab: Sangyu Xu, Xianyuan Zhang, Farhan Mohammad, Jurga Mituzaitė, and Stanislav Ott."
  },
  {
    "objectID": "read_me.html#testing",
    "href": "read_me.html#testing",
    "title": "DABEST-Python",
    "section": "Testing",
    "text": "Testing\nTo test DABEST, you will need to install pytest.\nRun pytest in the root directory of the source distribution. This runs the test suite in the folder dabest/tests. The test suite will ensure that the bootstrapping functions and the plotting functions perform as expected."
  },
  {
    "objectID": "read_me.html#dabest-in-other-languages",
    "href": "read_me.html#dabest-in-other-languages",
    "title": "DABEST-Python",
    "section": "DABEST in other languages",
    "text": "DABEST in other languages\nDABEST is also available in R (dabestr) and Matlab (DABEST-Matlab)."
  },
  {
    "objectID": "01-getting_started.html",
    "href": "01-getting_started.html",
    "title": "Getting Started",
    "section": "",
    "text": "Python 3.8 is strongly recommended. DABEST has also been tested with Python 3.6 and 3.7.\nIn addition, the following packages are also required (listed with their minimal versions):\n\nnumpy 1.22.3\nscipy 1.9.3\nmatplotlib 3.5.1\npandas 1.5.0\nseaborn 0.11.2\nlqrt 0.3\n\nTo obtain these package dependencies easily, it is highly recommended to download the Anaconda distribution of Python."
  },
  {
    "objectID": "01-getting_started.html#requirements",
    "href": "01-getting_started.html#requirements",
    "title": "Getting Started",
    "section": "",
    "text": "Python 3.8 is strongly recommended. DABEST has also been tested with Python 3.6 and 3.7.\nIn addition, the following packages are also required (listed with their minimal versions):\n\nnumpy 1.22.3\nscipy 1.9.3\nmatplotlib 3.5.1\npandas 1.5.0\nseaborn 0.11.2\nlqrt 0.3\n\nTo obtain these package dependencies easily, it is highly recommended to download the Anaconda distribution of Python."
  },
  {
    "objectID": "01-getting_started.html#installation",
    "href": "01-getting_started.html#installation",
    "title": "Getting Started",
    "section": "Installation",
    "text": "Installation\n\nUsing pip\n\nAt the command line, run\n$ pip install dabest\n\nUsing Github\n\nClone the DABEST-python repo locally (see instructions [here] (https://help.github.com/articles/cloning-a-repository/).\nThen, navigate to the cloned repo in the command line and run\n$ pip install"
  },
  {
    "objectID": "01-getting_started.html#testing",
    "href": "01-getting_started.html#testing",
    "title": "Getting Started",
    "section": "Testing",
    "text": "Testing\nTo test DABEST, you will need to install pytest.\nRun pytest in the root directory of the source distribution. This runs the test suite in dabest/tests folder. The test suite will ensure that the bootstrapping functions and the plotting functions perform as expected."
  },
  {
    "objectID": "01-getting_started.html#bugs",
    "href": "01-getting_started.html#bugs",
    "title": "Getting Started",
    "section": "Bugs",
    "text": "Bugs\nPlease report any bugs on the Github issue tracker for DABEST-python."
  },
  {
    "objectID": "01-getting_started.html#contributing",
    "href": "01-getting_started.html#contributing",
    "title": "Getting Started",
    "section": "Contributing",
    "text": "Contributing\nAll contributions are welcome. Please fork the Github repo and open a pull request."
  },
  {
    "objectID": "API/misc_tools.html",
    "href": "API/misc_tools.html",
    "title": "misc_tools",
    "section": "",
    "text": "source\n\nget_varname\n\n get_varname (obj)\n\n\nsource\n\n\nprint_greeting\n\n print_greeting ()\n\n\nsource\n\n\nunpack_and_add\n\n unpack_and_add (l, c)\n\nConvenience function to allow me to add to an existing list without altering that list.\n\nsource\n\n\nmerge_two_dicts\n\n merge_two_dicts (x:dict, y:dict)\n\nGiven two dicts, merge them into a new dict as a shallow copy. Any overlapping keys in y will override the values in x.\nTaken from here\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nx\ndict\n\n\n\ny\ndict\n\n\n\nReturns\ndict\nA dictionary containing a union of all keys in both original dicts."
  },
  {
    "objectID": "API/plotter.html",
    "href": "API/plotter.html",
    "title": "Plot",
    "section": "",
    "text": "source\n\nEffectSizeDataFramePlotter\n\n EffectSizeDataFramePlotter (EffectSizeDataFrame, **plot_kwargs)\n\nCustom function that creates an estimation plot from an EffectSizeDataFrame.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nEffectSizeDataFrame\n\nA dabest EffectSizeDataFrame object.\n\n\nplot_kwargs\n\ncolor_col=Noneraw_marker_size=6, es_marker_size=9,swarm_label=None, contrast_label=None, delta2_label=None,swarm_ylim=None, contrast_ylim=None, delta2_ylim=None,custom_palette=None, swarm_desat=0.5, halfviolin_desat=1,halfviolin_alpha=0.8,face_color = None,bar_label=None, bar_desat=0.8, bar_width = 0.5,bar_ylim = None,ci=None, ci_type=‘bca’, err_color=None,float_contrast=True,show_pairs=True,show_delta2=True,group_summaries=None,group_summaries_offset=0.1,fig_size=None,dpi=100,ax=None,swarmplot_kwargs=None,violinplot_kwargs=None,slopegraph_kwargs=None,sankey_kwargs=None,reflines_kwargs=None,group_summary_kwargs=None,legend_kwargs=None,"
  },
  {
    "objectID": "API/confint_2group_diff.html",
    "href": "API/confint_2group_diff.html",
    "title": "confint_2group_diff",
    "section": "",
    "text": "source\n\ncalculate_weighted_delta\n\n calculate_weighted_delta (group_var, differences, resamples)\n\nCompute the weighted deltas.\n\nsource\n\n\ncalculate_group_var\n\n calculate_group_var (control_var, control_N, test_var, test_N)\n\n\nsource\n\n\ncompute_interval_limits\n\n compute_interval_limits (bias, acceleration, n_boots, ci=95)\n\nReturns the indexes of the interval limits for a given bootstrap.\nSupply the bias, acceleration factor, and number of bootstraps.\n\nsource\n\n\ncompute_meandiff_bias_correction\n\n compute_meandiff_bias_correction (bootstraps, effsize)\n\nComputes the bias correction required for the BCa method of confidence interval construction.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nbootstraps\n\nAn numerical iterable, comprising bootstrap resamples of the effect size.\n\n\neffsize\n\nThe effect size for the original sample.\n\n\nReturns\nbias: numeric\nThe bias correction value for the given bootstrapsand effect size.\n\n\n\n\nsource\n\n\ncompute_bootstrapped_diff\n\n compute_bootstrapped_diff (x0, x1, is_paired, effect_size,\n                            resamples=5000, random_seed=12345)\n\nBootstraps the effect_size for 2 groups.\n\nsource\n\n\ncompute_meandiff_jackknife\n\n compute_meandiff_jackknife (x0, x1, is_paired, effect_size)\n\nGiven two arrays, returns the jackknife for their effect size.\n\nsource\n\n\ncreate_repeated_indexes\n\n create_repeated_indexes (data)\n\nConvenience function. Given an array-like with length N, returns a generator that yields N indexes [0, 1, …, N].\n/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Keywords\n  else: warn(msg)\n\nsource\n\n\ncreate_jackknife_indexes\n\n create_jackknife_indexes (data)\n\nGiven an array-like, creates a jackknife bootstrap.\nFor a given set of data Y, the jackknife bootstrap sample J[i] is defined as the data set Y with the ith data point deleted.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ndata\n\n\n\n\nReturns\nGenerator that yields all jackknife bootstrap samples."
  },
  {
    "objectID": "API/class.html",
    "href": "API/class.html",
    "title": "Class",
    "section": "",
    "text": "Dabest\n\n Dabest (data, idx, x, y, paired, id_col, ci, resamples, random_seed,\n         proportional, delta2, experiment, experiment_label, x1_level,\n         mini_meta)\n\nClass for estimation statistics and plots.\n\nExample: mean_diff\n\ncontrol = norm.rvs(loc=0, size=30, random_state=12345)\ntest    = norm.rvs(loc=0.5, size=30, random_state=12345)\nmy_df   = pd.DataFrame({\"control\": control,\n                            \"test\": test})\nmy_dabest_object = dabest.load(my_df, idx=(\"control\", \"test\"))\nmy_dabest_object.mean_diff\n\nDABEST v2023.2.14\n=================\n                 \nGood evening!\nThe current time is Fri Mar 31 19:41:17 2023.\n\nThe unpaired mean difference between control and test is 0.5 [95%CI -0.0412, 1.0].\nThe p-value of the two-sided permutation t-test is 0.0758, calculated for legacy purposes only. \n\n5000 bootstrap samples were taken; the confidence interval is bias-corrected and accelerated.\nAny p-value reported is the probability of observing theeffect size (or greater),\nassuming the null hypothesis ofzero difference is true.\nFor each p-value, 5000 reshuffles of the control and test labels were performed.\n\nTo get the results of all valid statistical tests, use `.mean_diff.statistical_tests`\n\n\nThis is simply the mean of the control group subtracted from the mean of the test group.\n\\[\\text{Mean difference} = \\overline{x}_{Test} - \\overline{x}_{Control}\\]\nwhere \\(\\overline{x}\\) is the mean for the group \\(x\\).\n\n\nExample: median_diff\n\ncontrol = norm.rvs(loc=0, size=30, random_state=12345)\ntest    = norm.rvs(loc=0.5, size=30, random_state=12345)\nmy_df   = pd.DataFrame({\"control\": control,\n                            \"test\": test})\nmy_dabest_object = dabest.load(my_df, idx=(\"control\", \"test\"))\nmy_dabest_object.median_diff\n\nc:\\users\\zhang\\desktop\\vnbdev-dabest\\dabest-python\\dabest\\effsize.py:72: UserWarning: Using median as the statistic in bootstrapping may result in a biased estimate and cause problems with BCa confidence intervals. Consider using a different statistic, such as the mean.\nWhen plotting, please consider using percetile confidence intervals by specifying `ci_type='percentile'`. For detailed information, refer to https://github.com/ACCLAB/DABEST-python/issues/129 \n\n  return func_difference(control, test, np.median, is_paired)\n\n\nDABEST v2023.2.14\n=================\n                 \nGood afternoon!\nThe current time is Thu Mar 30 17:07:33 2023.\n\nThe unpaired median difference between control and test is 0.5 [95%CI -0.0758, 0.991].\nThe p-value of the two-sided permutation t-test is 0.103, calculated for legacy purposes only. \n\n5000 bootstrap samples were taken; the confidence interval is bias-corrected and accelerated.\nAny p-value reported is the probability of observing theeffect size (or greater),\nassuming the null hypothesis ofzero difference is true.\nFor each p-value, 5000 reshuffles of the control and test labels were performed.\n\nTo get the results of all valid statistical tests, use `.median_diff.statistical_tests`\n\n\nThis is the median difference between the control group and the test group.\nIf the comparison(s) are unpaired, median_diff is computed with the following equation:\n\\[\\text{Median difference} = \\widetilde{x}_{Test} - \\widetilde{x}_{Control}\\]\nwhere \\(\\widetilde{x}\\) is the median for the group \\(x\\).\nIf the comparison(s) are paired, median_diff is computed with the following equation:\n\\[\\text{Median difference} = \\widetilde{x}_{Test - Control}\\]\n\nThings to note\nUsing median difference as the statistic in bootstrapping may result in a biased estimate and cause problems with BCa confidence intervals. Consider using mean difference instead.\nWhen plotting, consider using percentile confidence intervals instead of BCa confidence intervals by specifying ci_type = 'percentile' in .plot().\nFor detailed information, please refer to Issue 129.\n\n\n\nExample: cohens_d\n\ncontrol = norm.rvs(loc=0, size=30, random_state=12345)\ntest    = norm.rvs(loc=0.5, size=30, random_state=12345)\nmy_df   = pd.DataFrame({\"control\": control,\n                            \"test\": test})\nmy_dabest_object = dabest.load(my_df, idx=(\"control\", \"test\"))\nmy_dabest_object.cohens_d\n\nDABEST v2023.2.14\n=================\n                 \nGood afternoon!\nThe current time is Thu Mar 30 17:07:39 2023.\n\nThe unpaired Cohen's d between control and test is 0.471 [95%CI -0.0843, 0.976].\nThe p-value of the two-sided permutation t-test is 0.0758, calculated for legacy purposes only. \n\n5000 bootstrap samples were taken; the confidence interval is bias-corrected and accelerated.\nAny p-value reported is the probability of observing theeffect size (or greater),\nassuming the null hypothesis ofzero difference is true.\nFor each p-value, 5000 reshuffles of the control and test labels were performed.\n\nTo get the results of all valid statistical tests, use `.cohens_d.statistical_tests`\n\n\nCohen’s d is simply the mean of the control group subtracted from the mean of the test group.\nIf paired is None, then the comparison(s) are unpaired; otherwise the comparison(s) are paired.\nIf the comparison(s) are unpaired, Cohen’s d is computed with the following equation:\n\\[d = \\frac{\\overline{x}_{Test} - \\overline{x}_{Control}} {\\text{pooled standard deviation}}\\]\nFor paired comparisons, Cohen’s d is given by\n\\[d = \\frac{\\overline{x}_{Test} - \\overline{x}_{Control}} {\\text{average standard deviation}}\\]\nwhere \\(\\overline{x}\\) is the mean of the respective group of observations, \\({Var}_{x}\\) denotes the variance of that group,\n\\[\\text{pooled standard deviation} = \\sqrt{ \\frac{(n_{control} - 1) * {Var}_{control} + (n_{test} - 1) * {Var}_{test} } {n_{control} + n_{test} - 2} }\\]\nand\n\\[\\text{average standard deviation} = \\sqrt{ \\frac{{Var}_{control} + {Var}_{test}} {2}}\\]\nThe sample variance (and standard deviation) uses N-1 degrees of freedoms. This is an application of Bessel’s correction, and yields the unbiased sample variance.\nReferences:\nhttps://en.wikipedia.org/wiki/Effect_size#Cohen's_d\nhttps://en.wikipedia.org/wiki/Bessel%27s_correction\nhttps://en.wikipedia.org/wiki/Standard_deviation#Corrected_sample_standard_deviation\n\n\nExample: cohens_h\n\ncontrol = randint.rvs(0, 2, size=30, random_state=12345)\ntest    = randint.rvs(0, 2, size=30, random_state=12345)\nmy_df   = pd.DataFrame({\"control\": control,\n                            \"test\": test})\nmy_dabest_object = dabest.load(my_df, idx=(\"control\", \"test\"))\nmy_dabest_object.cohens_h\n\nDABEST v2023.2.14\n=================\n                 \nGood evening!\nThe current time is Mon Mar 27 00:48:59 2023.\n\nThe unpaired Cohen's h between control and test is 0.0 [95%CI -0.613, 0.429].\nThe p-value of the two-sided permutation t-test is 0.799, calculated for legacy purposes only. \n\n5000 bootstrap samples were taken; the confidence interval is bias-corrected and accelerated.\nAny p-value reported is the probability of observing theeffect size (or greater),\nassuming the null hypothesis ofzero difference is true.\nFor each p-value, 5000 reshuffles of the control and test labels were performed.\n\nTo get the results of all valid statistical tests, use `.cohens_h.statistical_tests`\n\n\nCohen’s h uses the information of proportion in the control and test groups to calculate the distance between two proportions.\nIt can be used to describe the difference between two proportions as “small”, “medium”, or “large”.\nIt can be used to determine if the difference between two proportions is “meaningful”.\nA directional Cohen’s h is computed with the following equation:\n\\[h = 2 * \\arcsin{\\sqrt{proportion_{Test}}} - 2 * \\arcsin{\\sqrt{proportion_{Control}}}\\]\nFor a non-directional Cohen’s h, the equation is:\n\\[h = |2 * \\arcsin{\\sqrt{proportion_{Test}}} - 2 * \\arcsin{\\sqrt{proportion_{Control}}}|\\]\nReferences:\nhttps://en.wikipedia.org/wiki/Cohen%27s_h\n\n\nExample: hedges_g\n\ncontrol = norm.rvs(loc=0, size=30, random_state=12345)\ntest    = norm.rvs(loc=0.5, size=30, random_state=12345)\nmy_df   = pd.DataFrame({\"control\": control,\n                            \"test\": test})\nmy_dabest_object = dabest.load(my_df, idx=(\"control\", \"test\"))\nmy_dabest_object.hedges_g\n\nDABEST v2023.2.14\n=================\n                 \nGood evening!\nThe current time is Mon Mar 27 00:50:18 2023.\n\nThe unpaired Hedges' g between control and test is 0.465 [95%CI -0.0832, 0.963].\nThe p-value of the two-sided permutation t-test is 0.0758, calculated for legacy purposes only. \n\n5000 bootstrap samples were taken; the confidence interval is bias-corrected and accelerated.\nAny p-value reported is the probability of observing theeffect size (or greater),\nassuming the null hypothesis ofzero difference is true.\nFor each p-value, 5000 reshuffles of the control and test labels were performed.\n\nTo get the results of all valid statistical tests, use `.hedges_g.statistical_tests`\n\n\nHedges’ g is cohens_d corrected for bias via multiplication with the following correction factor:\n\\[\\frac{ \\Gamma( \\frac{a} {2} )} {\\sqrt{ \\frac{a} {2} } \\times \\Gamma( \\frac{a - 1} {2} )}\\]\nwhere\n\\[a = {n}_{control} + {n}_{test} - 2\\]\nand \\(\\Gamma(x)\\) is the Gamma function.\nReferences:\nhttps://en.wikipedia.org/wiki/Effect_size#Hedges'_g\nhttps://journals.sagepub.com/doi/10.3102/10769986006002107\n\n\nExample: cliffs_delta\n\ncontrol = norm.rvs(loc=0, size=30, random_state=12345)\ntest    = norm.rvs(loc=0.5, size=30, random_state=12345)\nmy_df   = pd.DataFrame({\"control\": control,\n                            \"test\": test})\nmy_dabest_object = dabest.load(my_df, idx=(\"control\", \"test\"))\nmy_dabest_object.cliffs_delta\n\nDABEST v2023.2.14\n=================\n                 \nGood evening!\nThe current time is Mon Mar 27 00:53:30 2023.\n\nThe unpaired Cliff's delta between control and test is 0.28 [95%CI -0.0244, 0.533].\nThe p-value of the two-sided permutation t-test is 0.061, calculated for legacy purposes only. \n\n5000 bootstrap samples were taken; the confidence interval is bias-corrected and accelerated.\nAny p-value reported is the probability of observing theeffect size (or greater),\nassuming the null hypothesis ofzero difference is true.\nFor each p-value, 5000 reshuffles of the control and test labels were performed.\n\nTo get the results of all valid statistical tests, use `.cliffs_delta.statistical_tests`\n\n\nCliff’s delta is a measure of ordinal dominance, ie. how often the values from the test sample are larger than values from the control sample.\n\\[\\text{Cliff's delta} = \\frac{\\#({x}_{test} &gt; {x}_{control}) - \\#({x}_{test} &lt; {x}_{control})} {{n}_{Test} \\times {n}_{Control}}\\]\nwhere \\(\\#\\) denotes the number of times a value from the test sample exceeds (or is lesser than) values in the control sample.\nCliff’s delta ranges from -1 to 1; it can also be thought of as a measure of the degree of overlap between the two samples. An attractive aspect of this effect size is that it does not make an assumptions about the underlying distributions that the samples were drawn from.\nReferences:\nhttps://en.wikipedia.org/wiki/Effect_size#Effect_size_for_ordinal_data\nhttps://psycnet.apa.org/record/1994-08169-001\n\n\n\n\nDeltaDelta\n\n DeltaDelta (effectsizedataframe, permutation_count, ci=95)\n\nA class to compute and store the delta-delta statistics for experiments with a 2-by-2 arrangement where two independent variables, A and B, each have two categorical values, 1 and 2. The data is divided into two pairs of two groups, and a primary delta is first calculated as the mean difference between each of the pairs:\n\\[\\Delta_{1} = \\overline{X}_{A_{2}, B_{1}} - \\overline{X}_{A_{1}, B_{1}}\\]\n\\[\\Delta_{2} = \\overline{X}_{A_{2}, B_{2}} - \\overline{X}_{A_{1}, B_{2}}\\]\nwhere \\(\\overline{X}_{A_{i}, B_{j}}\\) is the mean of the sample with A = i and B = j, \\(\\Delta\\) is the mean difference between two samples.\nA delta-delta value is then calculated as the mean difference between the two primary deltas:\n\\[\\Delta_{\\Delta} = \\Delta_{2} - \\Delta_{1}\\]\nand the standard deviation of the delta-delta value is calculated from a pooled variance of the 4 samples:\n\\[s_{\\Delta_{\\Delta}} = \\sqrt{\\frac{(n_{A_{2}, B_{1}}-1)s_{A_{2}, B_{1}}^2+(n_{A_{1}, B_{1}}-1)s_{A_{1}, B_{1}}^2+(n_{A_{2}, B_{2}}-1)s_{A_{2}, B_{2}}^2+(n_{A_{1}, B_{2}}-1)s_{A_{1}, B_{2}}^2}{(n_{A_{2}, B_{1}} - 1) + (n_{A_{1}, B_{1}} - 1) + (n_{A_{2}, B_{2}} - 1) + (n_{A_{1}, B_{2}} - 1)}}\\]\nwhere \\(s\\) is the standard deviation and \\(n\\) is the sample size.\n\nExample: delta-delta\n\nnp.random.seed(9999) # Fix the seed so the results are replicable.\nN = 20\n# Create samples\ny = norm.rvs(loc=3, scale=0.4, size=N*4)\ny[N:2*N] = y[N:2*N]+1\ny[2*N:3*N] = y[2*N:3*N]-0.5\n# Add a `Treatment` column\nt1 = np.repeat('Placebo', N*2).tolist()\nt2 = np.repeat('Drug', N*2).tolist()\ntreatment = t1 + t2 \n# Add a `Rep` column as the first variable for the 2 replicates of experiments done\nrep = []\nfor i in range(N*2):\n    rep.append('Rep1')\n    rep.append('Rep2')\n# Add a `Genotype` column as the second variable\nwt = np.repeat('W', N).tolist()\nmt = np.repeat('M', N).tolist()\nwt2 = np.repeat('W', N).tolist()\nmt2 = np.repeat('M', N).tolist()\ngenotype = wt + mt + wt2 + mt2\n# Add an `id` column for paired data plotting.\nid = list(range(0, N*2))\nid_col = id + id \n# Combine all columns into a DataFrame.\ndf_delta2 = pd.DataFrame({'ID'        : id_col,\n                  'Rep'      : rep,\n                   'Genotype'  : genotype, \n                   'Treatment': treatment,\n                   'Y'         : y\n                })\nunpaired_delta2 = dabest.load(data = df_delta2, x = [\"Genotype\", \"Genotype\"], y = \"Y\", delta2 = True, experiment = \"Treatment\")\nunpaired_delta2.mean_diff.plot();\n\n\n\n\n\n\n\n\nMiniMetaDelta\n\n MiniMetaDelta (effectsizedataframe, permutation_count, ci=95)\n\nA class to compute and store the weighted delta. A weighted delta is calculated if the argument mini_meta=True is passed during dabest.load().\nThe weighted delta is calcuated as follows:\n\\[\\theta_{\\text{weighted}} = \\frac{\\Sigma\\hat{\\theta_{i}}w_{i}}{{\\Sigma}w_{i}}\\]\nwhere:\n\\[\\hat{\\theta_{i}} = \\text{Mean difference for replicate }i\\]\n\\[w_{i} = \\text{Weight for replicate }i = \\frac{1}{s_{i}^2} \\]\n\\[s_{i}^2 = \\text{Pooled variance for replicate }i = \\frac{(n_{test}-1)s_{test}^2+(n_{control}-1)s_{control}^2}{n_{test}+n_{control}-2}\\]\n\\[n = \\text{sample size and }s^2 = \\text{variance for control/test.}\\]\n\nExample: mini-meta-delta\n\nNs = 20\nc1 = norm.rvs(loc=3, scale=0.4, size=Ns)\nc2 = norm.rvs(loc=3.5, scale=0.75, size=Ns)\nc3 = norm.rvs(loc=3.25, scale=0.4, size=Ns)\nt1 = norm.rvs(loc=3.5, scale=0.5, size=Ns)\nt2 = norm.rvs(loc=2.5, scale=0.6, size=Ns)\nt3 = norm.rvs(loc=3, scale=0.75, size=Ns)\nmy_df   = pd.DataFrame({'Control 1' : c1,     'Test 1' : t1,\n                   'Control 2' : c2,     'Test 2' : t2,\n                   'Control 3' : c3,     'Test 3' : t3})\nmy_dabest_object = dabest.load(my_df, idx=((\"Control 1\", \"Test 1\"), (\"Control 2\", \"Test 2\"), (\"Control 3\", \"Test 3\")), mini_meta=True)\nmy_dabest_object.mean_diff.mini_meta_delta\n\nDABEST v2023.2.14\n=================\n                 \nGood morning!\nThe current time is Mon Mar 27 01:01:11 2023.\n\nThe weighted-average unpaired mean differences is 0.0336 [95%CI -0.137, 0.228].\nThe p-value of the two-sided permutation t-test is 0.736, calculated for legacy purposes only. \n\n5000 bootstrap samples were taken; the confidence interval is bias-corrected and accelerated.\nAny p-value reported is the probability of observing theeffect size (or greater),\nassuming the null hypothesis ofzero difference is true.\nFor each p-value, 5000 reshuffles of the control and test labels were performed.\n\n\nAs of version 2023.02.14, weighted delta can only be calculated for mean difference, and not for standardized measures such as Cohen’s d.\nDetails about the calculated weighted delta are accessed as attributes of the mini_meta_delta class. See the minimetadelta for details on usage.\nRefer to Chapter 10 of the Cochrane handbook for further information on meta-analysis: https://training.cochrane.org/handbook/current/chapter-10\n\n\n\n\nTwoGroupsEffectSize\n\n TwoGroupsEffectSize (control, test, effect_size, proportional=False,\n                      is_paired=None, ci=95, resamples=5000,\n                      permutation_count=5000, random_seed=12345)\n\nA class to compute and store the results of bootstrapped mean differences between two groups.\nCompute the effect size between two groups.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncontrol\narray-like\n\n\n\n\ntest\narray-like\n\nThese should be numerical iterables.\n\n\neffect_size\nstring.\n\nAny one of the following are accepted inputs:‘mean_diff’, ‘median_diff’, ‘cohens_d’, ‘hedges_g’, or ‘cliffs_delta’\n\n\nproportional\nbool\nFalse\n\n\n\nis_paired\nNoneType\nNone\n\n\n\nci\nint\n95\nThe confidence interval width. The default of 95 produces 95%confidence intervals.\n\n\nresamples\nint\n5000\nThe number of bootstrap resamples to be taken for the calculationof the confidence interval limits.\n\n\npermutation_count\nint\n5000\nThe number of permutations (reshuffles) to perform for the computation of the permutation p-value\n\n\nrandom_seed\nint\n12345\nrandom_seed is used to seed the random number generator duringbootstrap resampling. This ensures that the confidence intervalsreported are replicable.\n\n\nReturns\npy:class:TwoGroupEffectSize object:\n\ndifference : float The effect size of the difference between the control and the test.effect_size : string The type of effect size reported.is_paired : string The type of repeated-measures experiment.ci : float Returns the width of the confidence interval, in percent.alpha : float Returns the significance level of the statistical test as a float between 0 and 1.resamples : int The number of resamples performed during the bootstrap procedure.bootstraps : numpy ndarray The generated bootstraps of the effect size.random_seed : int The number used to initialise the numpy random seed generator, ie.seed_value from numpy.random.seed(seed_value) is returned.bca_low, bca_high : float The bias-corrected and accelerated confidence interval lower limit and upper limits, respectively.pct_low, pct_high : float The percentile confidence interval lower limit and upper limits, respectively.\n\n\n\n\nExample\n\nnp.random.seed(12345)\ncontrol = norm.rvs(loc=0, size=30)\ntest = norm.rvs(loc=0.5, size=30)\neffsize = dabest.TwoGroupsEffectSize(control, test, \"mean_diff\")\neffsize\n\nThe unpaired mean difference is -0.253 [95%CI -0.78, 0.25].\nThe p-value of the two-sided permutation t-test is 0.348, calculated for legacy purposes only. \n\n5000 bootstrap samples were taken; the confidence interval is bias-corrected and accelerated.\nAny p-value reported is the probability of observing theeffect size (or greater),\nassuming the null hypothesis ofzero difference is true.\nFor each p-value, 5000 reshuffles of the control and test labels were performed.\n\n\n\neffsize.to_dict()\n\n{'alpha': 0.05,\n 'bca_high': 0.24951887238295106,\n 'bca_interval_idx': (125, 4875),\n 'bca_low': -0.7801782111071534,\n 'bootstraps': array([-0.3649424 , -0.45018155, -0.56034412, ..., -0.49805581,\n        -0.25334475, -0.55206229]),\n 'ci': 95,\n 'difference': -0.25315417702752846,\n 'effect_size': 'mean difference',\n 'is_paired': None,\n 'pct_high': 0.24951887238295106,\n 'pct_interval_idx': (125, 4875),\n 'pct_low': -0.7801782111071534,\n 'permutation_count': 5000,\n 'permutations': array([ 0.17221029,  0.03112419, -0.13911387, ..., -0.38007941,\n         0.30261507, -0.09073054]),\n 'permutations_var': array([0.07201642, 0.07251104, 0.07219407, ..., 0.07003705, 0.07094885,\n        0.07238581]),\n 'proportional_difference': nan,\n 'pvalue_brunner_munzel': nan,\n 'pvalue_kruskal': nan,\n 'pvalue_mann_whitney': 0.5201446121616038,\n 'pvalue_mcnemar': nan,\n 'pvalue_paired_students_t': nan,\n 'pvalue_permutation': 0.3484,\n 'pvalue_students_t': 0.34743913903372836,\n 'pvalue_welch': 0.3474493875548964,\n 'pvalue_wilcoxon': nan,\n 'random_seed': 12345,\n 'resamples': 5000,\n 'statistic_brunner_munzel': nan,\n 'statistic_kruskal': nan,\n 'statistic_mann_whitney': 494.0,\n 'statistic_mcnemar': nan,\n 'statistic_paired_students_t': nan,\n 'statistic_students_t': 0.9472545159069105,\n 'statistic_welch': 0.9472545159069105,\n 'statistic_wilcoxon': nan}\n\n\n\n\n\n\nEffectSizeDataFrame\n\n EffectSizeDataFrame (dabest, effect_size, is_paired, ci=95,\n                      proportional=False, resamples=5000,\n                      permutation_count=5000, random_seed=12345,\n                      x1_level=None, x2=None, delta2=False,\n                      experiment_label=None, mini_meta=False)\n\nA class that generates and stores the results of bootstrapped effect sizes for several comparisons.\n\nExample: plot\nCreate a Gardner-Altman estimation plot for the mean difference.\n\nnp.random.seed(9999) # Fix the seed so the results are replicable.\n# pop_size = 10000 # Size of each population.\nNs = 20 # The number of samples taken from each population\n\n# Create samples\nc1 = norm.rvs(loc=3, scale=0.4, size=Ns)\nc2 = norm.rvs(loc=3.5, scale=0.75, size=Ns)\nc3 = norm.rvs(loc=3.25, scale=0.4, size=Ns)\n\nt1 = norm.rvs(loc=3.5, scale=0.5, size=Ns)\nt2 = norm.rvs(loc=2.5, scale=0.6, size=Ns)\nt3 = norm.rvs(loc=3, scale=0.75, size=Ns)\nt4 = norm.rvs(loc=3.5, scale=0.75, size=Ns)\nt5 = norm.rvs(loc=3.25, scale=0.4, size=Ns)\nt6 = norm.rvs(loc=3.25, scale=0.4, size=Ns)\n\n\n# Add a `gender` column for coloring the data.\nfemales = np.repeat('Female', Ns/2).tolist()\nmales = np.repeat('Male', Ns/2).tolist()\ngender = females + males\n\n# Add an `id` column for paired data plotting.\nid_col = pd.Series(range(1, Ns+1))\n\n# Combine samples and gender into a DataFrame.\ndf = pd.DataFrame({'Control 1' : c1,     'Test 1' : t1,\n                 'Control 2' : c2,     'Test 2' : t2,\n                 'Control 3' : c3,     'Test 3' : t3,\n                 'Test 4'    : t4,     'Test 5' : t5, 'Test 6' : t6,\n                 'Gender'    : gender, 'ID'  : id_col\n                })\nmy_data = dabest.load(df, idx=(\"Control 1\", \"Test 1\"))\n\n\nfig1 = my_data.mean_diff.plot();\n\n\n\n\nCreate a Gardner-Altman plot for the Hedges’ g effect size.\n\nfig2 = my_data.hedges_g.plot();\n\n\n\n\nCreate a Cumming estimation plot for the mean difference.\n\nfig3 = my_data.mean_diff.plot(float_contrast=True);\n\n\n\n\nCreate a paired Gardner-Altman plot.\n\nmy_data_paired = dabest.load(df, idx=(\"Control 1\", \"Test 1\"),\n                       id_col = \"ID\", paired='baseline')\nfig4 = my_data_paired.mean_diff.plot();\n\n\n\n\nCreate a multi-group Cumming plot.\n\nmy_multi_groups = dabest.load(df, id_col = \"ID\", \n                             idx=((\"Control 1\", \"Test 1\"),\n                                 (\"Control 2\", \"Test 2\")))\nfig5 = my_multi_groups.mean_diff.plot();\n\n\n\n\nCreate a shared control Cumming plot.\n\nmy_shared_control = dabest.load(df, id_col = \"ID\",\n                                 idx=(\"Control 1\", \"Test 1\",\n                                          \"Test 2\", \"Test 3\"))\nfig6 = my_shared_control.mean_diff.plot();\n\n\n\n\nCreate a repeated meausures (against baseline) Slopeplot.\n\nmy_rm_baseline = dabest.load(df, id_col = \"ID\", paired = \"baseline\",\n                                 idx=(\"Control 1\", \"Test 1\",\n                                          \"Test 2\", \"Test 3\"))\nfig7 = my_rm_baseline.mean_diff.plot();\n\n\n\n\nCreate a repeated meausures (sequential) Slopeplot.\n\nmy_rm_sequential = dabest.load(df, id_col = \"ID\", paired = \"sequential\",\n                                 idx=(\"Control 1\", \"Test 1\",\n                                          \"Test 2\", \"Test 3\"))\nfig8 = my_rm_sequential.mean_diff.plot();\n\n\n\n\n\n\n\n\nPermutationTest\n\n PermutationTest (control:np.array, test:np.array, effect_size:str,\n                  is_paired:str=None, permutation_count:int=5000,\n                  random_seed:int=12345, **kwargs)\n\nA class to compute and report permutation tests.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncontrol\nnp.array\n\n\n\n\ntest\nnp.array\n\nThese should be numerical iterables.\n\n\neffect_size\nstr\n\nAny one of the following are accepted inputs: ‘mean_diff’, ‘median_diff’, ‘cohens_d’, ‘hedges_g’, or ‘cliffs_delta’\n\n\nis_paired\nstr\nNone\n\n\n\npermutation_count\nint\n5000\nThe number of permutations (reshuffles) to perform.\n\n\nrandom_seed\nint\n12345\nrandom_seed is used to seed the random number generator during bootstrap resampling. This ensures that the generated permutations are replicable.\n\n\nkwargs\n\n\n\n\n\nReturns\npy:class:PermutationTest object:\n\ndifference:float The effect size of the difference between the control and the test.effect_size:string The type of effect size reported.\n\n\n\nNotes:\nThe basic concept of permutation tests is the same as that behind bootstrapping. In an “exact” permutation test, all possible resuffles of the control and test labels are performed, and the proportion of effect sizes that equal or exceed the observed effect size is computed. This is the probability, under the null hypothesis of zero difference between test and control groups, of observing the effect size: the p-value of the Student’s t-test.\nExact permutation tests are impractical: computing the effect sizes for all reshuffles quickly exceeds trivial computational loads. A control group and a test group both with 10 observations each would have a total of \\(20!\\) or \\(2.43 \\times {10}^{18}\\) reshuffles. Therefore, in practice, “approximate” permutation tests are performed, where a sufficient number of reshuffles are performed (5,000 or 10,000), from which the p-value is computed.\nMore information can be found here.\n\nExample: permutation test\n\ncontrol = norm.rvs(loc=0, size=30, random_state=12345)\ntest = norm.rvs(loc=0.5, size=30, random_state=12345)\nperm_test = dabest.PermutationTest(control, test, \n                                   effect_size=\"mean_diff\", \n                                   is_paired=None)\nperm_test\n\n5000 permutations were taken. The p-value is 0.0758."
  },
  {
    "objectID": "API/confint_1group.html",
    "href": "API/confint_1group.html",
    "title": "confint_1group",
    "section": "",
    "text": "source\n\nsummary_ci_1group\n\n summary_ci_1group (x:&lt;built-infunctionarray&gt;, func, resamples:int=5000,\n                    alpha:float=0.05, random_seed:int=12345,\n                    sort_bootstraps:bool=True, *args, **kwargs)\n\nGiven an array-like x, returns func(x), and a bootstrap confidence interval of func(x).\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nx\nnp.array\n\nAn numerical iterable.\n\n\nfunc\n\n\nThe function to be applied to x.\n\n\nresamples\nint\n5000\nThe number of bootstrap resamples to be taken of func(x).\n\n\nalpha\nfloat\n0.05\nDenotes the likelihood that the confidence interval produced does not include the true summary statistic. When alpha = 0.05, a 95% confidence interval is produced.\n\n\nrandom_seed\nint\n12345\nrandom_seed is used to seed the random number generator during bootstrap resampling. This ensures that the confidence intervals reported are replicable.\n\n\nsort_bootstraps\nbool\nTrue\n\n\n\nargs\n\n\n\n\n\nkwargs\n\n\n\n\n\nReturns\nA dictionary with the following five keys:\n\nsummary: float. The outcome of func(x).func: function. The function applied to x.bca_ci_low: floatbca_ci_high: float. The bias-corrected and accelerated confidence interval, for the given alpha.bootstraps: array. The bootstraps used to generate the confidence interval. These will be sorted in ascending order if sort_bootstraps was True.\n\n\n\n\nsource\n\n\ncompute_1group_bias_correction\n\n compute_1group_bias_correction (x, bootstraps, func, *args, **kwargs)\n\n\nsource\n\n\ncompute_1group_bootstraps\n\n compute_1group_bootstraps (x, func, resamples=5000, random_seed=12345,\n                            *args, **kwargs)\n\nBootstraps func(x), with the number of specified resamples.\n\nsource\n\n\ncompute_1group_acceleration\n\n compute_1group_acceleration (jack_dist)\n\n\nsource\n\n\ncompute_1group_jackknife\n\n compute_1group_jackknife (x, func, *args, **kwargs)\n\nReturns the jackknife bootstraps for func(x).\n\nsource\n\n\ncreate_bootstrap_indexes\n\n create_bootstrap_indexes (array, resamples=5000, random_seed=12345)\n\nGiven an array-like, returns a generator of bootstrap indexes to be used for resampling."
  },
  {
    "objectID": "API/index.html",
    "href": "API/index.html",
    "title": "API",
    "section": "",
    "text": "This section contains API details for each of dabest’s python submodules. This reference documentation is mainly useful for people looking to customise or build on top of dabest, or wanting detailed information about how dabest works.\n\n\n\n\n\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nLoading Data\n\n\nLoading data and relevant groups\n\n\n\n\nClass\n\n\nSeveral classes for estimating statistics and generating plots.\n\n\n\n\nBootstrap\n\n\n\n\n\n\n\nPlot\n\n\nCreating estimation plots.\n\n\n\n\nplot_tools\n\n\nA set of convenience functions used for producing plots in dabest.\n\n\n\n\neffsize\n\n\nA range of functions to compute various effect sizes.\n\n\n\n\nconfint_1group\n\n\nA range of functions to compute bootstraps for a single sample.\n\n\n\n\nconfint_2group_diff\n\n\nA range of functions to compute bootstraps for the mean difference\n\n\n\n\nmisc_tools\n\n\nConvenience functions that don’t directly deal with plotting or bootstrap computations are placed here.\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "API/effsize.html",
    "href": "API/effsize.html",
    "title": "effsize",
    "section": "",
    "text": "source\n\ntwo_group_difference\n\n two_group_difference (control:Union[list,tuple,numpy.ndarray],\n                       test:Union[list,tuple,numpy.ndarray],\n                       is_paired=None, effect_size:str='mean_diff')\n\nComputes the following metrics for control and test:\n- Unstandardized mean difference\n- Standardized mean differences (paired or unpaired)\n    * Cohen's d\n    * Hedges' g\n- Median difference\n- Cliff's Delta\n- Cohen's h (distance between two proportions)\nSee the Wikipedia entry here\neffect_size:\nmean_diff:      This is simply the mean of `control` subtracted from\n                the mean of `test`.\n\ncohens_d:       This is the mean of control subtracted from the\n                mean of test, divided by the pooled standard deviation\n                of control and test. The pooled SD is the square as:\n\n                       (n1 - 1) * var(control) + (n2 - 1) * var(test)\n                sqrt (   -------------------------------------------  )\n                                         (n1 + n2 - 2)\n\n                where n1 and n2 are the sizes of control and test\n                respectively.\n\nhedges_g:       This is Cohen's d corrected for bias via multiplication\n                 with the following correction factor:\n\n                                gamma(n/2)\n                J(n) = ------------------------------\n                       sqrt(n/2) * gamma((n - 1) / 2)\n\n                where n = (n1 + n2 - 2).\n\nmedian_diff:    This is the median of `control` subtracted from the\n                median of `test`.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncontrol\nlist | tuple | np.ndarray\n\nAccepts lists, tuples, or numpy ndarrays of numeric types.\n\n\ntest\nlist | tuple | np.ndarray\n\nAccepts lists, tuples, or numpy ndarrays of numeric types.\n\n\nis_paired\nNoneType\nNone\nIf not None, returns the paired Cohen’s d\n\n\neffect_size\nstr\nmean_diff\nAny one of the following effect sizes: [“mean_diff”, “median_diff”, “cohens_d”, “hedges_g”, “cliffs_delta”]\n\n\nReturns\nfloat\n\nThe desired effect size.\n\n\n\n\nsource\n\n\nfunc_difference\n\n func_difference (control:Union[list,tuple,numpy.ndarray],\n                  test:Union[list,tuple,numpy.ndarray], func,\n                  is_paired:str)\n\nApplies func to control and test, and then returns the difference.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ncontrol\nlist | tuple | np.ndarray\nNaNs are automatically discarded.\n\n\ntest\nlist | tuple | np.ndarray\nNaNs are automatically discarded.\n\n\nfunc\n\nSummary function to apply.\n\n\nis_paired\nstr\nIf not None, computes func(test - control). If None, computes func(test) - func(control).\n\n\nReturns\nfloat\n\n\n\n\n\nsource\n\n\ncohens_d\n\n cohens_d (control:Union[list,tuple,numpy.ndarray],\n           test:Union[list,tuple,numpy.ndarray], is_paired:str=None)\n\nComputes Cohen’s d for test v.s. control. See here\nIf is_paired is None, returns:\n\\[\n\\frac{\\bar{X}_2 - \\bar{X}_1}{s_{pooled}}\n\\]\nwhere\n\\[\ns_{pooled} = \\sqrt{\\frac{(n_1 - 1) s_1^2 + (n_2 - 1) s_2^2}{n_1 + n_2 - 2}}\n\\]\nIf is_paired is not None, returns:\n\\[\n\\frac{\\bar{X}_2 - \\bar{X}_1}{s_{avg}}\n\\]\nwhere\n\\[\ns_{avg} = \\sqrt{\\frac{s_1^2 + s_2^2}{2}}\n\\]\nNotes:\n\nThe sample variance (and standard deviation) uses N-1 degrees of freedoms. This is an application of Bessel’s correction, and yields the unbiased sample variance.\n\nReferences:\n- https://en.wikipedia.org/wiki/Bessel%27s_correction\n- https://en.wikipedia.org/wiki/Standard_deviation#Corrected_sample_standard_deviation\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncontrol\nlist | tuple | np.ndarray\n\n\n\n\ntest\nlist | tuple | np.ndarray\n\n\n\n\nis_paired\nstr\nNone\nIf not None, the paired Cohen’s d is returned.\n\n\nReturns\nfloat\n\n\n\n\n\n\nsource\n\n\ncohens_h\n\n cohens_h (control:Union[list,tuple,numpy.ndarray],\n           test:Union[list,tuple,numpy.ndarray])\n\nComputes Cohen’s h for test v.s. control. See here\nNotes:\n\nAssuming the input data type is binary, i.e. a series of 0s and 1s, and a dict for mapping the 0s and 1s to the actual labels, e.g.{1: “Smoker”, 0: “Non-smoker”}\n\n\nsource\n\n\nhedges_g\n\n hedges_g (control:Union[list,tuple,numpy.ndarray],\n           test:Union[list,tuple,numpy.ndarray], is_paired:str=None)\n\nComputes Hedges’ g for for test v.s. control. It first computes Cohen’s d, then calulates a correction factor based on the total degress of freedom using the gamma function.\nSee here\n\nsource\n\n\ncliffs_delta\n\n cliffs_delta (control:Union[list,tuple,numpy.ndarray],\n               test:Union[list,tuple,numpy.ndarray])\n\nComputes Cliff’s delta for 2 samples. See here\n\nsource\n\n\nweighted_delta\n\n weighted_delta (difference, group_var)\n\nCompute the weighted deltas where the weight is the inverse of the pooled group difference."
  },
  {
    "objectID": "API/bootstrap.html",
    "href": "API/bootstrap.html",
    "title": "Bootstrap",
    "section": "",
    "text": "bootstrap\n\n bootstrap (x1:&lt;built-infunctionarray&gt;, x2:&lt;built-infunctionarray&gt;=None,\n            paired:bool=False, statfunction:&lt;built-\n            infunctioncallable&gt;=&lt;function mean&gt;, smoothboot:bool=False,\n            alpha_level:float=0.05, reps:int=5000)\n\nComputes the summary statistic and a bootstrapped confidence interval.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nx1\narray\n\nThe data in a one-dimensional array form. Only x1 is required. If x2 is given, the bootstrapped summary difference between the two groups (x2-x1) is computed. NaNs are automatically discarded.\n\n\nx2\narray\nNone\nThe data in a one-dimensional array form. Only x1 is required. If x2 is given, the bootstrapped summary difference between the two groups (x2-x1) is computed. NaNs are automatically discarded.\n\n\npaired\nbool\nFalse\nWhether or not x1 and x2 are paired samples. If ‘paired’ is None then the data will not be treated as paired data in the subsequent calculations. If ‘paired’ is ‘baseline’, then in each tuple of x, other groups will be paired up with the first group (as control). If ‘paired’ is ‘sequential’, then in each tuple of x, each group will be paired up with the previous group (as control).\n\n\nstatfunction\ncallable\nmean\nThe summary statistic called on data.\n\n\nsmoothboot\nbool\nFalse\nTaken from seaborn.algorithms.bootstrap. If True, performs a smoothed bootstrap (draws samples from a kernel destiny estimate).\n\n\nalpha_level\nfloat\n0.05\nDenotes the likelihood that the confidence interval produced does not include the true summary statistic. When alpha = 0.05, a 95% confidence interval is produced.\n\n\nreps\nint\n5000\nNumber of bootstrap iterations to perform.\n\n\nReturns\nAn bootstrap object reporting the summary statistics, percentile CIs, bias-corrected and accelerated (BCa) CIs, and the settings used:\n\nsummary: float. The summary statistic.is_difference: boolean. Whether or not the summary is the difference between two groups. If False, only x1 was supplied.is_paired: string, default None The type of the experiment under which the data are obtainedstatistic: callable The function used to compute the summary.reps: int The number of bootstrap iterations performed.stat_array:array A sorted array of values obtained by bootstrapping the input arrays.ci:float The size of the confidence interval reported (in percentage).pct_ci_low,pct_ci_high:floats The upper and lower bounds of the confidence interval as computed by taking the percentage bounds.pct_low_high_indices:array An array with the indices in stat_array corresponding to the percentage confidence interval bounds.bca_ci_low, bca_ci_high: floats The upper and lower bounds of the bias-corrected and accelerated(BCa) confidence interval. See Efron 1977.bca_low_high_indices: array An array with the indices in stat_array corresponding to the BCa confidence interval bounds.pvalue_1samp_ttest: float P-value obtained from scipy.stats.ttest_1samp. If 2 arrays were passed (x1 and x2), returns ‘NIL’. See https://docs.scipy.org/doc/scipy-1.0.0/reference/generated/scipy.stats.ttest_1samp.htmlpvalue_2samp_ind_ttest: float P-value obtained from scipy.stats.ttest_ind. If a single array was given (x1 only), or if paired is not None, returns ‘NIL’. See https://docs.scipy.org/doc/scipy-1.0.0/reference/generated/scipy.stats.ttest_ind.htmlpvalue_2samp_related_ttest: float P-value obtained from scipy.stats.ttest_rel. If a single array was given (x1 only), or if paired is None, returns ‘NIL’. See https://docs.scipy.org/doc/scipy-1.0.0/reference/generated/scipy.stats.ttest_rel.htmlpvalue_wilcoxon: float P-value obtained from scipy.stats.wilcoxon. If a single array was given (x1 only), or if paired is None, returns ‘NIL’. The Wilcoxons signed-rank test is a nonparametric paired test of the null hypothesis that the related samples x1 and x2 are from the same distribution. See https://docs.scipy.org/doc/scipy-1.0.0/reference/scipy.stats.wilcoxon.htmlpvalue_mann_whitney: float Two-sided p-value obtained from scipy.stats.mannwhitneyu. If a single array was given (x1 only), returns ‘NIL’. The Mann-Whitney U-test is a nonparametric unpaired test of the null hypothesis that x1 and x2 are from the same distribution. See https://docs.scipy.org/doc/scipy-1.0.0/reference/generated/scipy.stats.mannwhitneyu.html\n\n\n\n\n\n\nbca\n\n bca (data, alphas, statarray, statfunction, ostat, reps)\n\nSubroutine called to calculate the BCa statistics. Borrowed heavily from scikits.bootstrap code.\n\n\n\njackknife_indexes\n\n jackknife_indexes (data)\n\nFrom the scikits.bootstrap package. Given an array, returns a list of arrays where each array is a set of jackknife indexes.\nFor a given set of data Y, the jackknife sample J[i] is defined as the data set Y with the ith data point deleted."
  },
  {
    "objectID": "API/plot_tools.html",
    "href": "API/plot_tools.html",
    "title": "plot_tools",
    "section": "",
    "text": "source\n\nsankeydiag\n\n sankeydiag (data:pandas.core.frame.DataFrame, xvar:str, yvar:str,\n             left_idx:str, right_idx:str, leftLabels:list=None,\n             rightLabels:list=None, palette:Union[str,dict]=None, ax=None,\n             one_sankey:bool=False, width:float=0.4,\n             rightColor:bool=False, align:str='center', alpha:float=0.65,\n             **kwargs)\n\nRead in melted pd.DataFrame, and draw multiple sankey diagram on a single axes using the value in column yvar according to the value in column xvar left_idx in the column xvar is on the left side of each sankey diagram right_idx in the column xvar is on the right side of each sankey diagram\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\npd.DataFrame\n\n\n\n\nxvar\nstr\n\nx column to be plotted.\n\n\nyvar\nstr\n\ny column to be plotted.\n\n\nleft_idx\nstr\n\nthe value in column xvar that is on the left side of each sankey diagram\n\n\nright_idx\nstr\n\nthe value in column xvar that is on the right side of each sankey diagram, if len(left_idx) == 1, it will be broadcasted to the same length as right_idx, otherwise it should have the same length as right_idx\n\n\nleftLabels\nlist\nNone\nlabels for the left side of the diagram. The diagram will be sorted by these labels.\n\n\nrightLabels\nlist\nNone\nlabels for the right side of the diagram. The diagram will be sorted by these labels.\n\n\npalette\nstr | dict\nNone\n\n\n\nax\nNoneType\nNone\nmatplotlib axes to be drawn on\n\n\none_sankey\nbool\nFalse\ndetermined by the driver function on plotter.py, if True, draw the sankey diagram across the whole raw data axes\n\n\nwidth\nfloat\n0.4\nthe width of each sankey diagram\n\n\nrightColor\nbool\nFalse\nif True, each strip of the diagram will be colored according to the corresponding left labels\n\n\nalign\nstr\ncenter\nthe alignment of each sankey diagram, can be ‘center’ or ‘left’\n\n\nalpha\nfloat\n0.65\nthe transparency of each strip\n\n\nkwargs\n\n\n\n\n\n\n\nsource\n\n\nsingle_sankey\n\n single_sankey (left:&lt;built-infunctionarray&gt;, right:&lt;built-\n                infunctionarray&gt;, xpos:float=0, leftWeight:&lt;built-\n                infunctionarray&gt;=None, rightWeight:&lt;built-\n                infunctionarray&gt;=None, colorDict:dict=None,\n                leftLabels:list=None, rightLabels:list=None, ax=None,\n                width=0.5, alpha=0.65, bar_width=0.2,\n                rightColor:bool=False, align:bool='center')\n\nMake a single Sankey diagram showing proportion flow from left to right Original code from: https://github.com/anazalea/pySankey Changes are added to normalize each diagram’s height to be 1\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nleft\nnp.array\n\ndata on the left of the diagram\n\n\nright\nnp.array\n\ndata on the right of the diagram, len(left) == len(right)\n\n\nxpos\nfloat\n0\nthe starting point on the x-axis\n\n\nleftWeight\nnp.array\nNone\nweights for the left labels, if None, all weights are 1\n\n\nrightWeight\nnp.array\nNone\nweights for the right labels, if None, all weights are corresponding leftWeight\n\n\ncolorDict\ndict\nNone\ninput format: {‘label’: ‘color’}\n\n\nleftLabels\nlist\nNone\nlabels for the left side of the diagram. The diagram will be sorted by these labels.\n\n\nrightLabels\nlist\nNone\nlabels for the right side of the diagram. The diagram will be sorted by these labels.\n\n\nax\nNoneType\nNone\nmatplotlib axes to be drawn on\n\n\nwidth\nfloat\n0.5\n\n\n\nalpha\nfloat\n0.65\n\n\n\nbar_width\nfloat\n0.2\n\n\n\nrightColor\nbool\nFalse\nif True, each strip of the diagram will be colored according to the corresponding left labels\n\n\nalign\nbool\ncenter\nif ‘center’, the diagram will be centered on each xtick, if ‘edge’, the diagram will be aligned with the left edge of each xtick\n\n\n\n\nsource\n\n\nnormalize_dict\n\n normalize_dict (nested_dict, target)\n\n\nsource\n\n\ncheck_data_matches_labels\n\n check_data_matches_labels (labels, data, side:str)\n\nFunction to check that the labels and data match in the sankey diagram. And enforce labels and data to be lists. Raises an exception if the labels and data do not match.\n\n\n\n\nType\nDetails\n\n\n\n\nlabels\n\nlist of input labels\n\n\ndata\n\nPandas Series of input data\n\n\nside\nstr\n‘left’ or ‘right’ on the sankey diagram\n\n\n\n\nsource\n\n\nerror_bar\n\n error_bar (data:pandas.core.frame.DataFrame, x:str, y:str,\n            type:str='mean_sd', offset:float=0.2, ax=None,\n            line_color='black', gap_width_percent=1, pos:list=[0, 1],\n            method:str='gapped_lines', **kwargs:dict)\n\nFunction to plot the standard deviations as vertical errorbars. The mean is a gap defined by negative space.\nThis function combines the functionality of gapped_lines(), proportional_error_bar(), and sankey_error_bar().\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\npd.DataFrame\n\nThis DataFrame should be in ‘long’ format.\n\n\nx\nstr\n\nx column to be plotted.\n\n\ny\nstr\n\ny column to be plotted.\n\n\ntype\nstr\nmean_sd\nChoose from [‘mean_sd’, ‘median_quartiles’]. Plots the summary statistics for each group. If ‘mean_sd’, then the mean and standard deviation of each group is plotted as a gapped line. If ‘median_quantiles’, then the median and 25th and 75th percentiles of each group is plotted instead.\n\n\noffset\nfloat\n0.2\nGive a single float (that will be used as the x-offset of all gapped lines), or an iterable containing the list of x-offsets.\n\n\nax\nNoneType\nNone\nIf a matplotlib Axes object is specified, the gapped lines will be plotted in order on this axes. If None, the current axes (plt.gca()) is used.\n\n\nline_color\nstr\nblack\n\n\n\ngap_width_percent\nint\n1\n\n\n\npos\nlist\n[0, 1]\nThe positions of the error bars for the sankey_error_bar method.\n\n\nmethod\nstr\ngapped_lines\nThe method to use for drawing the error bars. Options are: ‘gapped_lines’, ‘proportional_error_bar’, and ‘sankey_error_bar’.\n\n\nkwargs\ndict\n\n\n\n\n\n\nsource\n\n\nget_swarm_spans\n\n get_swarm_spans (coll)\n\nGiven a matplotlib Collection, will obtain the x and y spans for the collection. Will return None if this fails.\n\nsource\n\n\nhalfviolin\n\n halfviolin (v, half='right', fill_color='k', alpha=1, line_color='k',\n             line_width=0)"
  },
  {
    "objectID": "API/load.html#example",
    "href": "API/load.html#example",
    "title": "Loading Data",
    "section": "Example",
    "text": "Example\n\nimport numpy as np\nimport pandas as pd\nimport scipy as sp\nimport dabest\n\nCreate dummy data for demonstration.\n\nnp.random.seed(88888)\nN = 10\nc1 = sp.stats.norm.rvs(loc=100, scale=5, size=N)\nt1 = sp.stats.norm.rvs(loc=115, scale=5, size=N)\ndf = pd.DataFrame({'Control 1' : c1, 'Test 1': t1})\n\nLoad the data.\n\nmy_data = dabest.load(df, idx=(\"Control 1\", \"Test 1\"))\nmy_data\n\nDABEST v2023.2.14\n=================\n                 \nGood evening!\nThe current time is Thu Mar 30 12:22:55 2023.\n\nEffect size(s) with 95% confidence intervals will be computed for:\n1. Test 1 minus Control 1\n\n5000 resamples will be used to generate the effect size bootstraps.\n\n\nFor proportion plot.\n\nnp.random.seed(88888)\nN = 10\nc1 = np.random.binomial(1, 0.2, size=N)\nt1 = np.random.binomial(1, 0.5, size=N)\ndf = pd.DataFrame({'Control 1' : c1, 'Test 1': t1})\nmy_data = dabest.load(df, idx=(\"Control 1\", \"Test 1\"),proportional=True)"
  }
]